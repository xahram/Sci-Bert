{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sci_Bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rnneo_nxcRUc",
        "3zj2dWrVs5uD",
        "Pn5qVbd2t4B3"
      ],
      "authorship_tag": "ABX9TyPJMGmFkTdVR9TzG0B771J7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aa82f1d10de642ee8161d83f3dda75b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb0885d2a17947c59b0d0fd314b6a9ec",
              "IPY_MODEL_524d189ce12a4e99aa82ee66c4fb120b",
              "IPY_MODEL_ad47c4328e924105a079ece64cf4f516"
            ],
            "layout": "IPY_MODEL_b1e8127dddb84cb5af98f5be435ad383"
          }
        },
        "fb0885d2a17947c59b0d0fd314b6a9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe97cb8dd28a413483b9cb5b3419afca",
            "placeholder": "​",
            "style": "IPY_MODEL_432ad82ebfb34feab079fe7d01b54c16",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "524d189ce12a4e99aa82ee66c4fb120b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8246d9e2e3994394b91ccb25d575df53",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42c5c57e45904c9da095ba393a39235e",
            "value": 231508
          }
        },
        "ad47c4328e924105a079ece64cf4f516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f22bf4f3374418e941b29656194afba",
            "placeholder": "​",
            "style": "IPY_MODEL_098ab108ac6144b6b5b97fa26da623ff",
            "value": " 226k/226k [00:00&lt;00:00, 4.48MB/s]"
          }
        },
        "b1e8127dddb84cb5af98f5be435ad383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe97cb8dd28a413483b9cb5b3419afca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "432ad82ebfb34feab079fe7d01b54c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8246d9e2e3994394b91ccb25d575df53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42c5c57e45904c9da095ba393a39235e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f22bf4f3374418e941b29656194afba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "098ab108ac6144b6b5b97fa26da623ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f5c68a238c8439aa4c90e5ad70cc49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07cec6a0f6104585a3a0ef6132dc111a",
              "IPY_MODEL_43d8180cbccf4669ae641f0ec0fab2ea",
              "IPY_MODEL_c976d0466b3040ffba10f5a73f6649bd"
            ],
            "layout": "IPY_MODEL_122773b107f44807a5aac585e7078753"
          }
        },
        "07cec6a0f6104585a3a0ef6132dc111a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637330d2eb3b489c9340cd3df814a675",
            "placeholder": "​",
            "style": "IPY_MODEL_01f5a80eac44443ba12acb906c04d0f5",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "43d8180cbccf4669ae641f0ec0fab2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e93e3358dfde48a6a0199b431627cf51",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_711cb7a393ed40e1975f87f365fb71d2",
            "value": 28
          }
        },
        "c976d0466b3040ffba10f5a73f6649bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d937c5e97fba40db9b3cc5f52ff8f13a",
            "placeholder": "​",
            "style": "IPY_MODEL_50c1aebdd45644be99023b7ae5901113",
            "value": " 28.0/28.0 [00:00&lt;00:00, 866B/s]"
          }
        },
        "122773b107f44807a5aac585e7078753": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "637330d2eb3b489c9340cd3df814a675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01f5a80eac44443ba12acb906c04d0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e93e3358dfde48a6a0199b431627cf51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711cb7a393ed40e1975f87f365fb71d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d937c5e97fba40db9b3cc5f52ff8f13a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c1aebdd45644be99023b7ae5901113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35c56dce3da4e0780e05c5fb3b9e8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89dc70a500e74ddb9c88a4fd78fed6e8",
              "IPY_MODEL_a487d01998584642a524600d1a6e4e50",
              "IPY_MODEL_e125a1a12cb04417b95c8e6e9f186966"
            ],
            "layout": "IPY_MODEL_961cc23f79e54c68aec8916bb968093d"
          }
        },
        "89dc70a500e74ddb9c88a4fd78fed6e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f743cdda084ef8bd90b6c74ce47602",
            "placeholder": "​",
            "style": "IPY_MODEL_56ac5c85f4e240cf854bee8ac1880a73",
            "value": "Downloading config.json: 100%"
          }
        },
        "a487d01998584642a524600d1a6e4e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6424c4632ca402c8f7b6d74c551efca",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66148edba41346fdbe2e5991762419cd",
            "value": 570
          }
        },
        "e125a1a12cb04417b95c8e6e9f186966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_343c2abf26e840aa9cfe9ebc24cef8ad",
            "placeholder": "​",
            "style": "IPY_MODEL_eba4033583f84f00a021a92a81fc515a",
            "value": " 570/570 [00:00&lt;00:00, 18.3kB/s]"
          }
        },
        "961cc23f79e54c68aec8916bb968093d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f743cdda084ef8bd90b6c74ce47602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ac5c85f4e240cf854bee8ac1880a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6424c4632ca402c8f7b6d74c551efca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66148edba41346fdbe2e5991762419cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "343c2abf26e840aa9cfe9ebc24cef8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eba4033583f84f00a021a92a81fc515a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beafc5cb688845ca92edb3ab7232462e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57a44348c386411bb087776f99340e0b",
              "IPY_MODEL_e9b1295c659b4226af0f2e00f7ab22d0",
              "IPY_MODEL_6d102b860cac494c9331c621ec0dd692"
            ],
            "layout": "IPY_MODEL_a9322d049b1b4debb94b1039bb3df086"
          }
        },
        "57a44348c386411bb087776f99340e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e802c0dca4e44004ae8f7e0cb4d9d95d",
            "placeholder": "​",
            "style": "IPY_MODEL_b3901258f5774aeeb49fc661f6b50dcd",
            "value": "Downloading: 100%"
          }
        },
        "e9b1295c659b4226af0f2e00f7ab22d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c32bd93d4d64756bbfd67f6fef839d2",
            "max": 690,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7000910bdf7b4923963667e0a64eb33d",
            "value": 690
          }
        },
        "6d102b860cac494c9331c621ec0dd692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b6e090509644b2f91bc8a6dcc30ed52",
            "placeholder": "​",
            "style": "IPY_MODEL_adb286147cbb4e20a72a0d5531dcc69d",
            "value": " 690/690 [00:00&lt;00:00, 5.53kB/s]"
          }
        },
        "a9322d049b1b4debb94b1039bb3df086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e802c0dca4e44004ae8f7e0cb4d9d95d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3901258f5774aeeb49fc661f6b50dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c32bd93d4d64756bbfd67f6fef839d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7000910bdf7b4923963667e0a64eb33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b6e090509644b2f91bc8a6dcc30ed52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adb286147cbb4e20a72a0d5531dcc69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac1bdab52af64bbdac8e0175aa4f76a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fa9ee568aa74769b9f98e9ccd8a340c",
              "IPY_MODEL_46be22f8739341129348c28c326c7da9",
              "IPY_MODEL_00c69415680f4f42829c9a254c1d9da3"
            ],
            "layout": "IPY_MODEL_a5afdb81c00e44e4bbd2c71c62eb237d"
          }
        },
        "4fa9ee568aa74769b9f98e9ccd8a340c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7789575d8ff4ee8879b46ff54219fb1",
            "placeholder": "​",
            "style": "IPY_MODEL_70fd25488e4f4620bdfe70d02956245d",
            "value": "Downloading: 100%"
          }
        },
        "46be22f8739341129348c28c326c7da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ecdbd62a6454ed9acb365cf44e134b7",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0b8214ba91d432b9cef0f05a2cf0c16",
            "value": 190
          }
        },
        "00c69415680f4f42829c9a254c1d9da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a507269683ff4151aa59beb35cedd904",
            "placeholder": "​",
            "style": "IPY_MODEL_86b42d7f54ae4f3c898c7e444d4f3d23",
            "value": " 190/190 [00:00&lt;00:00, 1.76kB/s]"
          }
        },
        "a5afdb81c00e44e4bbd2c71c62eb237d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7789575d8ff4ee8879b46ff54219fb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70fd25488e4f4620bdfe70d02956245d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ecdbd62a6454ed9acb365cf44e134b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b8214ba91d432b9cef0f05a2cf0c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a507269683ff4151aa59beb35cedd904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b42d7f54ae4f3c898c7e444d4f3d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2664ccfd15cf4d298b0cf76868cc0328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93c4c6a4dccc4bfdba382824c1884593",
              "IPY_MODEL_f18162ba9855423f80b6f73e5347c2bc",
              "IPY_MODEL_ddb0ec12a15841eca26262061dddcb75"
            ],
            "layout": "IPY_MODEL_6098cdddb9af47559517ba726f179b49"
          }
        },
        "93c4c6a4dccc4bfdba382824c1884593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34005abca2054b968de23f6f9dce7b8f",
            "placeholder": "​",
            "style": "IPY_MODEL_40229e67f1f94afd935dd21266d58515",
            "value": "Downloading: 100%"
          }
        },
        "f18162ba9855423f80b6f73e5347c2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b26a1e145085457587d62ab061ba2e1b",
            "max": 3988,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56c45ccea737479990cda8b5fd02e51c",
            "value": 3988
          }
        },
        "ddb0ec12a15841eca26262061dddcb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86d0c7c8a2d342928eb2c179b74b74da",
            "placeholder": "​",
            "style": "IPY_MODEL_e0ddb77b501544d2a2f8ba7f90b13fa1",
            "value": " 3.99k/3.99k [00:00&lt;00:00, 40.9kB/s]"
          }
        },
        "6098cdddb9af47559517ba726f179b49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34005abca2054b968de23f6f9dce7b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40229e67f1f94afd935dd21266d58515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b26a1e145085457587d62ab061ba2e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56c45ccea737479990cda8b5fd02e51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86d0c7c8a2d342928eb2c179b74b74da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0ddb77b501544d2a2f8ba7f90b13fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdd438862057434c92896ef2fb2aab25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11032e95e1cc4a568ee3fcbf693d9bf8",
              "IPY_MODEL_eb5e1d9fcb0f4ee38a8f04b58774d732",
              "IPY_MODEL_8b55459f30a741238f73f64906254b8a"
            ],
            "layout": "IPY_MODEL_a0f701f2a9aa4ee8a7993f44b25a2a69"
          }
        },
        "11032e95e1cc4a568ee3fcbf693d9bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df0e679c06b4861a53fc5633dabd05c",
            "placeholder": "​",
            "style": "IPY_MODEL_457a7be2a91f4688837bba6fb4422642",
            "value": "Downloading: 100%"
          }
        },
        "eb5e1d9fcb0f4ee38a8f04b58774d732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cdc32a0b3f40238dd53c59c45e1d90",
            "max": 550,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a88e6ddb0ad34a238ecaccd71f0a324b",
            "value": 550
          }
        },
        "8b55459f30a741238f73f64906254b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b40503ee935f481fa8232aa2667fbbf7",
            "placeholder": "​",
            "style": "IPY_MODEL_58adf9751d824e009116f9764390238e",
            "value": " 550/550 [00:00&lt;00:00, 8.80kB/s]"
          }
        },
        "a0f701f2a9aa4ee8a7993f44b25a2a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df0e679c06b4861a53fc5633dabd05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "457a7be2a91f4688837bba6fb4422642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1cdc32a0b3f40238dd53c59c45e1d90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a88e6ddb0ad34a238ecaccd71f0a324b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b40503ee935f481fa8232aa2667fbbf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58adf9751d824e009116f9764390238e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48b935e983d844f18869ab5ed99e5fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18bbaff4bf4c47ba84804e11622eebbc",
              "IPY_MODEL_87f9056435a24b4ab38e827212731b71",
              "IPY_MODEL_a7fed666c24f42929b0b82951cbaefe4"
            ],
            "layout": "IPY_MODEL_67164429cb56481ea7066c4c3b19ee34"
          }
        },
        "18bbaff4bf4c47ba84804e11622eebbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6409c6863ecc460d875fc5f6be558328",
            "placeholder": "​",
            "style": "IPY_MODEL_3b46f665f92145c1912d7dcd5c749ff8",
            "value": "Downloading: 100%"
          }
        },
        "87f9056435a24b4ab38e827212731b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f760616800d54fcfaa765f6d64ff2a65",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ea2ee6cbf7e4496bea0e9a5a027092e",
            "value": 122
          }
        },
        "a7fed666c24f42929b0b82951cbaefe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_061c5234a7de4e2da387083d916b5cec",
            "placeholder": "​",
            "style": "IPY_MODEL_9d6ee8c522c24b48af4797bd9f21b7ce",
            "value": " 122/122 [00:00&lt;00:00, 4.24kB/s]"
          }
        },
        "67164429cb56481ea7066c4c3b19ee34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6409c6863ecc460d875fc5f6be558328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b46f665f92145c1912d7dcd5c749ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f760616800d54fcfaa765f6d64ff2a65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ea2ee6cbf7e4496bea0e9a5a027092e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "061c5234a7de4e2da387083d916b5cec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d6ee8c522c24b48af4797bd9f21b7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9192837a064c4ff8bd01246d2a96d25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b582308e12d145388d839519a8061e3a",
              "IPY_MODEL_2d07d48b03534379a766c6bfc770b555",
              "IPY_MODEL_1999fb7961e14a0482dd551030b40ac8"
            ],
            "layout": "IPY_MODEL_9fc8d345475e413398d3d0fdc3f5eaa8"
          }
        },
        "b582308e12d145388d839519a8061e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d75413978e4c749f920c6d22e888e7",
            "placeholder": "​",
            "style": "IPY_MODEL_1257362e8bf44daf9dedad12c59a5bff",
            "value": "Downloading: 100%"
          }
        },
        "2d07d48b03534379a766c6bfc770b555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c23c2046edf488db10ff834489d14c9",
            "max": 265486777,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92f95598656a418a91409c10e18d5f6b",
            "value": 265486777
          }
        },
        "1999fb7961e14a0482dd551030b40ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71e63dda09e240ab8ff25eae9e90dc67",
            "placeholder": "​",
            "style": "IPY_MODEL_be1eb45f593a49c481dab9ea25db29a1",
            "value": " 265M/265M [00:06&lt;00:00, 29.3MB/s]"
          }
        },
        "9fc8d345475e413398d3d0fdc3f5eaa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66d75413978e4c749f920c6d22e888e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1257362e8bf44daf9dedad12c59a5bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c23c2046edf488db10ff834489d14c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f95598656a418a91409c10e18d5f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71e63dda09e240ab8ff25eae9e90dc67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be1eb45f593a49c481dab9ea25db29a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf74b24ffcec45308e3cfc8718fbf031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3b388cb748f4fad9475901df3e1adbe",
              "IPY_MODEL_659ce4a99a7e4982aa12cbc19112bfda",
              "IPY_MODEL_cda4654f2945490cb474fad3b540b0cb"
            ],
            "layout": "IPY_MODEL_329b0b6ee0534f54aadf8cc7771dd5cc"
          }
        },
        "d3b388cb748f4fad9475901df3e1adbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1ce28eb2fc94c828676025de95034fa",
            "placeholder": "​",
            "style": "IPY_MODEL_bd8bc53f6eec4496a717ba58bc0427c5",
            "value": "Downloading: 100%"
          }
        },
        "659ce4a99a7e4982aa12cbc19112bfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebd02a0da7db49238cb25a57c78c7f66",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58a80953f7d94641b5b726c99c60095b",
            "value": 53
          }
        },
        "cda4654f2945490cb474fad3b540b0cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fc8facef93b49e39bd4da0f5c6289e3",
            "placeholder": "​",
            "style": "IPY_MODEL_048327af68a7452dbd370a10369349d5",
            "value": " 53.0/53.0 [00:00&lt;00:00, 715B/s]"
          }
        },
        "329b0b6ee0534f54aadf8cc7771dd5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ce28eb2fc94c828676025de95034fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8bc53f6eec4496a717ba58bc0427c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebd02a0da7db49238cb25a57c78c7f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58a80953f7d94641b5b726c99c60095b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fc8facef93b49e39bd4da0f5c6289e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048327af68a7452dbd370a10369349d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fd207d330dc4d8e99cbd431707a2f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c6d78fde26e4812a940eab32af74b74",
              "IPY_MODEL_c7a533e7c743446d8a668de4d1869574",
              "IPY_MODEL_67dac8775ed84b1e8a3ed202f9a6b5e0"
            ],
            "layout": "IPY_MODEL_b7961c0aea3b4a20b17b1987f7899124"
          }
        },
        "3c6d78fde26e4812a940eab32af74b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36eda66024bf4f16b53d73c2b585faaf",
            "placeholder": "​",
            "style": "IPY_MODEL_e19e211c2af54ce4a8ba350a2ee0fd36",
            "value": "Downloading: 100%"
          }
        },
        "c7a533e7c743446d8a668de4d1869574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fcb00797e8c447eb9a482f539ea5908",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e13b2dabda6d4845a643b1ec40075a8f",
            "value": 112
          }
        },
        "67dac8775ed84b1e8a3ed202f9a6b5e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fe535163f9944b097481b706046e9e2",
            "placeholder": "​",
            "style": "IPY_MODEL_a2941fedfc6b436699e77ba742755b31",
            "value": " 112/112 [00:00&lt;00:00, 1.28kB/s]"
          }
        },
        "b7961c0aea3b4a20b17b1987f7899124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36eda66024bf4f16b53d73c2b585faaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e19e211c2af54ce4a8ba350a2ee0fd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fcb00797e8c447eb9a482f539ea5908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e13b2dabda6d4845a643b1ec40075a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fe535163f9944b097481b706046e9e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2941fedfc6b436699e77ba742755b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b09231f3de14613b0edea733333de08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba1b490f892f4db6bbceff58b90e9442",
              "IPY_MODEL_0f30e56389d8489987b928678a78d624",
              "IPY_MODEL_c7ebc71ae54d4ba78f498aebd2f073f2"
            ],
            "layout": "IPY_MODEL_7424db164ef5472e868d98323850efcd"
          }
        },
        "ba1b490f892f4db6bbceff58b90e9442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ad2cb9c80d540a3b6407747d4c7b56e",
            "placeholder": "​",
            "style": "IPY_MODEL_e6b9cac619234d7fa93c7aae46de8cac",
            "value": "Downloading: 100%"
          }
        },
        "0f30e56389d8489987b928678a78d624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c10d7ad74d6b41dca0d175fcd2e224db",
            "max": 466081,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e339b79beb04e6aafc5134365347bc1",
            "value": 466081
          }
        },
        "c7ebc71ae54d4ba78f498aebd2f073f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637ae4eaeff549daa4b8ec67182247be",
            "placeholder": "​",
            "style": "IPY_MODEL_677e60d5778d439cbe39937947f535da",
            "value": " 466k/466k [00:00&lt;00:00, 2.76MB/s]"
          }
        },
        "7424db164ef5472e868d98323850efcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad2cb9c80d540a3b6407747d4c7b56e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b9cac619234d7fa93c7aae46de8cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c10d7ad74d6b41dca0d175fcd2e224db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e339b79beb04e6aafc5134365347bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "637ae4eaeff549daa4b8ec67182247be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677e60d5778d439cbe39937947f535da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7dbd0d4339745bcbdf8c3add1112a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f0016f6a20a4d26a43f0635278c77c6",
              "IPY_MODEL_d4c5e4674c9e41a8bad3111d0710a4c5",
              "IPY_MODEL_49bab01662aa4f3daa80662ae1e37994"
            ],
            "layout": "IPY_MODEL_635811d85cab44739ff713fb80c38ecc"
          }
        },
        "1f0016f6a20a4d26a43f0635278c77c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a41a8001d5b2428497f29d4c8076dedc",
            "placeholder": "​",
            "style": "IPY_MODEL_28e50bfe7c684478947cd403fb361d2c",
            "value": "Downloading: 100%"
          }
        },
        "d4c5e4674c9e41a8bad3111d0710a4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1842a394a09e417ca03566971583ea88",
            "max": 450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1cb169f2e51c45d68cf12e1c2430dd9d",
            "value": 450
          }
        },
        "49bab01662aa4f3daa80662ae1e37994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faf8399e7dbc4a6aa95345d131113fa2",
            "placeholder": "​",
            "style": "IPY_MODEL_3303c2df8de547c793743a11a583a284",
            "value": " 450/450 [00:00&lt;00:00, 4.03kB/s]"
          }
        },
        "635811d85cab44739ff713fb80c38ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a41a8001d5b2428497f29d4c8076dedc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28e50bfe7c684478947cd403fb361d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1842a394a09e417ca03566971583ea88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cb169f2e51c45d68cf12e1c2430dd9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faf8399e7dbc4a6aa95345d131113fa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3303c2df8de547c793743a11a583a284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f489225530049a081a04eb8960bf7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee5ac421d7234d2d89c2f0fe58ccdf9a",
              "IPY_MODEL_fa68ca91c3f54de18c8c51c9a70d9c48",
              "IPY_MODEL_d67f027cf0114a99a68aeabdb1b9513a"
            ],
            "layout": "IPY_MODEL_962e7b1fae7c443489cf4616e1992aad"
          }
        },
        "ee5ac421d7234d2d89c2f0fe58ccdf9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b5c3b7373b3440792045c394c6ac75c",
            "placeholder": "​",
            "style": "IPY_MODEL_fd343d5f0f9f48c68680c61b45068039",
            "value": "Downloading: 100%"
          }
        },
        "fa68ca91c3f54de18c8c51c9a70d9c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d2f8673af74447089b6aa6d4baa71b9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7cfc1f0774345e1b48f5713db495acc",
            "value": 231508
          }
        },
        "d67f027cf0114a99a68aeabdb1b9513a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c4decff5ed5453aa6d7147253237c3a",
            "placeholder": "​",
            "style": "IPY_MODEL_508ba87dc4224c659260405a97bf795a",
            "value": " 232k/232k [00:00&lt;00:00, 2.33MB/s]"
          }
        },
        "962e7b1fae7c443489cf4616e1992aad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5c3b7373b3440792045c394c6ac75c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd343d5f0f9f48c68680c61b45068039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d2f8673af74447089b6aa6d4baa71b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7cfc1f0774345e1b48f5713db495acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c4decff5ed5453aa6d7147253237c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "508ba87dc4224c659260405a97bf795a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1552b4f3f584dc1aedc5e4ee7bab37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32de9c39474e4635a4574922f7f127d0",
              "IPY_MODEL_3ddd0633fff64e22bdb48d327e0e33ba",
              "IPY_MODEL_321a723e661e4d14b2b920ffbbfb7acd"
            ],
            "layout": "IPY_MODEL_c7e2c829d89549dc840e351eeaeb8a3e"
          }
        },
        "32de9c39474e4635a4574922f7f127d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f82f12724194a9a8b4d0c84d1d370ec",
            "placeholder": "​",
            "style": "IPY_MODEL_0c685f7d23664f66b10dd20e3ba3c131",
            "value": "Downloading: 100%"
          }
        },
        "3ddd0633fff64e22bdb48d327e0e33ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_283d7184ce334c20922844e28495c886",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64e22e931b4b48329ffb6bbd12a5402e",
            "value": 229
          }
        },
        "321a723e661e4d14b2b920ffbbfb7acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb360b184e542b6ad947c7007649675",
            "placeholder": "​",
            "style": "IPY_MODEL_f4a31f7288b74fb48473c25ceb960c76",
            "value": " 229/229 [00:00&lt;00:00, 2.55kB/s]"
          }
        },
        "c7e2c829d89549dc840e351eeaeb8a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f82f12724194a9a8b4d0c84d1d370ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c685f7d23664f66b10dd20e3ba3c131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "283d7184ce334c20922844e28495c886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64e22e931b4b48329ffb6bbd12a5402e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "beb360b184e542b6ad947c7007649675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a31f7288b74fb48473c25ceb960c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5008c41fab98446192efb54edeb90b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08132ed3ec6540e7ad118fafbb834f3f",
              "IPY_MODEL_a03e32d09384405ab7dd1b0c4b115ca2",
              "IPY_MODEL_493a920f49ec4468a04b91e337b1e430"
            ],
            "layout": "IPY_MODEL_df17bd23e4064464926c0a9aba307316"
          }
        },
        "08132ed3ec6540e7ad118fafbb834f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d58cc9afb684653bda3e3bf88a60d98",
            "placeholder": "​",
            "style": "IPY_MODEL_b59a6ee8e5af4a20abd0e43afe5f1dfb",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "a03e32d09384405ab7dd1b0c4b115ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78ac7abb7b554068a567ba8effe2aeec",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d3581295fbf47aea94c4decac824446",
            "value": 440473133
          }
        },
        "493a920f49ec4468a04b91e337b1e430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6971da95bf5347a79ff91d6babf6294e",
            "placeholder": "​",
            "style": "IPY_MODEL_510c071f783241b7b8adb3d3e7ca5e03",
            "value": " 420M/420M [00:12&lt;00:00, 29.5MB/s]"
          }
        },
        "df17bd23e4064464926c0a9aba307316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d58cc9afb684653bda3e3bf88a60d98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b59a6ee8e5af4a20abd0e43afe5f1dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78ac7abb7b554068a567ba8effe2aeec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3581295fbf47aea94c4decac824446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6971da95bf5347a79ff91d6babf6294e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "510c071f783241b7b8adb3d3e7ca5e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba4f52e9467444cc80f4a1dcfe780d77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2363d13ea96a406e8bd042d537bda599",
              "IPY_MODEL_6305c5a1e8764696ace560778780bc9e",
              "IPY_MODEL_23f4177328454c28ab97b6a139213d82"
            ],
            "layout": "IPY_MODEL_49b53f150ab648348f88d05daa6eb65d"
          }
        },
        "2363d13ea96a406e8bd042d537bda599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58cb02b3b8174959912153dff4971fc8",
            "placeholder": "​",
            "style": "IPY_MODEL_4349a979dc1a4542844befb7fe5aa20b",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "6305c5a1e8764696ace560778780bc9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80a3bfa1a68d4b999c77c9e444202bc8",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a08249b5f60e4d1c9b5b0213ea92812f",
            "value": 466062
          }
        },
        "23f4177328454c28ab97b6a139213d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd77407844dc4f51a2497a105d424288",
            "placeholder": "​",
            "style": "IPY_MODEL_3d3cdc3640a94926a4e7c618569bcadf",
            "value": " 455k/455k [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "49b53f150ab648348f88d05daa6eb65d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58cb02b3b8174959912153dff4971fc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4349a979dc1a4542844befb7fe5aa20b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80a3bfa1a68d4b999c77c9e444202bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a08249b5f60e4d1c9b5b0213ea92812f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd77407844dc4f51a2497a105d424288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d3cdc3640a94926a4e7c618569bcadf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xahram/Sci-Bert/blob/main/Sci_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First step is to load the NIPS data that is uploaded in the Google Drive"
      ],
      "metadata": {
        "id": "oPZ9rjzVQFYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the google drive folder into the directory to access files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmrUd6BcBAKB",
        "outputId": "0155fd1d-2ae1-472a-d62a-fbeab72d85eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YZGpDT8-BrZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33b4a23-bd89-43fa-af21-4f72aa363263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the NIPS dataset from the drive\n",
        "\n",
        "nips_papers_df = pd.read_csv('/gdrive/My Drive/Master_dataset/papers.csv')  \n",
        "nips_papers_df.head()\n",
        "\n",
        "nips_papers = nips_papers_df.infer_objects()\n",
        "\n",
        "nips_papers.dtypes\n",
        "\n",
        "nips_papers[\"year\"] = pd.to_datetime(nips_papers[\"year\"], format=\"%Y\")\n",
        "# nips_papers['year'] = nips_papers['year'].dt.year\n",
        "nips_papers.sort_values(by='year')\n",
        "\n",
        "print(nips_papers.dtypes)\n",
        "\n",
        "max(nips_papers[\"year\"])\n",
        "min(nips_papers[\"year\"])\n",
        "\n",
        "nips_papers = nips_papers.sort_values(by = \"year\")\n"
      ],
      "metadata": {
        "id": "z0H4P5yIBexP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ea3cdc-ab00-4af9-95a8-82ec214b2b9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                     int64\n",
            "year          datetime64[ns]\n",
            "title                 object\n",
            "event_type            object\n",
            "pdf_name              object\n",
            "abstract              object\n",
            "paper_text            object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import close\n",
        "# Slice Data Frame by 3 year interval\n",
        "\n",
        "\n",
        "# print(len(nips_papers))\n",
        "\n",
        "# Partition/Group Papers into df by the interval/freq of 3 years, closed = left to start combinbing from the 1987\n",
        "nips_papers_3y_grouped = nips_papers.groupby(pd.Grouper(key='year', freq='3Y', sort=True, closed=\"left\"))\n",
        "\n",
        "\n",
        "\n",
        "# Save partitions in the Dictionary format with 10 intervals\n",
        "nips_papers_partitions = {}\n",
        "initial_partition_id = 0\n",
        "for i, g  in nips_papers_3y_grouped:\n",
        "    nips_papers_partitions[initial_partition_id] = g\n",
        "    initial_partition_id = initial_partition_id + 1\n",
        "\n",
        "\n",
        "print(nips_papers_partitions)\n",
        "# nips_papers_three_year_partition[0].tail()\n",
        "\n",
        "\n",
        "#for i, g in nips_papers.groupby(pd.Grouper(key=nips_papers[\"year\"], freq='A')):\n",
        "#     print(g)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4A_aWwViSXFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d0e244-a416-48b8-b425-9ae7f7787859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0:        id       year                                              title  \\\n",
            "0       1 1987-01-01  Self-Organization of Associative Database and ...   \n",
            "328    13 1987-01-01   Temporal Patterns of Activity in Neural Networks   \n",
            "6853   72 1987-01-01  Ensemble' Boltzmann Units have Collective Comp...   \n",
            "6743   71 1987-01-01  Centric Models of the Orientation Map in Prima...   \n",
            "6632   70 1987-01-01  On the Power of Neural Networks for Solving Ha...   \n",
            "...   ...        ...                                                ...   \n",
            "1650  250 1989-01-01                               Optimal Brain Damage   \n",
            "1661  251 1989-01-01  A Self-organizing Associative Memory System fo...   \n",
            "1672  252 1989-01-01  Can Simple Cells Learn Curves? A Hebbian Model...   \n",
            "1683  253 1989-01-01  Subgrouping Reduces Complexity and Speeds Up L...   \n",
            "1638  249 1989-01-01  Neural Network Analysis of Distributed Represe...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "0           NaN  1-self-organization-of-associative-database-an...   \n",
            "328         NaN  13-temporal-patterns-of-activity-in-neural-net...   \n",
            "6853        NaN  72-ensemble-boltzmann-units-have-collective-co...   \n",
            "6743        NaN  71-centric-models-of-the-orientation-map-in-pr...   \n",
            "6632        NaN  70-on-the-power-of-neural-networks-for-solving...   \n",
            "...         ...                                                ...   \n",
            "1650        NaN                       250-optimal-brain-damage.pdf   \n",
            "1661        NaN  251-a-self-organizing-associative-memory-syste...   \n",
            "1672        NaN  252-can-simple-cells-learn-curves-a-hebbian-mo...   \n",
            "1683        NaN  253-subgrouping-reduces-complexity-and-speeds-...   \n",
            "1638        NaN  249-neural-network-analysis-of-distributed-rep...   \n",
            "\n",
            "              abstract                                         paper_text  \n",
            "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
            "328   Abstract Missing  297\\n\\nTEMPORAL PATTERNS OF ACTIVITY IN\\nNEURA...  \n",
            "6853  Abstract Missing  223\\n\\n'Ensemble' Boltzmann Units\\nhave Collec...  \n",
            "6743  Abstract Missing  62\\n\\nCentric Models of the Orientation Map in...  \n",
            "6632  Abstract Missing  137\\n\\nOn the Power of Neural Networks for\\nSo...  \n",
            "...                ...                                                ...  \n",
            "1650  Abstract Missing  598\\n\\nLe Cun, Denker and Solla\\n\\nOptimal Bra...  \n",
            "1661  Abstract Missing  332\\n\\nHormel\\n\\nA Sell-organizing Associative...  \n",
            "1672  Abstract Missing  Can Simple Cells Learn Curves? A Hebbian Model...  \n",
            "1683  Abstract Missing  638\\n\\nZipser\\n\\nSubgrouping Reduces Complexit...  \n",
            "1638  Abstract Missing  28\\n\\nLockery t Fang and Sejnowski\\n\\nNeu.?al ...  \n",
            "\n",
            "[285 rows x 7 columns], 1:        id       year                                              title  \\\n",
            "3212  391 1990-01-01  Designing Linear Threshold Based Neural Networ...   \n",
            "3401  408 1990-01-01                           Adaptive Spline Networks   \n",
            "3278  397 1990-01-01  Integrated Segmentation and Recognition of Han...   \n",
            "3390  407 1990-01-01         Convergence of a Neural Network Classifier   \n",
            "3245  394 1990-01-01  Chaitin-Kolmogorov Complexity and Generalizati...   \n",
            "...   ...        ...                                                ...   \n",
            "5946  638 1992-01-01  Network Structuring and Training Using Rule-ba...   \n",
            "5769  622 1992-01-01    Information, Prediction, and Query by Committee   \n",
            "5758  621 1992-01-01  Some Solutions to the Missing Feature Problem ...   \n",
            "5747  620 1992-01-01  Connected Letter Recognition with a Multi-Stat...   \n",
            "5802  625 1992-01-01  Visual Motion Computation in Analog VLSI Using...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "3212        NaN  391-designing-linear-threshold-based-neural-ne...   \n",
            "3401        NaN                   408-adaptive-spline-networks.pdf   \n",
            "3278        NaN  397-integrated-segmentation-and-recognition-of...   \n",
            "3390        NaN  407-convergence-of-a-neural-network-classifier...   \n",
            "3245        NaN  394-chaitin-kolmogorov-complexity-and-generali...   \n",
            "...         ...                                                ...   \n",
            "5946        NaN  638-network-structuring-and-training-using-rul...   \n",
            "5769        NaN  622-information-prediction-and-query-by-commit...   \n",
            "5758        NaN  621-some-solutions-to-the-missing-feature-prob...   \n",
            "5747        NaN  620-connected-letter-recognition-with-a-multi-...   \n",
            "5802        NaN  625-visual-motion-computation-in-analog-vlsi-u...   \n",
            "\n",
            "              abstract                                         paper_text  \n",
            "3212  Abstract Missing  Designing Linear Threshold Based Neural\\nNetwo...  \n",
            "3401  Abstract Missing  ADAPTIVE SPLINE NETWORKS\\n\\nJerome H. Friedman...  \n",
            "3278  Abstract Missing  Integrated Segmentation and Recognition of\\nHa...  \n",
            "3390  Abstract Missing  Convergence of a Neural Network Classifier\\n\\n...  \n",
            "3245  Abstract Missing  Chaitin-Kolmogorov Complexity\\nand Generalizat...  \n",
            "...                ...                                                ...  \n",
            "5946  Abstract Missing  Network Structuring And Training Using\\nRule-b...  \n",
            "5769  Abstract Missing  Information, prediction, and query by\\ncommitt...  \n",
            "5758  Abstract Missing  Some Solutions to the Missing Feature Problem\\...  \n",
            "5747  Abstract Missing  Connected Letter Recognition with a\\nMulti-Sta...  \n",
            "5802  Abstract Missing  Visual Motion Computation in Analog\\nVLSI usin...  \n",
            "\n",
            "[414 rows x 7 columns], 2:         id       year                                              title  \\\n",
            "7006   782 1993-01-01  Optimal Unsupervised Motor Learning Predicts t...   \n",
            "7005   781 1993-01-01  A Comparison of Dynamic Reposing and Tangent D...   \n",
            "7004   780 1993-01-01         Dynamic Modulation of Neurons and Networks   \n",
            "7002   779 1993-01-01    Address Block Location with a Neural Net System   \n",
            "7001   778 1993-01-01  A Learning Analog Neural Network Chip with Con...   \n",
            "...    ...        ...                                                ...   \n",
            "69    1060 1995-01-01  Statistical Theory of Overtraining - Is Cross-...   \n",
            "63    1055 1995-01-01  Adaptive Retina with Center-Surround Receptive...   \n",
            "150   1135 1995-01-01               Information through a Spiking Neuron   \n",
            "62    1054 1995-01-01  Implementation Issues in the Fourier Transform...   \n",
            "65    1057 1995-01-01  When is an Integrate-and-fire Neuron like a Po...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "7006        NaN  782-optimal-unsupervised-motor-learning-predic...   \n",
            "7005        NaN  781-a-comparison-of-dynamic-reposing-and-tange...   \n",
            "7004        NaN  780-dynamic-modulation-of-neurons-and-networks...   \n",
            "7002        NaN  779-address-block-location-with-a-neural-net-s...   \n",
            "7001        NaN  778-a-learning-analog-neural-network-chip-with...   \n",
            "...         ...                                                ...   \n",
            "69          NaN  1060-statistical-theory-of-overtraining-is-cro...   \n",
            "63          NaN  1055-adaptive-retina-with-center-surround-rece...   \n",
            "150         NaN      1135-information-through-a-spiking-neuron.pdf   \n",
            "62          NaN  1054-implementation-issues-in-the-fourier-tran...   \n",
            "65          NaN  1057-when-is-an-integrate-and-fire-neuron-like...   \n",
            "\n",
            "              abstract                                         paper_text  \n",
            "7006  Abstract Missing  Optimal Unsupervised Motor Learning\\nPredicts ...  \n",
            "7005  Abstract Missing  A Comparison of Dynamic Reposing and\\nTangent ...  \n",
            "7004  Abstract Missing  Dynamic Modulation of Neurons and Networks\\n\\n...  \n",
            "7002  Abstract Missing  Address Block Location with a Neural Net Syste...  \n",
            "7001  Abstract Missing  A Learning Analog Neural Network Chip\\nwith Co...  \n",
            "...                ...                                                ...  \n",
            "69    Abstract Missing  Statistical Theory of Overtraining - Is\\nCross...  \n",
            "63    Abstract Missing  Adaptive Retina with Center-Surround\\nReceptiv...  \n",
            "150   Abstract Missing  Information through a Spiking Neuron\\n\\nCharle...  \n",
            "62    Abstract Missing  Implementation Issues in the Fourier\\nTransfor...  \n",
            "65    Abstract Missing  When is an Integrate-and-fire Neuron\\nlike a P...  \n",
            "\n",
            "[450 rows x 7 columns], 3:        id       year                                              title  \\\n",
            "288  1263 1996-01-01  Training Algorithms for Hidden Markov Models u...   \n",
            "290  1265 1996-01-01  A Silicon Model of Amplitude Modulation Detect...   \n",
            "291  1266 1996-01-01  Learning Temporally Persistent Hierarchical Re...   \n",
            "289  1264 1996-01-01                       Hidden Markov Decision Trees   \n",
            "279  1255 1996-01-01  A Model of Recurrent Interactions in Primary V...   \n",
            "..    ...        ...                                                ...   \n",
            "534  1489 1998-01-01            A Neuromorphic Monaural Sound Localizer   \n",
            "536  1490 1998-01-01  Very Fast EM-Based Mixture Model Clustering Us...   \n",
            "537  1491 1998-01-01        Kernel PCA and De-Noising in Feature Spaces   \n",
            "538  1492 1998-01-01  Viewing Classifier Systems as Model Free Learn...   \n",
            "532  1487 1998-01-01  A Reinforcement Learning Algorithm in Partiall...   \n",
            "\n",
            "    event_type                                           pdf_name  \\\n",
            "288        NaN  1263-training-algorithms-for-hidden-markov-mod...   \n",
            "290        NaN  1265-a-silicon-model-of-amplitude-modulation-d...   \n",
            "291        NaN  1266-learning-temporally-persistent-hierarchic...   \n",
            "289        NaN              1264-hidden-markov-decision-trees.pdf   \n",
            "279        NaN  1255-a-model-of-recurrent-interactions-in-prim...   \n",
            "..         ...                                                ...   \n",
            "534        NaN   1489-a-neuromorphic-monaural-sound-localizer.pdf   \n",
            "536        NaN  1490-very-fast-em-based-mixture-model-clusteri...   \n",
            "537        NaN  1491-kernel-pca-and-de-noising-in-feature-spac...   \n",
            "538        NaN  1492-viewing-classifier-systems-as-model-free-...   \n",
            "532        NaN  1487-a-reinforcement-learning-algorithm-in-par...   \n",
            "\n",
            "             abstract                                         paper_text  \n",
            "288  Abstract Missing  Training Algorithms for Hidden Markov Models\\n...  \n",
            "290  Abstract Missing  A Silicon Model of\\nAmplitude Modulation Detec...  \n",
            "291  Abstract Missing  Learning temporally persistent\\nhierarchical r...  \n",
            "289  Abstract Missing  Hidden Markov decision trees\\nMichael I. Jorda...  \n",
            "279  Abstract Missing  A Model of Recurrent Interactions in\\nPrimary ...  \n",
            "..                ...                                                ...  \n",
            "534  Abstract Missing  A N euromorphic Monaural Sound\\nLocalizer\\nJoh...  \n",
            "536  Abstract Missing  Very Fast EM-based Mixture Model\\nClustering u...  \n",
            "537  Abstract Missing  Kernel peA and De-Noising in Feature Spaces\\n\\...  \n",
            "538  Abstract Missing  Viewing Classifier Systems\\nas Model Free Lear...  \n",
            "532  Abstract Missing  A Reinforcement Learning Algorithm\\nin Partial...  \n",
            "\n",
            "[453 rows x 7 columns], 4:         id       year                                              title  \\\n",
            "803   1735 1999-01-01                     Uniqueness of the SVM Solution   \n",
            "807   1739 1999-01-01  Algebraic Analysis for Non-regular Learning Ma...   \n",
            "804   1736 1999-01-01  Nonlinear Discriminant Analysis Using Kernel F...   \n",
            "805   1737 1999-01-01                                Potential Boosters?   \n",
            "781   1715 1999-01-01  Invariant Feature Extraction and Classificatio...   \n",
            "...    ...        ...                                                ...   \n",
            "1125  2026 2001-01-01  Modeling Temporal Structure in Classical Condi...   \n",
            "1126  2027 2001-01-01  TAP Gibbs Free Energy, Belief Propagation and ...   \n",
            "1128  2029 2001-01-01  Hyperbolic Self-Organizing Maps for Semantic N...   \n",
            "1121  2022 2001-01-01  Learning Lateral Interactions for Feature Bind...   \n",
            "1086  1992 2001-01-01         Spectral Relaxation for K-means Clustering   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "803         NaN            1735-uniqueness-of-the-svm-solution.pdf   \n",
            "807         NaN  1739-algebraic-analysis-for-non-regular-learni...   \n",
            "804         NaN  1736-nonlinear-discriminant-analysis-using-ker...   \n",
            "805         NaN                        1737-potential-boosters.pdf   \n",
            "781         NaN  1715-invariant-feature-extraction-and-classifi...   \n",
            "...         ...                                                ...   \n",
            "1125        NaN  2026-modeling-temporal-structure-in-classical-...   \n",
            "1126        NaN  2027-tap-gibbs-free-energy-belief-propagation-...   \n",
            "1128        NaN  2029-hyperbolic-self-organizing-maps-for-seman...   \n",
            "1121        NaN  2022-learning-lateral-interactions-for-feature...   \n",
            "1086        NaN  1992-spectral-relaxation-for-k-means-clusterin...   \n",
            "\n",
            "              abstract                                         paper_text  \n",
            "803   Abstract Missing  Uniqueness of the SVM Solution\\n\\nChristopher ...  \n",
            "807   Abstract Missing  Algebraic Analysis for Non-Regular\\nLearning M...  \n",
            "804   Abstract Missing  Nonlinear Discriminant Analysis using\\nKernel ...  \n",
            "805   Abstract Missing  Potential Boosters ?\\nNigel Duffy\\nDepartment ...  \n",
            "781   Abstract Missing  U nmixing Hyperspectral Data\\n\\nLucas Parra, C...  \n",
            "...                ...                                                ...  \n",
            "1125  Abstract Missing  Modeling Temporal Structure in Classical\\nCond...  \n",
            "1126  Abstract Missing  TAP Gibbs Free Energy, Belief Propagation and\\...  \n",
            "1128  Abstract Missing  Hyperbolic Self-Organizing Maps for Semantic\\n...  \n",
            "1121  Abstract Missing  Learning Lateral Interactions for\\nFeature Bin...  \n",
            "1086  Abstract Missing  Spectral Relaxation for K-means\\nClustering\\n\\...  \n",
            "\n",
            "[499 rows x 7 columns], 5:         id       year                                              title  \\\n",
            "1319  2200 2002-01-01                 A Bilinear Model for Sparse Coding   \n",
            "1392  2267 2002-01-01  Data-Dependent Bounds for Bayesian Mixture Met...   \n",
            "1393  2268 2002-01-01  Shape Recipes: Scene Representations that Refe...   \n",
            "1394  2269 2002-01-01  Ranking with Large Margin Principle: Two Appro...   \n",
            "1396  2270 2002-01-01  An Estimation-Theoretic Framework for the Pres...   \n",
            "...    ...        ...                                                ...   \n",
            "1717  2560 2004-01-01                         Adaptive Manifold Learning   \n",
            "1718  2561 2004-01-01                       Dependent Gaussian Processes   \n",
            "1704  2549 2004-01-01  The Power of Selective Memory: Self-Bounded Le...   \n",
            "1719  2562 2004-01-01  Edge of Chaos Computation in Mixed-Mode VLSI -...   \n",
            "1713  2557 2004-01-01  Conditional Models of Identity Uncertainty wit...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "1319        NaN        2200-a-bilinear-model-for-sparse-coding.pdf   \n",
            "1392        NaN  2267-data-dependent-bounds-for-bayesian-mixtur...   \n",
            "1393        NaN  2268-shape-recipes-scene-representations-that-...   \n",
            "1394        NaN  2269-ranking-with-large-margin-principle-two-a...   \n",
            "1396        NaN  2270-an-estimation-theoretic-framework-for-the...   \n",
            "...         ...                                                ...   \n",
            "1717        NaN                2560-adaptive-manifold-learning.pdf   \n",
            "1718        NaN              2561-dependent-gaussian-processes.pdf   \n",
            "1704        NaN  2549-the-power-of-selective-memory-self-bounde...   \n",
            "1719        NaN  2562-edge-of-chaos-computation-in-mixed-mode-v...   \n",
            "1713        NaN  2557-conditional-models-of-identity-uncertaint...   \n",
            "\n",
            "              abstract                                         paper_text  \n",
            "1319  Abstract Missing  A Bilinear Model for Sparse Coding\\n\\nDavid B....  \n",
            "1392  Abstract Missing  Data-Dependent Bounds for Bayesian\\nMixture Me...  \n",
            "1393  Abstract Missing  Shape Recipes: Scene Representations that Refe...  \n",
            "1394  Abstract Missing  Ranking with Large Margin Principle: Two\\nAppr...  \n",
            "1396  Abstract Missing  An Estimation-Theoretic Framework for\\nthe Pre...  \n",
            "...                ...                                                ...  \n",
            "1717  Abstract Missing  Adaptive Manifold Learning\\n\\nJing Wang, Zheny...  \n",
            "1718  Abstract Missing  Dependent Gaussian Processes\\n\\nPhillip Boyle ...  \n",
            "1704  Abstract Missing  The Power of Selective Memory:\\nSelf-Bounded L...  \n",
            "1719  Abstract Missing  Edge of Chaos Computation in\\nMixed-Mode VLSI ...  \n",
            "1713  Abstract Missing  Conditional Models of Identity Uncertainty\\nwi...  \n",
            "\n",
            "[612 rows x 7 columns], 6:         id       year                                              title  \\\n",
            "2012  2828 2005-01-01  Asymptotics of Gaussian Regularized Least Squares   \n",
            "2013  2829 2005-01-01     Two view learning: SVM-2K, Theory and Practice   \n",
            "2015  2830 2005-01-01         Saliency Based on Information Maximization   \n",
            "2016  2831 2005-01-01     Faster Rates in Regression via Active Learning   \n",
            "2017  2832 2005-01-01                           Layered Dynamic Textures   \n",
            "...    ...        ...                                                ...   \n",
            "2462  3233 2007-01-01  Fitted Q-iteration in continuous action-space ...   \n",
            "2463  3234 2007-01-01      Topmoumoute Online Natural Gradient Algorithm   \n",
            "2464  3235 2007-01-01  Sparse Overcomplete Latent Variable Decomposit...   \n",
            "2465  3236 2007-01-01  Second Order Bilinear Discriminant Analysis fo...   \n",
            "2466  3237 2007-01-01  Learning Horizontal Connections in a Sparse Co...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "2012        NaN  2828-asymptotics-of-gaussian-regularized-least...   \n",
            "2013        NaN  2829-two-view-learning-svm-2k-theory-and-pract...   \n",
            "2015        NaN  2830-saliency-based-on-information-maximizatio...   \n",
            "2016        NaN  2831-faster-rates-in-regression-via-active-lea...   \n",
            "2017        NaN                  2832-layered-dynamic-textures.pdf   \n",
            "...         ...                                                ...   \n",
            "2462        NaN  3233-fitted-q-iteration-in-continuous-action-s...   \n",
            "2463        NaN  3234-topmoumoute-online-natural-gradient-algor...   \n",
            "2464        NaN  3235-sparse-overcomplete-latent-variable-decom...   \n",
            "2465        NaN  3236-second-order-bilinear-discriminant-analys...   \n",
            "2466        NaN  3237-learning-horizontal-connections-in-a-spar...   \n",
            "\n",
            "                                               abstract  \\\n",
            "2012                                   Abstract Missing   \n",
            "2013                                   Abstract Missing   \n",
            "2015                                   Abstract Missing   \n",
            "2016                                   Abstract Missing   \n",
            "2017                                   Abstract Missing   \n",
            "...                                                 ...   \n",
            "2462  We consider continuous state, continuous actio...   \n",
            "2463  Guided by the goal of obtaining an optimizatio...   \n",
            "2464  An important problem in many fields is the ana...   \n",
            "2465  Traditional analysis methods for single-trial ...   \n",
            "2466  It has been shown that adapting a dictionary o...   \n",
            "\n",
            "                                             paper_text  \n",
            "2012  Asymptotics of Gaussian Regularized\\nLeast-Squ...  \n",
            "2013  Two view learning: SVM-2K, Theory and\\nPractic...  \n",
            "2015  Saliency Based on Information Maximization\\n\\n...  \n",
            "2016  Faster Rates in Regression via Active Learning...  \n",
            "2017  Layered Dynamic Textures\\n\\nAntoni B. Chan\\nNu...  \n",
            "...                                                 ...  \n",
            "2462  Fitted Q-iteration in continuous action-space ...  \n",
            "2463  Topmoumoute online natural gradient algorithm\\...  \n",
            "2464  Sparse Overcomplete Latent Variable Decomposit...  \n",
            "2465  Second Order Bilinear Discriminant Analysis fo...  \n",
            "2466  Learning Horizontal Connections in a Sparse Co...  \n",
            "\n",
            "[628 rows x 7 columns], 7:         id       year                                              title  \\\n",
            "2810  3548 2008-01-01  Nonlinear causal discovery with additive noise...   \n",
            "2809  3547 2008-01-01  Goal-directed decision making in prefrontal co...   \n",
            "2808  3546 2008-01-01  Nonparametric Bayesian Learning of Switching L...   \n",
            "2807  3545 2008-01-01     Policy Search for Motor Primitives in Robotics   \n",
            "2806  3544 2008-01-01       Inferring rankings under constrained sensing   \n",
            "...    ...        ...                                                ...   \n",
            "3334  4019 2010-01-01  Two-Layer Generalization Analysis for Ranking ...   \n",
            "3444  4119 2010-01-01  b-Bit Minwise Hashing for Estimating Three-Way...   \n",
            "3336  4020 2010-01-01  Over-complete representations on recurrent neu...   \n",
            "3329  4014 2010-01-01       Agnostic Active Learning Without Constraints   \n",
            "3279  3970 2010-01-01  Sodium entry efficiency during action potentia...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "2810        NaN  3548-nonlinear-causal-discovery-with-additive-...   \n",
            "2809        NaN  3547-goal-directed-decision-making-in-prefront...   \n",
            "2808        NaN  3546-nonparametric-bayesian-learning-of-switch...   \n",
            "2807        NaN  3545-policy-search-for-motor-primitives-in-rob...   \n",
            "2806        NaN  3544-inferring-rankings-under-constrained-sens...   \n",
            "...         ...                                                ...   \n",
            "3334        NaN  4019-two-layer-generalization-analysis-for-ran...   \n",
            "3444        NaN  4119-b-bit-minwise-hashing-for-estimating-thre...   \n",
            "3336        NaN  4020-over-complete-representations-on-recurren...   \n",
            "3329        NaN  4014-agnostic-active-learning-without-constrai...   \n",
            "3279        NaN  3970-sodium-entry-efficiency-during-action-pot...   \n",
            "\n",
            "                                               abstract  \\\n",
            "2810  The discovery of causal relationships between ...   \n",
            "2809  Research in animal learning and behavioral neu...   \n",
            "2808  Many nonlinear dynamical phenomena can be effe...   \n",
            "2807  Many motor skills in humanoid robotics can be ...   \n",
            "2806                                   Abstract Missing   \n",
            "...                                                 ...   \n",
            "3334  This paper is concerned with the generalizatio...   \n",
            "3444  Computing two-way and multi-way set similariti...   \n",
            "3336  A striking aspect of cortical neural networks ...   \n",
            "3329  We present and analyze an agnostic active lear...   \n",
            "3279  Sodium entry during an action potential determ...   \n",
            "\n",
            "                                             paper_text  \n",
            "2810  Nonlinear causal discovery with additive noise...  \n",
            "2809  Goal-directed decision making in prefrontal\\nc...  \n",
            "2808  Nonparametric Bayesian Learning of Switching\\n...  \n",
            "2807  Policy Search for Motor Primitives in Robotics...  \n",
            "2806  Inferring rankings under constrained sensing\\n...  \n",
            "...                                                 ...  \n",
            "3334  Two-layer Generalization Analysis for Ranking ...  \n",
            "3444  b-Bit Minwise Hashing for Estimating Three-Way...  \n",
            "3336  Over-complete representations on recurrent neu...  \n",
            "3329  Agnostic Active Learning Without Constraints\\n...  \n",
            "3279  Sodium entry efficiency during action potentia...  \n",
            "\n",
            "[804 rows x 7 columns], 8:         id       year                                              title  \\\n",
            "3620  4278 2011-01-01  Learning in Hilbert vs. Banach Spaces: A Measu...   \n",
            "3798  4439 2011-01-01             Generalized Beta Mixtures of Gaussians   \n",
            "3744  4390 2011-01-01  Hogwild: A Lock-Free Approach to Parallelizing...   \n",
            "3742  4389 2011-01-01      An Exact Algorithm for F-Measure Maximization   \n",
            "3741  4388 2011-01-01                 Prediction strategies without loss   \n",
            "...    ...        ...                                                ...   \n",
            "4618  5179 2013-01-01  Learning Trajectory Preferences for  Manipulat...   \n",
            "4642  5200 2013-01-01         Non-Linear Domain Adaptation with Boosting   \n",
            "4638  5198 2013-01-01  Higher Order Priors for Joint Intrinsic Image,...   \n",
            "4643  5201 2013-01-01  Modeling Clutter Perception using Parametric P...   \n",
            "4516  5087 2013-01-01  Efficient Optimization for Sparse Gaussian Pro...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "3620        NaN  4278-learning-in-hilbert-vs-banach-spaces-a-me...   \n",
            "3798        NaN    4439-generalized-beta-mixtures-of-gaussians.pdf   \n",
            "3744        NaN  4390-hogwild-a-lock-free-approach-to-paralleli...   \n",
            "3742        NaN  4389-an-exact-algorithm-for-f-measure-maximiza...   \n",
            "3741        NaN        4388-prediction-strategies-without-loss.pdf   \n",
            "...         ...                                                ...   \n",
            "4618     Poster  5179-learning-trajectory-preferences-for-manip...   \n",
            "4642     Poster  5200-non-linear-domain-adaptation-with-boostin...   \n",
            "4638     Poster  5198-higher-order-priors-for-joint-intrinsic-i...   \n",
            "4643     Poster  5201-modeling-clutter-perception-using-paramet...   \n",
            "4516     Poster  5087-efficient-optimization-for-sparse-gaussia...   \n",
            "\n",
            "                                               abstract  \\\n",
            "3620  The goal of this paper is to investigate the a...   \n",
            "3798  In recent years, a rich variety of shrinkage p...   \n",
            "3744  Stochastic Gradient Descent (SGD) is a popular...   \n",
            "3742  The F-measure, originally introduced in inform...   \n",
            "3741  Consider a sequence of bits where we are tryin...   \n",
            "...                                                 ...   \n",
            "4618  We consider the problem of learning good traje...   \n",
            "4642  A common assumption in machine vision is that ...   \n",
            "4638  Many methods have been proposed to recover the...   \n",
            "4643  Visual clutter, the perception of an image as ...   \n",
            "4516  We propose an efficient discrete optimization ...   \n",
            "\n",
            "                                             paper_text  \n",
            "3620  Learning in Hilbert vs. Banach Spaces: A Measu...  \n",
            "3798  Generalized Beta Mixtures of Gaussians\\nArtin ...  \n",
            "3744  H OGWILD !: A Lock-Free Approach to Paralleliz...  \n",
            "3742  An Exact Algorithm for F-Measure Maximization\\...  \n",
            "3741  Prediction strategies without loss\\n\\nRina Pan...  \n",
            "...                                                 ...  \n",
            "4618  Learning Trajectory Preferences for Manipulato...  \n",
            "4642  Non-Linear Domain Adaptation with Boosting\\n\\n...  \n",
            "4638  Higher Order Priors for Joint Intrinsic Image,...  \n",
            "4643  Modeling Clutter Perception using Parametric\\n...  \n",
            "4516  Efficient Optimization for\\nSparse Gaussian Pr...  \n",
            "\n",
            "[1034 rows x 7 columns], 9:         id       year                                              title  \\\n",
            "4813  5358 2014-01-01  Probabilistic low-rank matrix completion on fi...   \n",
            "4805  5350 2014-01-01  Learning to Discover Efficient Mathematical Id...   \n",
            "4806  5351 2014-01-01  Searching for Higgs Boson Decay Modes with Dee...   \n",
            "4807  5352 2014-01-01  Semi-supervised Learning with Deep Generative ...   \n",
            "4808  5353 2014-01-01  Two-Stream Convolutional Networks for Action R...   \n",
            "...    ...        ...                                                ...   \n",
            "6042  6466 2016-01-01  Bayesian optimization for automated model sele...   \n",
            "6029  6454 2016-01-01  A Non-parametric Learning Method for Confident...   \n",
            "6041  6465 2016-01-01  R-FCN: Object Detection via Region-based Fully...   \n",
            "6040  6464 2016-01-01       Learning Deep Embeddings with Histogram Loss   \n",
            "6043  6467 2016-01-01  Generalization of ERM in Stochastic Convex Opt...   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "4813     Poster  5358-probabilistic-low-rank-matrix-completion-...   \n",
            "4805  Spotlight  5350-learning-to-discover-efficient-mathematic...   \n",
            "4806  Spotlight  5351-searching-for-higgs-boson-decay-modes-wit...   \n",
            "4807  Spotlight  5352-semi-supervised-learning-with-deep-genera...   \n",
            "4808  Spotlight  5353-two-stream-convolutional-networks-for-act...   \n",
            "...         ...                                                ...   \n",
            "6042     Poster  6466-bayesian-optimization-for-automated-model...   \n",
            "6029     Poster  6454-a-non-parametric-learning-method-for-conf...   \n",
            "6041     Poster  6465-r-fcn-object-detection-via-region-based-f...   \n",
            "6040     Poster  6464-learning-deep-embeddings-with-histogram-l...   \n",
            "6043     Poster  6467-generalization-of-erm-in-stochastic-conve...   \n",
            "\n",
            "                                               abstract  \\\n",
            "4813  The task of reconstructing a matrix given a sa...   \n",
            "4805  In this paper we explore how machine learning ...   \n",
            "4806  Particle colliders enable us to probe the fund...   \n",
            "4807  The ever-increasing size of modern data sets c...   \n",
            "4808  We investigate architectures of discriminative...   \n",
            "...                                                 ...   \n",
            "6042  Despite the success of kernel-based nonparamet...   \n",
            "6029  Estimating patient's clinical state from multi...   \n",
            "6041  We present region-based, fully convolutional n...   \n",
            "6040  We suggest a new loss for learning deep embedd...   \n",
            "6043  In stochastic convex optimization the goal is ...   \n",
            "\n",
            "                                             paper_text  \n",
            "4813  Probabilistic low-rank matrix completion on fi...  \n",
            "4805  Learning to Discover\\nEfficient Mathematical I...  \n",
            "4806  Searching for Higgs Boson Decay Modes\\nwith De...  \n",
            "4807  Semi-supervised Learning with\\nDeep Generative...  \n",
            "4808  Two-Stream Convolutional Networks\\nfor Action ...  \n",
            "...                                                 ...  \n",
            "6042  Bayesian optimization for automated model sele...  \n",
            "6029  A Non-parametric Learning Method for Confident...  \n",
            "6041  R-FCN: Object Detection via\\nRegion-based Full...  \n",
            "6040  Learning Deep Embeddings with Histogram Loss\\n...  \n",
            "6043  Generalization of ERM in Stochastic Convex\\nOp...  \n",
            "\n",
            "[1383 rows x 7 columns], 10:         id       year                                              title  \\\n",
            "6935  7273 2017-01-01  Houdini: Fooling Deep Structured Visual and Sp...   \n",
            "6933  7271 2017-01-01  Convergence of Gradient EM on Multi-component ...   \n",
            "6937  7275 2017-01-01  When Cyclic Coordinate Descent Outperforms Ran...   \n",
            "6936  7274 2017-01-01  Efficient and Flexible Inference for Stochasti...   \n",
            "6932  7270 2017-01-01  Kernel Feature Selection via Conditional Covar...   \n",
            "...    ...        ...                                                ...   \n",
            "6707  7067 2017-01-01             Context Selection for Embedding Models   \n",
            "6708  7068 2017-01-01  Working hard to know your neighbor's margins: ...   \n",
            "6709  7069 2017-01-01  Accelerated Stochastic Greedy Coordinate Desce...   \n",
            "6405  6794 2017-01-01  Consistent Multitask Learning with Nonlinear O...   \n",
            "6665  7029 2017-01-01                      Federated Multi-Task Learning   \n",
            "\n",
            "     event_type                                           pdf_name  \\\n",
            "6935     Poster  7273-houdini-fooling-deep-structured-visual-an...   \n",
            "6933     Poster  7271-convergence-of-gradient-em-on-multi-compo...   \n",
            "6937     Poster  7275-when-cyclic-coordinate-descent-outperform...   \n",
            "6936     Poster  7274-efficient-and-flexible-inference-for-stoc...   \n",
            "6932     Poster  7270-kernel-feature-selection-via-conditional-...   \n",
            "...         ...                                                ...   \n",
            "6707     Poster    7067-context-selection-for-embedding-models.pdf   \n",
            "6708     Poster  7068-working-hard-to-know-your-neighbors-margi...   \n",
            "6709     Poster  7069-accelerated-stochastic-greedy-coordinate-...   \n",
            "6405     Poster  6794-consistent-multitask-learning-with-nonlin...   \n",
            "6665     Poster             7029-federated-multi-task-learning.pdf   \n",
            "\n",
            "                                               abstract  \\\n",
            "6935  Generating adversarial examples is a critical ...   \n",
            "6933  In this paper, we study convergence properties...   \n",
            "6937  The coordinate descent (CD) method is a classi...   \n",
            "6936  Many real world dynamical systems are describe...   \n",
            "6932  We propose a method for feature selection that...   \n",
            "...                                                 ...   \n",
            "6707  Word embeddings are an effective tool to analy...   \n",
            "6708  We introduce a loss for metric learning, which...   \n",
            "6709  In this paper we study the well-known greedy c...   \n",
            "6405  Key to multitask learning is exploiting the re...   \n",
            "6665  Federated learning poses new statistical and s...   \n",
            "\n",
            "                                             paper_text  \n",
            "6935  Houdini: Fooling Deep Structured Visual and Sp...  \n",
            "6933  Convergence of Gradient EM on Multi-component\\...  \n",
            "6937  When Cyclic Coordinate Descent Outperforms\\nRa...  \n",
            "6936  Efficient and Flexible Inference for Stochasti...  \n",
            "6932  Kernel Feature Selection via\\nConditional Cova...  \n",
            "...                                                 ...  \n",
            "6707  Context Selection for Embedding Models\\n\\nLi-P...  \n",
            "6708  Working hard to know your neighbor?s margins:\\...  \n",
            "6709  Accelerated Stochastic Greedy Coordinate Desce...  \n",
            "6405  Consistent Multitask Learning with\\nNonlinear ...  \n",
            "6665  Federated Multi-Task Learning\\n\\nVirginia Smit...  \n",
            "\n",
            "[679 rows x 7 columns]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT word2phrase to create bigrams and unigrams\n",
        "!git clone https://github.com/travisbrady/word2phrase.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6I115Y--CqF",
        "outputId": "22aa9a89-199d-4cdc-c80e-3e7ce840c2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'word2phrase'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Total 93 (delta 0), reused 0 (delta 0), pack-reused 93\u001b[K\n",
            "Unpacking objects: 100% (93/93), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIRST TIME WINDOW SCI-BERT"
      ],
      "metadata": {
        "id": "ruZAthuVampe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONVERT First Time Window's Paper content into list\n",
        "\n",
        "paper_contents_list = nips_papers_partitions[0][\"paper_text\"].tolist()\n",
        "\n",
        "# Concatenate all the papers of the first time window\n",
        "#### MEASURE THE EXECUTION TIME FOR RUNNING THE CONCATENATION CODE\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "first_time_window_all_paper_content = \" \".join(paper_contents_list)[:20000]\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "\n",
        "len(first_time_window_all_paper_content)"
      ],
      "metadata": {
        "id": "u71QqekdD57T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee21944f-758f-48c1-be01-27159b9d4c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0031838417053222656\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# files.upload()\n"
      ],
      "metadata": {
        "id": "T5_Zz06aSxen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT and Download NLTK corpus dependencies\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "QsUTN4EOhhZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb0dfd2-df60-4957-b625-8ca47ffca121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocessing of papers: -? Stopwords removal...\n",
        "\n",
        "# papers_text_list = nips_papers_df[\"PaperText\"].tolist()\n",
        "\n",
        "# first_article_word_tokens = nltk.word_tokenize(papers_text_list[0])\n",
        "\n",
        "# stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "# cleaned_first_article_word_tokens = [w for w in first_article_word_tokens if w not in stopwords]\n",
        "\n",
        "# print(cleaned_first_article_word_tokens)\n",
        "# %load word2phrase/word2phrase.py\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# natural language processing: n-gram ranking\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "  \"\"\"\n",
        "  A simple function to clean up the data. All the words that\n",
        "  are not designated as a stop word is then lemmatized after\n",
        "  encoding and basic regex parsing are performed.\n",
        "  \"\"\"\n",
        "  wnl = nltk.stem.WordNetLemmatizer()\n",
        "  stopwords = nltk.corpus.stopwords.words('english') \n",
        "  text = (unicodedata.normalize('NFKD', text)\n",
        "    .encode('ascii', 'ignore')\n",
        "    .decode('utf-8', 'ignore')\n",
        "    .lower())\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "words = basic_clean(first_time_window_all_paper_content)\n",
        "\n",
        "\n",
        "# Get number similar items and their frequencies with value_count and convert it to dictionary\n",
        "bi_grams_unfiltered = pd.Series(nltk.ngrams(words, 2)).value_counts().to_dict()\n",
        "\n",
        "\n",
        "# Import Islice from Itertool for getting n-items from the dictionary\n",
        "from itertools import islice\n",
        "\n",
        "# Get list of Bi_grams Key-value pairs\n",
        "bi_grams_freq_list = list(bi_grams_unfiltered.items())[0:40]\n",
        "\n",
        "\n",
        "# Format Bi_grams using _ \n",
        "# Convert list of two dimensional tuples into single ones e.g ((neural,network), 2) -> (neural_network,2)\n",
        "\n",
        "formatted_bi_grams = [(\" \".join(bi_gram[0]), bi_gram[1])  for bi_gram in bi_grams_freq_list]\n",
        "formatted_bi_grams\n",
        "\n",
        "\n",
        "bi_gram_features = {}\n",
        "for tup in formatted_bi_grams:\n",
        "  bi_gram_features[tup[0]] = tup[1]\n",
        "\n",
        "bi_gram_features\n",
        "# top_freq_bi_grams = islice(bi_grams_freq_list,40)\n",
        "# list(top_freq_bi_grams)\n",
        "\n",
        "# for bi_gram in nltk.ngrams(words, 2):\n",
        "#     print(bi_gram)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jJiAuSAFHlaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63cce6ce-74bb-4814-ca4f-40f768f36b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'algorithm 3': 3,\n",
              " 'associative database': 13,\n",
              " 'autonomous mobile': 3,\n",
              " 'avoiding movement': 4,\n",
              " 'black point': 8,\n",
              " 'camera image': 6,\n",
              " 'define accordingly': 3,\n",
              " 'define following': 3,\n",
              " 'define x': 3,\n",
              " 'dx x': 8,\n",
              " 'e x': 9,\n",
              " 'estimation error': 6,\n",
              " 'even though': 3,\n",
              " 'fig 1': 4,\n",
              " 'fig 3': 4,\n",
              " 'fig 5': 5,\n",
              " 'follows algorithm': 3,\n",
              " 'image x': 6,\n",
              " 'learning machine': 9,\n",
              " 'letter recognition': 3,\n",
              " 'mobile robot': 3,\n",
              " 'n n': 5,\n",
              " 'neural network': 8,\n",
              " 'number sample': 5,\n",
              " 'observed sample': 4,\n",
              " 'obstacle avoiding': 4,\n",
              " 'parameter type': 4,\n",
              " 'position data': 4,\n",
              " 'position identification': 5,\n",
              " 'recognition rate': 5,\n",
              " 'recursive type': 4,\n",
              " 'selforganization associative': 5,\n",
              " 'structure f': 4,\n",
              " 'type learning': 3,\n",
              " 'window image': 4,\n",
              " 'x accordingly': 3,\n",
              " 'x define': 4,\n",
              " 'x e': 8,\n",
              " 'x x': 23,\n",
              " 'xn yin': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 : Pre Processing - Custom Stop Word And  Number Regex"
      ],
      "metadata": {
        "id": "PWlHBGUHPpnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "first_time_window_all_paper_content = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", first_time_window_all_paper_content)\n",
        "\n",
        "# Remove words with length less than 3 \n",
        "# https://stackoverflow.com/questions/24332025/remove-words-of-length-less-than-4-from-string\n",
        "\n",
        "first_time_window_all_paper_content = re.sub(r'\\b\\w{1,2}\\b', '',first_time_window_all_paper_content)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-B7n4F4qvfcH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Stopwords List for Scientific Literature \n",
        "\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "path_to_stop_words = '/gdrive/My Drive/Master_dataset/stopwords_10000_most_frequent_filtered.txt'\n",
        "\n",
        "with open(path_to_stop_words, \"r\") as file1:\n",
        "    FileasList = file1.readlines()\n",
        "\n",
        "\n",
        "stopwords = [s.strip('\\n') for s in FileasList]\n",
        "print(len(stopwords))\n",
        "\n",
        "\n",
        "scientific_literature_stopwords = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
        "\n",
        "len(scientific_literature_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KKRfopQCBbP",
        "outputId": "99b47f9d-7bf5-4733-86f1-412c7b53d481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9954\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9958"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Create Bag Of Unigrams and BIgrams"
      ],
      "metadata": {
        "id": "Y2nqQTAeP4fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 200 \n",
        "sent_tokens  = nltk.sent_tokenize(first_time_window_all_paper_content)"
      ],
      "metadata": {
        "id": "224P1qMMH-6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Calculate Bag Of Bi_grams\n",
        "\n",
        "vect = DictVectorizer()\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=scientific_literature_stopwords,ngram_range=(2,2), \n",
        "                             max_features=n_features,lowercase=True, max_df=0.6)\n",
        "# vectorizer_uni_grams = CountVectorizer(stop_words='english',ngram_range=(1,1), max_features=n_features)\n",
        "\n",
        "\n",
        "x_train = vectorizer.fit_transform(sent_tokens)\n",
        "\n",
        "df_bow_bi_grams = pd.DataFrame(x_train.toarray(),columns=vectorizer.get_feature_names())\n",
        "df_bow_bi_grams\n",
        "\n",
        "\n",
        "bigram_candidate_keywords = vectorizer.get_feature_names()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# df_bow_bi_grams = pd.DataFrame(x_train.toarray(),columns=vect.get_feature_names())\n",
        "# df_bow_bi_grams\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCnP8F42nL4h",
        "outputId": "44fc5eef-f262-4f4e-9e27-64e590abdc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['adj', 'ain', 'anglo', 'aren', 'cf', 'ch', 'col', 'cong', 'couldn', 'didn', 'doesn', 'false', 'fashioned', 'father', 'ff', 'friend', 'ft', 'gen', 'george', 'girl', 'god', 'hadn', 'hasn', 'haven', 'icel', 'imp', 'isn', 'king', 'lady', 'law', 'majesty', 'michael', 'mother', 'mustn', 'ne', 'needn', 'oe', 'pp', 'prof', 'shan', 'shouldn', 'son', 'sq', 'th', 've', 'viz', 'wasn', 'weren', 'wife', 'woman', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IIZJL8Rb_BqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Calculate Bag Of uni_grams\n",
        "\n",
        "\n",
        "vectorizer_uni = CountVectorizer(stop_words=scientific_literature_stopwords,ngram_range=(1,1), \n",
        "                             max_features=n_features,lowercase=True, min_df=1)\n",
        "# vectorizer_uni_grams = CountVectorizer(stop_words='english',ngram_range=(1,1), max_features=n_features)\n",
        "\n",
        "\n",
        "x_train_uni = vectorizer_uni.fit_transform(sent_tokens)\n",
        "\n",
        "df_bow_uni_grams = pd.DataFrame(x_train_uni.toarray(),columns=vectorizer_uni.get_feature_names())\n",
        "df_bow_uni_grams\n",
        "\n",
        "\n",
        "uni_gram_candidate_keywords = vectorizer_uni.get_feature_names()\n",
        "\n",
        "\n",
        "\n",
        "# df_bow_uni_grams = pd.DataFrame(x_train.toarray(),columns=vect.get_feature_names())\n",
        "# df_bow_uni_grams\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dSj0qIWz7dA",
        "outputId": "fd1417ad-ea01-4dc5-a616-5a7da7bf5b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['adj', 'ain', 'anglo', 'aren', 'cf', 'ch', 'col', 'cong', 'couldn', 'didn', 'doesn', 'false', 'fashioned', 'father', 'ff', 'friend', 'ft', 'gen', 'george', 'girl', 'god', 'hadn', 'hasn', 'haven', 'icel', 'imp', 'isn', 'king', 'lady', 'law', 'majesty', 'michael', 'mother', 'mustn', 'ne', 'needn', 'oe', 'pp', 'prof', 'shan', 'shouldn', 'son', 'sq', 'th', 've', 'viz', 'wasn', 'weren', 'wife', 'woman', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer_uni.vocabulary_"
      ],
      "metadata": {
        "id": "amCmZoh3_eKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_frequencies = x_train.toarray().sum(axis=0)\n",
        "\n",
        "\n",
        "bi_gram_keyword_frequency = {}\n",
        "\n",
        "for index, keyword in enumerate(vectorizer.get_feature_names()):\n",
        "  bi_gram_keyword_frequency[keyword]=feature_frequencies[index]\n",
        "\n",
        "\n",
        "feature_frequencies_uni = x_train_uni.toarray().sum(axis=0)\n",
        "\n",
        "print(feature_frequencies_uni)\n",
        "\n",
        "uni_gram_keyword_frequency = {}\n",
        "\n",
        "for index,keyword in enumerate(vectorizer_uni.get_feature_names()):\n",
        "  uni_gram_keyword_frequency[keyword]=feature_frequencies_uni[index]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import nltk\n",
        "# from nltk import word_tokenize\n",
        "# from nltk.util import ngrams\n",
        "# from collections import Counter\n",
        "\n",
        "\n",
        "# n_features = 200 \n",
        "# word_tokens  = nltk.word_tokenize(first_time_window_all_paper_content)\n",
        "\n",
        "# uni_grams = ngrams(word_tokens,1)\n",
        "# bi_grams = ngrams(word_tokens,2)\n",
        "\n",
        "# Counter(uni_grams)\n"
      ],
      "metadata": {
        "id": "5nkdPEY_FFyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824b722d-1633-436c-8e83-3898e0e17c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  1  2  1  1  1  1  1  1  1  2  1  2  8  3 13  3  2  3  1  1  1  1 11\n",
            "  3  1  1  1  1  2  2  1  3  1  2  1  3  1  1  3  1  1  3  1  2  2  1  1\n",
            "  3  1 12  7  1  1  1  1  2  2  1  2  1  3  2  2  3  2  2  2 21  4  2  3\n",
            "  3  2  2  2  2  2  2  5  2  2  2  8  4  2  1  2  2  1  1  1  1  2  1  3\n",
            "  2  2  6  8  1  6  1  1  1  2  2  1  1 10  3  1  2  2  1  1  4  4  2  1\n",
            "  1  1  1  2  2  6  1  2  1  2  1  1  1  1  1  4  3  1  4  1  1  1  1  1\n",
            "  3  1  1  1  1  2 21  1  1  1  1 13  1  1  1  1  1  2  1  1  1  4  1  1\n",
            "  3  1  1  1  1  2  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1  1  3  1  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join BOth Bag of Unigrams and Bigrams\n",
        "\n",
        "bag_of_candidate_keywords = {**bi_gram_keyword_frequency, **uni_gram_keyword_frequency}\n",
        "\n",
        "\n",
        "# Sort the bag of words\n",
        "candidate_keywords_bow = sorted(bag_of_candidate_keywords.items(), key=lambda item: item[1], reverse=True)\n",
        "len(candidate_keywords_bow)\n",
        "\n",
        "candidate_keywords_bow"
      ],
      "metadata": {
        "id": "VqQAi3yqcLV2",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c5a75d-c7da-4255-eea2-9c63e59a988c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fig', 21),\n",
              " ('robot', 21),\n",
              " ('associative', 13),\n",
              " ('samples', 13),\n",
              " ('database', 12),\n",
              " ('camera', 11),\n",
              " ('operator', 10),\n",
              " ('associative database', 8),\n",
              " ('algorithm', 8),\n",
              " ('letter', 8),\n",
              " ('neural', 8),\n",
              " ('databases', 7),\n",
              " ('neural networks', 6),\n",
              " ('networks', 6),\n",
              " ('nodes', 6),\n",
              " ('problem', 6),\n",
              " ('associative databases', 5),\n",
              " ('increases', 5),\n",
              " ('yin', 5),\n",
              " ('finite', 4),\n",
              " ('levels', 4),\n",
              " ('parameter', 4),\n",
              " ('parameters', 4),\n",
              " ('recognizer', 4),\n",
              " ('recursive', 4),\n",
              " ('structures', 4),\n",
              " ('autonomous mobile', 3),\n",
              " ('constructing associative', 3),\n",
              " ('mobile robot', 3),\n",
              " ('samples increases', 3),\n",
              " ('approaches', 3),\n",
              " ('assumed', 3),\n",
              " ('autonomous', 3),\n",
              " ('candidates', 3),\n",
              " ('compression', 3),\n",
              " ('computes', 3),\n",
              " ('constructing', 3),\n",
              " ('contrarily', 3),\n",
              " ('criterion', 3),\n",
              " ('dimension', 3),\n",
              " ('english', 3),\n",
              " ('frequency', 3),\n",
              " ('furthermore', 3),\n",
              " ('mobile', 3),\n",
              " ('organizing', 3),\n",
              " ('recognizing', 3),\n",
              " ('reset', 3),\n",
              " ('subject', 3),\n",
              " ('teaches', 3),\n",
              " ('white', 3),\n",
              " ('approaches samples', 2),\n",
              " ('criterion operator', 2),\n",
              " ('databases robot', 2),\n",
              " ('head letter', 2),\n",
              " ('input output', 2),\n",
              " ('operator teaches', 2),\n",
              " ('organizing associative', 2),\n",
              " ('robot eyesight', 2),\n",
              " ('subject constructing', 2),\n",
              " ('white fig', 2),\n",
              " ('yin yin', 2),\n",
              " ('18m', 2),\n",
              " ('400', 2),\n",
              " ('800', 2),\n",
              " ('attains', 2),\n",
              " ('coefficients', 2),\n",
              " ('compactness', 2),\n",
              " ('computation', 2),\n",
              " ('converged', 2),\n",
              " ('converges', 2),\n",
              " ('defines', 2),\n",
              " ('denote', 2),\n",
              " ('designing', 2),\n",
              " ('dissimilari', 2),\n",
              " ('economize', 2),\n",
              " ('eternally', 2),\n",
              " ('euclidean', 2),\n",
              " ('eyesight', 2),\n",
              " ('free', 2),\n",
              " ('global', 2),\n",
              " ('hand', 2),\n",
              " ('handwritten', 2),\n",
              " ('head', 2),\n",
              " ('implicitly', 2),\n",
              " ('incorrect', 2),\n",
              " ('input', 2),\n",
              " ('juts', 2),\n",
              " ('learns', 2),\n",
              " ('luminance', 2),\n",
              " ('mapping', 2),\n",
              " ('mappings', 2),\n",
              " ('min', 2),\n",
              " ('nand', 2),\n",
              " ('negligible', 2),\n",
              " ('observes', 2),\n",
              " ('obstacles', 2),\n",
              " ('osaka', 2),\n",
              " ('output', 2),\n",
              " ('passageway', 2),\n",
              " ('preliminarily', 2),\n",
              " ('preprocessing', 2),\n",
              " ('processed', 2),\n",
              " ('randomly', 2),\n",
              " ('roan', 2),\n",
              " ('slant', 2),\n",
              " ('summation', 2),\n",
              " ('100 distinguishable', 1),\n",
              " ('400 levels', 1),\n",
              " ('560 organizing', 1),\n",
              " ('800 nodes', 1),\n",
              " ('accomplishes recursively', 1),\n",
              " ('blem arises', 1),\n",
              " ('brushes obstacles', 1),\n",
              " ('bstitu white', 1),\n",
              " ('builds organizing', 1),\n",
              " ('camera 55deg', 1),\n",
              " ('camera autonomous', 1),\n",
              " ('camera face', 1),\n",
              " ('camera inquires', 1),\n",
              " ('camera loosened', 1),\n",
              " ('camera panorama', 1),\n",
              " ('camera preliminarily', 1),\n",
              " ('camera preprocessing', 1),\n",
              " ('candidates assumed', 1),\n",
              " ('candidates mappings', 1),\n",
              " ('candidates subset', 1),\n",
              " ('catches letter', 1),\n",
              " ('coefficients anza', 1),\n",
              " ('coefficients samples', 1),\n",
              " ('compactness almighty', 1),\n",
              " ('compactness assumed', 1),\n",
              " ('compress camera', 1),\n",
              " ('compression admittable', 1),\n",
              " ('compression camera', 1),\n",
              " ('compu ters', 1),\n",
              " ('computation proportionally', 1),\n",
              " ('compute coefficients', 1),\n",
              " ('computes parameters', 1),\n",
              " ('computing coefficients', 1),\n",
              " ('configuration autonomous', 1),\n",
              " ('constructions associative', 1),\n",
              " ('contrarily associative', 1),\n",
              " ('contrarily neural', 1),\n",
              " ('contrarily parameter', 1),\n",
              " ('controls 3km', 1),\n",
              " ('converged 400', 1),\n",
              " ('converges samples', 1),\n",
              " ('corresponds fig', 1),\n",
              " ('laboratories fig', 1),\n",
              " ('learns input', 1),\n",
              " ('learns landscapes', 1),\n",
              " ('letter autonomous', 1),\n",
              " ('letter fig', 1),\n",
              " ('letter letter', 1),\n",
              " ('letter recen', 1),\n",
              " ('letter recognizer', 1),\n",
              " ('letters3 matching', 1),\n",
              " ('letters4 global', 1),\n",
              " ('levels 202', 1),\n",
              " ('levels converged', 1),\n",
              " ('levels oftree', 1),\n",
              " ('loo fig', 1),\n",
              " ('luminance white', 1),\n",
              " ('mapping reals', 1),\n",
              " ('matching recognizing', 1),\n",
              " ('maximal letter', 1),\n",
              " ('methodologies problem', 1),\n",
              " ('minimization problem', 1),\n",
              " ('nand nodes', 1),\n",
              " ('networks accelerate', 1),\n",
              " ('networks approximations', 1),\n",
              " ('networks eternally', 1),\n",
              " ('networks recursi', 1),\n",
              " ('networks structures', 1),\n",
              " ('neural randomly', 1),\n",
              " ('nodes 145', 1),\n",
              " ('nodes frequency', 1),\n",
              " ('nodes increases', 1),\n",
              " ('nodes memorize', 1),\n",
              " ('notation candidates', 1),\n",
              " ('notations nand', 1),\n",
              " ('nualber sampl', 1),\n",
              " ('observes rewrites', 1),\n",
              " ('observes sampled', 1),\n",
              " ('obstacles operator', 1),\n",
              " ('oftree 522', 1),\n",
              " ('operator brushes', 1),\n",
              " ('operator obstacles', 1),\n",
              " ('operator subjectively', 1),\n",
              " ('orthogonal dimension', 1),\n",
              " ('osaka 560', 1),\n",
              " ('osaka toyonaka', 1),\n",
              " ('panorama 360deg', 1),\n",
              " ('parallelogram maximal', 1),\n",
              " ('parameter assumes', 1),\n",
              " ('parameter compactness', 1),\n",
              " ('parameter recursive', 1),\n",
              " ('parameters samples', 1),\n",
              " ('parameters uniquely', 1),\n",
              " ('passageway juts', 1),\n",
              " ('passageways 18m', 1),\n",
              " ('periodically operator', 1),\n",
              " ('physics designing', 1),\n",
              " ('pointer arrives', 1),\n",
              " ('preprocessing ion', 1),\n",
              " ('problem compression', 1),\n",
              " ('problem criterion', 1),\n",
              " ('problem designing', 1),\n",
              " ('problem identifying', 1),\n",
              " ('problem minimization', 1),\n",
              " ('problem wide', 1),\n",
              " ('procedures compute', 1),\n",
              " ('randomly algorithm', 1),\n",
              " ('realizes handwritten', 1),\n",
              " ('recalling camera', 1),\n",
              " ('recen tly', 1),\n",
              " ('recognizer accomplishes', 1),\n",
              " ('recognizer interaction', 1),\n",
              " ('recognizer parallelogram', 1),\n",
              " ('recognizer scans', 1),\n",
              " ('recognizing hand', 1),\n",
              " ('recognizing styles', 1),\n",
              " ('recognizing typographic', 1),\n",
              " ('recursi recursive', 1),\n",
              " ('recursive effectiveness', 1),\n",
              " ('recursive guarantees', 1),\n",
              " ('recursive methodology', 1),\n",
              " ('recursive observes', 1),\n",
              " ('reflects operator', 1),\n",
              " ('relating parameters', 1),\n",
              " ('reproduces criterion', 1),\n",
              " ('reset periodically', 1),\n",
              " ('reset pointer', 1),\n",
              " ('rewri ting', 1),\n",
              " ('rewrites correlated', 1),\n",
              " ('rewriting free', 1),\n",
              " ('rmbi robot', 1),\n",
              " ('roan fig', 1),\n",
              " ('roan uni', 1),\n",
              " ('robot 30kg', 1),\n",
              " ('robot arbitrarily', 1),\n",
              " ('robot camera', 1),\n",
              " ('robot database', 1),\n",
              " ('robot flourishingly6', 1),\n",
              " ('robot identifies', 1),\n",
              " ('robot operator', 1),\n",
              " ('robot recalling', 1),\n",
              " ('robot reset', 1),\n",
              " ('robot roan', 1),\n",
              " ('robot superimposes', 1),\n",
              " ('robot white', 1),\n",
              " ('sampl fig', 1),\n",
              " ('sampled randomly', 1),\n",
              " ('samples computes', 1),\n",
              " ('samples likelihood', 1),\n",
              " ('samples nualber', 1),\n",
              " ('scanner fig', 1),\n",
              " ('scans slant', 1),\n",
              " ('simplest constructions', 1),\n",
              " ('slant angular', 1),\n",
              " ('slitting recognizing', 1),\n",
              " ('speciiications 20dot', 1),\n",
              " ('stools handcarts', 1),\n",
              " ('structures learns', 1),\n",
              " ('subclass structures', 1),\n",
              " ('subset mappings', 1),\n",
              " ('suffices camera', 1),\n",
              " ('suguru arimoto', 1),\n",
              " ('summation divided', 1),\n",
              " ('superimposes camera', 1),\n",
              " ('suzuki suguru', 1),\n",
              " ('synapses branching', 1),\n",
              " ('teaches recognizer', 1),\n",
              " ('terminal node', 1),\n",
              " ('tion defines', 1),\n",
              " ('tly slitting', 1),\n",
              " ('toyonaka osaka', 1),\n",
              " ('trajectory blem', 1),\n",
              " ('triangular technique', 1),\n",
              " ('truncating finitely', 1),\n",
              " ('typographic english', 1),\n",
              " ('uni fig', 1),\n",
              " ('uniquely samples', 1),\n",
              " ('units computing', 1),\n",
              " ('universality neural', 1),\n",
              " ('variables nodes', 1),\n",
              " ('vertically filtered', 1),\n",
              " ('visual camera', 1),\n",
              " ('10dot', 1),\n",
              " ('145', 1),\n",
              " ('202', 1),\n",
              " ('20dot', 1),\n",
              " ('28deg', 1),\n",
              " ('30deg', 1),\n",
              " ('360deg', 1),\n",
              " ('360dot', 1),\n",
              " ('3km', 1),\n",
              " ('522', 1),\n",
              " ('blem', 1),\n",
              " ('brushes', 1),\n",
              " ('bstitu', 1),\n",
              " ('builds', 1),\n",
              " ('cartesian', 1),\n",
              " ('catches', 1),\n",
              " ('category', 1),\n",
              " ('clarify', 1),\n",
              " ('compress', 1),\n",
              " ('compu', 1),\n",
              " ('compute', 1),\n",
              " ('computing', 1),\n",
              " ('configuration', 1),\n",
              " ('constructions', 1),\n",
              " ('contradicts', 1),\n",
              " ('controls', 1),\n",
              " ('correlated', 1),\n",
              " ('corresponds', 1),\n",
              " ('cylinders', 1),\n",
              " ('decomposes', 1),\n",
              " ('decrease', 1),\n",
              " ('decreases', 1),\n",
              " ('definable', 1),\n",
              " ('denoting', 1),\n",
              " ('differs', 1),\n",
              " ('manages', 1),\n",
              " ('maximal', 1),\n",
              " ('memorize', 1),\n",
              " ('methodologies', 1),\n",
              " ('methodology', 1),\n",
              " ('minimization', 1),\n",
              " ('node', 1),\n",
              " ('notation', 1),\n",
              " ('notations', 1),\n",
              " ('nualber', 1),\n",
              " ('odyssey', 1),\n",
              " ('oftree', 1),\n",
              " ('orthogonal', 1),\n",
              " ('panorama', 1),\n",
              " ('parallelogram', 1),\n",
              " ('passageways', 1),\n",
              " ('periodically', 1),\n",
              " ('physics', 1),\n",
              " ('pointer', 1),\n",
              " ('procedures', 1),\n",
              " ('proportionally', 1),\n",
              " ('realizes', 1),\n",
              " ('reals', 1),\n",
              " ('recalling', 1),\n",
              " ('recalls', 1),\n",
              " ('recen', 1),\n",
              " ('recursi', 1),\n",
              " ('recursively', 1),\n",
              " ('reflects', 1),\n",
              " ('relating', 1),\n",
              " ('reproduces', 1),\n",
              " ('researches', 1),\n",
              " ('rewri', 1),\n",
              " ('rewrites', 1),\n",
              " ('rewriting', 1),\n",
              " ('rmbi', 1),\n",
              " ('roles', 1),\n",
              " ('routes', 1),\n",
              " ('sampl', 1),\n",
              " ('sampled', 1),\n",
              " ('scanner', 1),\n",
              " ('scans', 1),\n",
              " ('sections', 1),\n",
              " ('shifting', 1),\n",
              " ('simplest', 1),\n",
              " ('slitting', 1),\n",
              " ('speciiications', 1),\n",
              " ('stools', 1),\n",
              " ('styles', 1),\n",
              " ('subclass', 1),\n",
              " ('subjectively', 1),\n",
              " ('subset', 1),\n",
              " ('suffices', 1),\n",
              " ('suguru', 1),\n",
              " ('superimposes', 1),\n",
              " ('supplementary', 1),\n",
              " ('suzuki', 1),\n",
              " ('synapses', 1),\n",
              " ('technique', 1),\n",
              " ('terminal', 1),\n",
              " ('ters', 1),\n",
              " ('ting', 1),\n",
              " ('tion', 1),\n",
              " ('tly', 1),\n",
              " ('toyonaka', 1),\n",
              " ('trajectory', 1),\n",
              " ('triangular', 1),\n",
              " ('truncating', 1),\n",
              " ('typographic', 1),\n",
              " ('uni', 1),\n",
              " ('uniquely', 1),\n",
              " ('units', 1),\n",
              " ('universality', 1),\n",
              " ('variables', 1),\n",
              " ('vertically', 1),\n",
              " ('visual', 1),\n",
              " ('wide', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_keywords_bow = candidate_keywords_bow[:100]\n",
        "candidate_keywords_bow = [keyword[0] for keyword in candidate_keywords_bow] \n",
        "candidate_keywords_bow"
      ],
      "metadata": {
        "id": "rVxb4vzfzi9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f65b1b0-db29-4902-9860-641be10cafeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fig',\n",
              " 'robot',\n",
              " 'associative',\n",
              " 'samples',\n",
              " 'database',\n",
              " 'camera',\n",
              " 'operator',\n",
              " 'associative database',\n",
              " 'algorithm',\n",
              " 'letter',\n",
              " 'neural',\n",
              " 'databases',\n",
              " 'neural networks',\n",
              " 'networks',\n",
              " 'nodes',\n",
              " 'problem',\n",
              " 'associative databases',\n",
              " 'increases',\n",
              " 'yin',\n",
              " 'finite',\n",
              " 'levels',\n",
              " 'parameter',\n",
              " 'parameters',\n",
              " 'recognizer',\n",
              " 'recursive',\n",
              " 'structures',\n",
              " 'autonomous mobile',\n",
              " 'constructing associative',\n",
              " 'mobile robot',\n",
              " 'samples increases',\n",
              " 'approaches',\n",
              " 'assumed',\n",
              " 'autonomous',\n",
              " 'candidates',\n",
              " 'compression',\n",
              " 'computes',\n",
              " 'constructing',\n",
              " 'contrarily',\n",
              " 'criterion',\n",
              " 'dimension',\n",
              " 'english',\n",
              " 'frequency',\n",
              " 'furthermore',\n",
              " 'mobile',\n",
              " 'organizing',\n",
              " 'recognizing',\n",
              " 'reset',\n",
              " 'subject',\n",
              " 'teaches',\n",
              " 'white',\n",
              " 'approaches samples',\n",
              " 'criterion operator',\n",
              " 'databases robot',\n",
              " 'head letter',\n",
              " 'input output',\n",
              " 'operator teaches',\n",
              " 'organizing associative',\n",
              " 'robot eyesight',\n",
              " 'subject constructing',\n",
              " 'white fig',\n",
              " 'yin yin',\n",
              " '18m',\n",
              " '400',\n",
              " '800',\n",
              " 'attains',\n",
              " 'coefficients',\n",
              " 'compactness',\n",
              " 'computation',\n",
              " 'converged',\n",
              " 'converges',\n",
              " 'defines',\n",
              " 'denote',\n",
              " 'designing',\n",
              " 'dissimilari',\n",
              " 'economize',\n",
              " 'eternally',\n",
              " 'euclidean',\n",
              " 'eyesight',\n",
              " 'free',\n",
              " 'global',\n",
              " 'hand',\n",
              " 'handwritten',\n",
              " 'head',\n",
              " 'implicitly',\n",
              " 'incorrect',\n",
              " 'input',\n",
              " 'juts',\n",
              " 'learns',\n",
              " 'luminance',\n",
              " 'mapping',\n",
              " 'mappings',\n",
              " 'min',\n",
              " 'nand',\n",
              " 'negligible',\n",
              " 'observes',\n",
              " 'obstacles',\n",
              " 'osaka',\n",
              " 'output',\n",
              " 'passageway',\n",
              " 'preliminarily']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 : After Generating BOW_BiGrams, Next Step is to generate BERT based Embedding\n",
        "\n",
        "Install Pretrained interface of pytorch for BERT\n"
      ],
      "metadata": {
        "id": "rnneo_nxcRUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert transformers"
      ],
      "metadata": {
        "id": "kQLrNkRQjoh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae406f0-996e-4ad9-e8ee-7f504e19a462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 29.4 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.0+cu113)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.24.42-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 65.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.28.0,>=1.27.42\n",
            "  Downloading botocore-1.27.42-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 48.6 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.42->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.11-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.42->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, pyyaml, tokenizers, huggingface-hub, boto3, transformers, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.24.42 botocore-1.27.42 huggingface-hub-0.8.1 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 pyyaml-6.0 s3transfer-0.6.0 tokenizers-0.12.1 transformers-4.21.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries for BERT based embedding\n",
        " \n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "q78_XxWUxHlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090dc2d7-f5c5-4580-bc0a-b92bad2a39ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 22119798.86B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.1 - Sample Test for Using BERT is Performed Below // Expand To see guide on Generating BERT embeddings For Sentences "
      ],
      "metadata": {
        "id": "3zj2dWrVs5uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "example_text = 'I will watch Memento tonight'\n",
        "\n",
        "bert_input = tokenizer(example_text,padding='max_length', max_length = 10, \n",
        "                       truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(bert_input['input_ids'])\n",
        "print(bert_input['token_type_ids'])\n",
        "print(bert_input['attention_mask'])"
      ],
      "metadata": {
        "id": "PcwU_PlaygA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "aa82f1d10de642ee8161d83f3dda75b4",
            "fb0885d2a17947c59b0d0fd314b6a9ec",
            "524d189ce12a4e99aa82ee66c4fb120b",
            "ad47c4328e924105a079ece64cf4f516",
            "b1e8127dddb84cb5af98f5be435ad383",
            "fe97cb8dd28a413483b9cb5b3419afca",
            "432ad82ebfb34feab079fe7d01b54c16",
            "8246d9e2e3994394b91ccb25d575df53",
            "42c5c57e45904c9da095ba393a39235e",
            "3f22bf4f3374418e941b29656194afba",
            "098ab108ac6144b6b5b97fa26da623ff",
            "3f5c68a238c8439aa4c90e5ad70cc49f",
            "07cec6a0f6104585a3a0ef6132dc111a",
            "43d8180cbccf4669ae641f0ec0fab2ea",
            "c976d0466b3040ffba10f5a73f6649bd",
            "122773b107f44807a5aac585e7078753",
            "637330d2eb3b489c9340cd3df814a675",
            "01f5a80eac44443ba12acb906c04d0f5",
            "e93e3358dfde48a6a0199b431627cf51",
            "711cb7a393ed40e1975f87f365fb71d2",
            "d937c5e97fba40db9b3cc5f52ff8f13a",
            "50c1aebdd45644be99023b7ae5901113",
            "f35c56dce3da4e0780e05c5fb3b9e8c0",
            "89dc70a500e74ddb9c88a4fd78fed6e8",
            "a487d01998584642a524600d1a6e4e50",
            "e125a1a12cb04417b95c8e6e9f186966",
            "961cc23f79e54c68aec8916bb968093d",
            "94f743cdda084ef8bd90b6c74ce47602",
            "56ac5c85f4e240cf854bee8ac1880a73",
            "d6424c4632ca402c8f7b6d74c551efca",
            "66148edba41346fdbe2e5991762419cd",
            "343c2abf26e840aa9cfe9ebc24cef8ad",
            "eba4033583f84f00a021a92a81fc515a"
          ]
        },
        "outputId": "4a2a3081-0012-4f05-856f-8de34af9c57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa82f1d10de642ee8161d83f3dda75b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f5c68a238c8439aa4c90e5ad70cc49f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f35c56dce3da4e0780e05c5fb3b9e8c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1045,  2097,  3422,  2033, 23065,  3892,   102,     0,     0]])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_input)\n",
        "tokenizer.decode(bert_input.input_ids[0])"
      ],
      "metadata": {
        "id": "JovU5dyXIwg1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "3bf3d1be-26ba-4c6a-de3e-bfff8dfeb26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  2097,  3422,  2033, 23065,  3892,   102,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] i will watch memento tonight [SEP] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_input = tokenizer.encode_plus([\"Some random sentence 1\", \"Some random sentence 2\"])\n",
        "tokenizer.decode(bert_input.input_ids)\n",
        "bert_input"
      ],
      "metadata": {
        "id": "jK4A0In0WiJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534af676-18b9-483f-c4a4-bee79a69e768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 100, 100, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create A Utility Class for generating Bert Based Embeddding for the every sentence\n",
        "\n",
        "sentence_tokens_first_time_window = sent_tokens\n",
        "bert_tokens_first_time_window = [tokenizer(text,  padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in sentence_tokens_first_time_window]\n",
        "\n"
      ],
      "metadata": {
        "id": "wNKiaMhjZYsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bert_tokens_test = bert_tokens_first_time_window[:10]\n",
        "\n",
        "decoded_sentences = [tokenizer.decode(bert_token.input_ids[0]) for bert_token in bert_tokens_test]\n",
        "decoded_sentences"
      ],
      "metadata": {
        "id": "r2dDvNkYiGT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a617f3-02e6-4b29-d55d-c3f897c6a26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] self - organization associative database and its applications hisashi suzuki and suguru arimoto osaka university, toyonaka, osaka 560, japan abstract efficient method self - organizing associative databases proposed together with applications robot eyesight systems. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] the proposed databases can associate any input with some output. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] the first half part discussion, algorithm self - organization proposed. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] from aspect hardware, produces new style neural network. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] the latter half part, applicability handwritten letter recognition and that autonomous mobile robot system are demonstrated. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] introduction let mapping : - + given. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] here, finite infinite set, and another finite infinite set. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] learning machine observes any set pairs (, ) sampled randomly from. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] ( means the cartesian product and. ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
              " '[CLS] and, computes some estimate : - + make small, the estimation error some measure. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative 2 : Generating BERt Embeddings using BERT Encode method\n",
        "\n",
        "bert_input_ids = tokenizer.batch_encode_plus(sentence_tokens_first_time_window, return_token_type_ids=False, return_attention_mask=False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tHQlz5oBjY6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_input_ids.input_ids[0:10]"
      ],
      "metadata": {
        "id": "6N_nE1ZSkoR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e67825-1f85-44d2-b454-d67ae78c6ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[101,\n",
              "  2969,\n",
              "  1011,\n",
              "  3029,\n",
              "  4632,\n",
              "  10085,\n",
              "  2401,\n",
              "  6024,\n",
              "  7809,\n",
              "  1998,\n",
              "  2049,\n",
              "  5097,\n",
              "  2010,\n",
              "  12914,\n",
              "  14278,\n",
              "  1998,\n",
              "  10514,\n",
              "  27390,\n",
              "  2226,\n",
              "  10488,\n",
              "  15319,\n",
              "  13000,\n",
              "  2118,\n",
              "  1010,\n",
              "  9121,\n",
              "  7856,\n",
              "  2912,\n",
              "  1010,\n",
              "  13000,\n",
              "  21267,\n",
              "  1010,\n",
              "  2900,\n",
              "  10061,\n",
              "  8114,\n",
              "  4118,\n",
              "  2969,\n",
              "  1011,\n",
              "  10863,\n",
              "  4632,\n",
              "  10085,\n",
              "  2401,\n",
              "  6024,\n",
              "  17881,\n",
              "  3818,\n",
              "  2362,\n",
              "  2007,\n",
              "  5097,\n",
              "  8957,\n",
              "  2159,\n",
              "  18743,\n",
              "  3001,\n",
              "  1012,\n",
              "  102],\n",
              " [101, 1996, 3818, 17881, 2064, 5482, 2151, 7953, 2007, 2070, 6434, 1012, 102],\n",
              " [101,\n",
              "  1996,\n",
              "  2034,\n",
              "  2431,\n",
              "  2112,\n",
              "  6594,\n",
              "  1010,\n",
              "  9896,\n",
              "  2969,\n",
              "  1011,\n",
              "  3029,\n",
              "  3818,\n",
              "  1012,\n",
              "  102],\n",
              " [101, 2013, 7814, 8051, 1010, 7137, 2047, 2806, 15756, 2897, 1012, 102],\n",
              " [101,\n",
              "  1996,\n",
              "  3732,\n",
              "  2431,\n",
              "  2112,\n",
              "  1010,\n",
              "  10439,\n",
              "  19341,\n",
              "  8553,\n",
              "  2192,\n",
              "  15773,\n",
              "  3661,\n",
              "  5038,\n",
              "  1998,\n",
              "  2008,\n",
              "  8392,\n",
              "  4684,\n",
              "  8957,\n",
              "  2291,\n",
              "  2024,\n",
              "  7645,\n",
              "  1012,\n",
              "  102],\n",
              " [101, 4955, 2292, 12375, 1024, 1011, 1009, 2445, 1012, 102],\n",
              " [101,\n",
              "  2182,\n",
              "  1010,\n",
              "  10713,\n",
              "  10709,\n",
              "  2275,\n",
              "  1010,\n",
              "  1998,\n",
              "  2178,\n",
              "  10713,\n",
              "  10709,\n",
              "  2275,\n",
              "  1012,\n",
              "  102],\n",
              " [101,\n",
              "  4083,\n",
              "  3698,\n",
              "  24451,\n",
              "  2151,\n",
              "  2275,\n",
              "  7689,\n",
              "  1006,\n",
              "  1010,\n",
              "  1007,\n",
              "  18925,\n",
              "  18154,\n",
              "  2013,\n",
              "  1012,\n",
              "  102],\n",
              " [101, 1006, 2965, 1996, 11122, 25253, 4031, 1998, 1012, 1007, 102],\n",
              " [101,\n",
              "  1998,\n",
              "  1010,\n",
              "  24134,\n",
              "  2015,\n",
              "  2070,\n",
              "  10197,\n",
              "  1024,\n",
              "  1011,\n",
              "  1009,\n",
              "  2191,\n",
              "  2235,\n",
              "  1010,\n",
              "  1996,\n",
              "  24155,\n",
              "  7561,\n",
              "  2070,\n",
              "  5468,\n",
              "  1012,\n",
              "  102]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Cosine Similarity\n",
        "import numpy as np\n",
        "\n",
        "bert_tes =  [ bert_token.input_ids for bert_token in bert_tokens_test]\n",
        "# np.dot(bert_tes[0], bert_tes[1])\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pairwise_similarities=cosine_similarity(bert_tes[0], bert_tes[0])\n",
        "pairwise_similarities\n",
        "\n",
        "\n",
        "# bert_tes[0][0]"
      ],
      "metadata": {
        "id": "rxWXLjOknHA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea88195-7085-4975-dda6-2b7556b63cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST 1 - Creating BERT Embedding For NIPS paper and Calculate Cosine Similarity"
      ],
      "metadata": {
        "id": "Pn5qVbd2t4B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "3SlZJLBZuCA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c6db19-8b54-4f98-d013-db49545860ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=9c404046861ab8edc9074ebb6ea25d8b975931349345e06c9ae39a70e611604c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
        "bert_sentence_embedding_first_time_window = model.encode(sent_tokens[:100])\n",
        "\n",
        "keyword_feaature_bert_embedding = model.encode(bigram_candidate_keywords)\n"
      ],
      "metadata": {
        "id": "JFSP_xBuujhS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "beafc5cb688845ca92edb3ab7232462e",
            "57a44348c386411bb087776f99340e0b",
            "e9b1295c659b4226af0f2e00f7ab22d0",
            "6d102b860cac494c9331c621ec0dd692",
            "a9322d049b1b4debb94b1039bb3df086",
            "e802c0dca4e44004ae8f7e0cb4d9d95d",
            "b3901258f5774aeeb49fc661f6b50dcd",
            "2c32bd93d4d64756bbfd67f6fef839d2",
            "7000910bdf7b4923963667e0a64eb33d",
            "2b6e090509644b2f91bc8a6dcc30ed52",
            "adb286147cbb4e20a72a0d5531dcc69d",
            "ac1bdab52af64bbdac8e0175aa4f76a2",
            "4fa9ee568aa74769b9f98e9ccd8a340c",
            "46be22f8739341129348c28c326c7da9",
            "00c69415680f4f42829c9a254c1d9da3",
            "a5afdb81c00e44e4bbd2c71c62eb237d",
            "a7789575d8ff4ee8879b46ff54219fb1",
            "70fd25488e4f4620bdfe70d02956245d",
            "1ecdbd62a6454ed9acb365cf44e134b7",
            "a0b8214ba91d432b9cef0f05a2cf0c16",
            "a507269683ff4151aa59beb35cedd904",
            "86b42d7f54ae4f3c898c7e444d4f3d23",
            "2664ccfd15cf4d298b0cf76868cc0328",
            "93c4c6a4dccc4bfdba382824c1884593",
            "f18162ba9855423f80b6f73e5347c2bc",
            "ddb0ec12a15841eca26262061dddcb75",
            "6098cdddb9af47559517ba726f179b49",
            "34005abca2054b968de23f6f9dce7b8f",
            "40229e67f1f94afd935dd21266d58515",
            "b26a1e145085457587d62ab061ba2e1b",
            "56c45ccea737479990cda8b5fd02e51c",
            "86d0c7c8a2d342928eb2c179b74b74da",
            "e0ddb77b501544d2a2f8ba7f90b13fa1",
            "cdd438862057434c92896ef2fb2aab25",
            "11032e95e1cc4a568ee3fcbf693d9bf8",
            "eb5e1d9fcb0f4ee38a8f04b58774d732",
            "8b55459f30a741238f73f64906254b8a",
            "a0f701f2a9aa4ee8a7993f44b25a2a69",
            "6df0e679c06b4861a53fc5633dabd05c",
            "457a7be2a91f4688837bba6fb4422642",
            "f1cdc32a0b3f40238dd53c59c45e1d90",
            "a88e6ddb0ad34a238ecaccd71f0a324b",
            "b40503ee935f481fa8232aa2667fbbf7",
            "58adf9751d824e009116f9764390238e",
            "48b935e983d844f18869ab5ed99e5fdd",
            "18bbaff4bf4c47ba84804e11622eebbc",
            "87f9056435a24b4ab38e827212731b71",
            "a7fed666c24f42929b0b82951cbaefe4",
            "67164429cb56481ea7066c4c3b19ee34",
            "6409c6863ecc460d875fc5f6be558328",
            "3b46f665f92145c1912d7dcd5c749ff8",
            "f760616800d54fcfaa765f6d64ff2a65",
            "1ea2ee6cbf7e4496bea0e9a5a027092e",
            "061c5234a7de4e2da387083d916b5cec",
            "9d6ee8c522c24b48af4797bd9f21b7ce",
            "9192837a064c4ff8bd01246d2a96d25e",
            "b582308e12d145388d839519a8061e3a",
            "2d07d48b03534379a766c6bfc770b555",
            "1999fb7961e14a0482dd551030b40ac8",
            "9fc8d345475e413398d3d0fdc3f5eaa8",
            "66d75413978e4c749f920c6d22e888e7",
            "1257362e8bf44daf9dedad12c59a5bff",
            "7c23c2046edf488db10ff834489d14c9",
            "92f95598656a418a91409c10e18d5f6b",
            "71e63dda09e240ab8ff25eae9e90dc67",
            "be1eb45f593a49c481dab9ea25db29a1",
            "cf74b24ffcec45308e3cfc8718fbf031",
            "d3b388cb748f4fad9475901df3e1adbe",
            "659ce4a99a7e4982aa12cbc19112bfda",
            "cda4654f2945490cb474fad3b540b0cb",
            "329b0b6ee0534f54aadf8cc7771dd5cc",
            "c1ce28eb2fc94c828676025de95034fa",
            "bd8bc53f6eec4496a717ba58bc0427c5",
            "ebd02a0da7db49238cb25a57c78c7f66",
            "58a80953f7d94641b5b726c99c60095b",
            "9fc8facef93b49e39bd4da0f5c6289e3",
            "048327af68a7452dbd370a10369349d5",
            "4fd207d330dc4d8e99cbd431707a2f4e",
            "3c6d78fde26e4812a940eab32af74b74",
            "c7a533e7c743446d8a668de4d1869574",
            "67dac8775ed84b1e8a3ed202f9a6b5e0",
            "b7961c0aea3b4a20b17b1987f7899124",
            "36eda66024bf4f16b53d73c2b585faaf",
            "e19e211c2af54ce4a8ba350a2ee0fd36",
            "6fcb00797e8c447eb9a482f539ea5908",
            "e13b2dabda6d4845a643b1ec40075a8f",
            "5fe535163f9944b097481b706046e9e2",
            "a2941fedfc6b436699e77ba742755b31",
            "5b09231f3de14613b0edea733333de08",
            "ba1b490f892f4db6bbceff58b90e9442",
            "0f30e56389d8489987b928678a78d624",
            "c7ebc71ae54d4ba78f498aebd2f073f2",
            "7424db164ef5472e868d98323850efcd",
            "3ad2cb9c80d540a3b6407747d4c7b56e",
            "e6b9cac619234d7fa93c7aae46de8cac",
            "c10d7ad74d6b41dca0d175fcd2e224db",
            "9e339b79beb04e6aafc5134365347bc1",
            "637ae4eaeff549daa4b8ec67182247be",
            "677e60d5778d439cbe39937947f535da",
            "c7dbd0d4339745bcbdf8c3add1112a63",
            "1f0016f6a20a4d26a43f0635278c77c6",
            "d4c5e4674c9e41a8bad3111d0710a4c5",
            "49bab01662aa4f3daa80662ae1e37994",
            "635811d85cab44739ff713fb80c38ecc",
            "a41a8001d5b2428497f29d4c8076dedc",
            "28e50bfe7c684478947cd403fb361d2c",
            "1842a394a09e417ca03566971583ea88",
            "1cb169f2e51c45d68cf12e1c2430dd9d",
            "faf8399e7dbc4a6aa95345d131113fa2",
            "3303c2df8de547c793743a11a583a284",
            "0f489225530049a081a04eb8960bf7c5",
            "ee5ac421d7234d2d89c2f0fe58ccdf9a",
            "fa68ca91c3f54de18c8c51c9a70d9c48",
            "d67f027cf0114a99a68aeabdb1b9513a",
            "962e7b1fae7c443489cf4616e1992aad",
            "3b5c3b7373b3440792045c394c6ac75c",
            "fd343d5f0f9f48c68680c61b45068039",
            "6d2f8673af74447089b6aa6d4baa71b9",
            "f7cfc1f0774345e1b48f5713db495acc",
            "1c4decff5ed5453aa6d7147253237c3a",
            "508ba87dc4224c659260405a97bf795a",
            "e1552b4f3f584dc1aedc5e4ee7bab37f",
            "32de9c39474e4635a4574922f7f127d0",
            "3ddd0633fff64e22bdb48d327e0e33ba",
            "321a723e661e4d14b2b920ffbbfb7acd",
            "c7e2c829d89549dc840e351eeaeb8a3e",
            "1f82f12724194a9a8b4d0c84d1d370ec",
            "0c685f7d23664f66b10dd20e3ba3c131",
            "283d7184ce334c20922844e28495c886",
            "64e22e931b4b48329ffb6bbd12a5402e",
            "beb360b184e542b6ad947c7007649675",
            "f4a31f7288b74fb48473c25ceb960c76"
          ]
        },
        "outputId": "437461f7-0498-4743-ce5a-296c8c1fdbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "beafc5cb688845ca92edb3ab7232462e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac1bdab52af64bbdac8e0175aa4f76a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.99k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2664ccfd15cf4d298b0cf76868cc0328"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/550 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdd438862057434c92896ef2fb2aab25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48b935e983d844f18869ab5ed99e5fdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/265M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9192837a064c4ff8bd01246d2a96d25e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf74b24ffcec45308e3cfc8718fbf031"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fd207d330dc4d8e99cbd431707a2f4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b09231f3de14613b0edea733333de08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/450 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7dbd0d4339745bcbdf8c3add1112a63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f489225530049a081a04eb8960bf7c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1552b4f3f584dc1aedc5e4ee7bab37f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(keyword_feaature_bert_embedding))\n",
        "\n",
        "keyword_feaature_bert_embedding"
      ],
      "metadata": {
        "id": "UghFBJyF1hZC",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8428602-f39b-44be-ba38-cad9ec409a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.26855186, -0.6045678 ,  0.47657362, ..., -0.52285945,\n",
              "        -0.3049894 , -0.925612  ],\n",
              "       [-0.73744106, -0.8641267 ,  0.6937428 , ..., -1.3673078 ,\n",
              "         0.10443189, -0.5714228 ],\n",
              "       [-1.0187142 , -0.70818657,  0.36998767, ..., -1.0079896 ,\n",
              "        -0.30755278, -0.0738718 ],\n",
              "       ...,\n",
              "       [ 0.0975033 , -0.07088516,  0.26368952, ..., -0.05047144,\n",
              "        -0.1023604 , -0.8592203 ],\n",
              "       [-0.4740122 , -1.0899329 ,  0.67605585, ...,  0.71893084,\n",
              "        -0.27680606,  0.14375247],\n",
              "       [ 0.11279758, -0.1743159 ,  0.57081133, ..., -0.49074906,\n",
              "        -0.00307617,  0.5477061 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Cosine Similarity \n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "top_n = 300\n",
        "\n",
        "distances = cosine_similarity(keyword_feaature_bert_embedding, keyword_feaature_bert_embedding)\n",
        "\n",
        "print(len(distances))\n",
        "# keywords = [keyword_feaature_bert_embedding[index] for index in distances.argsort()[0][-top_n:]]\n",
        "keywords = [bigram_candidate_keywords[index] for index in distances.argsort()[0][-top_n:]]\n",
        "keywords\n",
        "\n",
        "top_300_embeddings_with_similar_cos_similarity = [keyword_feaature_bert_embedding[index] for index in distances.argsort()[0][-top_n:]]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F4PgYtmLwDfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8b7a40-8d49-4264-8374-7ca70fceeb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_300_embeddings_with_similar_cos_similarity[:1]"
      ],
      "metadata": {
        "id": "x8E35UKsXF96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adab6784-5180-4fc3-a19e-b8d83aeb45b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-1.60771585e+00,  7.43093863e-02,  3.71663332e-01, -6.22159123e-01,\n",
              "        -7.09010720e-01, -1.06052828e+00,  6.97452128e-01,  8.18166256e-01,\n",
              "        -1.45838216e-01, -4.33979601e-01,  6.26473427e-01,  6.41432822e-01,\n",
              "        -3.79246473e-02,  7.04947263e-02, -1.51248157e-01, -7.14931667e-01,\n",
              "        -5.47982216e-01,  1.19002936e-02,  4.99034941e-01, -4.76399243e-01,\n",
              "        -6.62177026e-01, -3.17990541e-01,  1.59147888e-01,  7.89199531e-01,\n",
              "        -5.46395779e-01,  5.71101606e-01, -4.06214707e-02,  1.02923703e+00,\n",
              "        -4.70834315e-01,  3.49680573e-01, -2.36328110e-01, -5.63173234e-01,\n",
              "        -1.85266733e-01,  4.49257910e-01,  8.55134666e-01,  1.34226307e-01,\n",
              "         7.19548464e-01, -3.23993206e-01, -6.14798248e-01,  6.88172400e-01,\n",
              "         6.33144617e-01,  1.72627181e-01,  1.31822512e-01,  1.39601916e-01,\n",
              "        -1.04471362e+00,  2.99834818e-01, -5.82042992e-01,  6.08406186e-01,\n",
              "        -1.59332681e+00, -2.03079000e-01,  1.62999537e-02,  2.71307647e-01,\n",
              "         1.31525099e+00, -2.80933857e-01, -5.90162158e-01, -4.72396761e-02,\n",
              "         9.80434120e-02, -7.96859622e-01,  7.47333020e-02,  9.47152734e-01,\n",
              "         4.49181527e-01,  3.04442048e-01, -7.54233897e-02, -6.73559248e-01,\n",
              "         2.19777152e-01, -5.62335216e-02,  1.17648530e+00,  1.40476465e+00,\n",
              "         2.25116551e-01, -1.69159159e-01,  2.11281925e-01, -1.01712155e+00,\n",
              "        -6.16887569e-01, -1.98453769e-01, -5.58654554e-02,  1.37314528e-01,\n",
              "         2.59213030e-01,  4.53925729e-02,  7.41893470e-01, -4.88352925e-02,\n",
              "         4.69047368e-01,  2.93467641e-01,  3.91127944e-01,  5.88607073e-01,\n",
              "         5.02229571e-01, -1.52796054e+00, -2.23293185e-01, -7.34062120e-02,\n",
              "        -1.53578663e+00,  2.35198796e-01,  6.06393933e-01, -3.43182683e-01,\n",
              "        -1.60734326e-01,  2.21209839e-01,  3.82488251e-01, -1.14393961e+00,\n",
              "         2.72834629e-01, -2.69824505e-01, -2.86522061e-01,  9.77164745e-01,\n",
              "        -1.57465839e+00, -2.55669743e-01, -1.51154473e-01,  6.42011166e-01,\n",
              "         3.42791259e-01, -6.28753364e-01,  7.19815671e-01, -7.27651715e-01,\n",
              "        -2.34123558e-01,  6.32546067e-01,  4.60122526e-01,  2.72078756e-02,\n",
              "         3.78463656e-01, -1.07372425e-01, -6.76090002e-01,  6.55916147e-03,\n",
              "        -7.06607759e-01, -1.54194444e-01,  4.54755835e-02,  2.40581363e-01,\n",
              "        -1.20708525e-01,  1.25308871e+00,  2.71445394e-01,  1.60381913e+00,\n",
              "         8.19291472e-02,  7.47788012e-01,  5.44024885e-01,  2.38944292e-01,\n",
              "         9.94200587e-01, -5.89033179e-02, -6.92131221e-02,  6.61159992e-01,\n",
              "        -1.23845212e-01, -3.26096892e-01,  6.98309004e-01, -3.41714352e-01,\n",
              "         5.15138924e-01, -1.15577012e-01, -3.34317148e-01,  1.03391922e+00,\n",
              "         3.59919518e-01,  5.52357554e-01,  6.22821450e-01, -4.72102106e-01,\n",
              "         4.38287079e-01, -1.58494306e+00, -1.51455492e-01, -4.83029068e-01,\n",
              "         5.87298989e-01, -1.92306012e-01, -1.41923952e+00,  3.72246087e-01,\n",
              "        -5.81153750e-01, -1.41367459e+00,  7.24290967e-01,  1.07688856e+00,\n",
              "        -1.47139747e-02, -1.51097506e-01,  5.78601539e-01,  9.70978320e-01,\n",
              "        -1.28232098e+00, -9.69111472e-02,  1.67069018e-01,  2.62438387e-01,\n",
              "        -1.00513422e+00, -1.16505861e+00, -4.25428540e-01,  6.69784844e-03,\n",
              "        -1.68323696e-01,  6.69088721e-01, -2.56041855e-01, -1.04825839e-01,\n",
              "         1.58040154e+00,  1.58453822e+00, -2.90787607e-01,  5.09715199e-01,\n",
              "         1.00808012e+00, -5.09412527e-01, -4.64540988e-01, -4.91385877e-01,\n",
              "         2.11775437e-01, -5.36589146e-01, -1.52794302e-01,  1.00212371e+00,\n",
              "         3.30167353e-01, -2.92653501e-01, -9.76261571e-02, -1.04122281e+00,\n",
              "         4.79440153e-01,  1.67637408e-01,  4.38558869e-02, -2.85993695e-01,\n",
              "        -8.07981491e-01,  5.48909426e-01,  1.04699636e+00,  8.24626327e-01,\n",
              "         3.17884386e-01, -1.80745915e-01,  2.02388197e-01,  2.64853220e-02,\n",
              "         3.49082083e-01,  5.11608012e-02,  7.34245181e-01, -1.85043097e-01,\n",
              "         3.56185496e-01, -3.01979303e-01, -1.11630961e-01, -5.49495339e-01,\n",
              "         7.61956155e-01, -1.77794188e-01, -3.03773880e-01,  1.29488707e-01,\n",
              "        -5.92911065e-01, -1.48947763e+00,  4.41520840e-01,  7.06475556e-01,\n",
              "         3.45949948e-01, -3.60456586e-01, -5.21414042e-01, -2.37902403e-01,\n",
              "         7.50981629e-01,  3.27887148e-01,  1.11639827e-01,  1.19152628e-01,\n",
              "        -4.23218012e-01, -2.79800296e-01,  6.55303359e-01, -4.26447928e-01,\n",
              "         2.99252689e-01,  2.24142820e-01, -3.91158432e-01,  1.60044563e+00,\n",
              "         1.97621062e-02, -1.81502048e-02,  3.82590473e-01, -1.53723672e-01,\n",
              "        -7.01367706e-02, -1.04771495e+00,  4.56074625e-01, -4.47707325e-02,\n",
              "         4.92383301e-01, -1.61450058e-02, -2.73456275e-01, -1.07068747e-01,\n",
              "         4.62297618e-01,  7.07496226e-01,  6.24478571e-02, -2.31035605e-01,\n",
              "         1.39650440e+00,  1.19876814e+00,  2.97687411e-01, -7.98667669e-01,\n",
              "        -4.00801562e-03, -4.71700132e-01,  3.23013514e-01, -1.54491082e-01,\n",
              "         5.29484808e-01, -5.24710655e-01,  2.79654145e-01,  1.89096302e-01,\n",
              "        -5.11418760e-01, -8.49391758e-01,  6.17225051e-01, -4.98147495e-02,\n",
              "        -1.09235239e+00,  2.66382396e-01, -4.84202266e-01, -6.76768720e-02,\n",
              "        -1.26960889e-01,  5.16075909e-01, -1.07846320e+00,  5.24336457e-01,\n",
              "        -3.23404133e-01,  2.67093368e-02,  7.21483350e-01, -1.21195577e-02,\n",
              "         5.30352890e-01,  1.87438786e-01, -6.71948418e-02,  3.67431700e-01,\n",
              "        -6.18912816e-01, -3.97697181e-01, -1.71232760e-01, -1.50088823e+00,\n",
              "        -6.40110373e-01,  2.29038537e-01, -2.07834430e-02, -1.58275890e+00,\n",
              "        -6.58961475e-01, -6.76523745e-02,  4.16109949e-01,  4.48263198e-01,\n",
              "        -1.32415044e+00,  1.29030144e+00,  2.63214260e-02, -1.36504078e+00,\n",
              "         1.04815371e-01,  4.98186886e-01,  9.24088433e-02,  1.32301331e-01,\n",
              "        -6.39660120e-01,  2.18666732e-01, -2.66369998e-01, -3.08207750e-01,\n",
              "        -4.65352312e-02,  2.07588181e-01,  3.25248420e-01, -8.86410475e-01,\n",
              "        -5.55960685e-02, -4.46771085e-01,  1.53504834e-01, -1.19106062e-01,\n",
              "         3.88992280e-01,  2.78846145e-01,  1.10754526e+00,  8.96736443e-01,\n",
              "         3.44696969e-01,  3.63007523e-02, -5.90909421e-01, -4.34716046e-01,\n",
              "         3.38618279e-01,  5.96483111e-01,  2.92484701e-01, -4.03175175e-01,\n",
              "         4.94085252e-01,  1.07667243e+00, -7.23267853e-01,  1.47377864e-01,\n",
              "        -2.78543700e-02,  3.58850330e-01,  5.73387861e-01, -1.95802882e-01,\n",
              "         5.88327408e-01,  6.01464152e-01, -4.32016790e-01,  3.14985588e-02,\n",
              "        -6.81640089e-01,  2.14023575e-01, -5.63446879e-01,  1.06868297e-01,\n",
              "        -5.13612986e-01, -3.84633780e-01,  2.05264613e-01, -6.50689080e-02,\n",
              "         9.31914926e-01,  5.99625468e-01,  9.47955191e-01, -2.48739898e-01,\n",
              "         9.63957429e-01, -9.57828462e-02, -2.59764075e-01,  6.92649126e-01,\n",
              "         9.39798594e-01, -7.82708824e-01, -5.93013987e-02, -3.19502592e-01,\n",
              "         3.44274431e-01,  5.13605297e-01,  6.20287299e-01,  1.05524087e+00,\n",
              "         4.44852531e-01,  7.11523533e-01,  3.57451588e-01, -2.97877491e-01,\n",
              "        -2.17319757e-01,  6.04175329e-01, -1.38651061e+00,  2.27053344e-01,\n",
              "         4.27764207e-01, -2.31098145e-01,  1.58778548e-01,  2.01148957e-01,\n",
              "         8.22073370e-02, -4.78904992e-01,  3.36938873e-02, -6.30278707e-01,\n",
              "         3.33006769e-01,  6.84764087e-02, -1.68581748e+00, -1.34739012e-01,\n",
              "        -2.49525756e-01, -4.20685887e-01, -8.91889453e-01,  5.43854952e-01,\n",
              "        -1.31293488e+00,  1.83964208e-01, -1.07229769e-01, -2.62742043e-01,\n",
              "        -7.19412565e-01,  4.05373216e-01,  9.30092186e-02,  4.66412663e-01,\n",
              "        -7.99422026e-01, -7.94399559e-01, -1.51827931e-03, -1.81342512e-01,\n",
              "        -6.41551673e-01,  9.29540277e-01,  4.42274928e-01, -1.22668958e+00,\n",
              "         1.08058739e+00,  1.02681510e-01, -1.52343780e-01, -1.81075245e-01,\n",
              "         8.29924345e-02, -2.99740255e-01,  1.04228628e+00, -3.79656941e-01,\n",
              "         2.26597711e-02, -5.68917096e-01,  1.30837724e-01, -1.61448061e-01,\n",
              "         5.83710849e-01, -7.99203277e-01,  1.17699206e-01, -5.41350901e-01,\n",
              "        -1.19523895e+00,  7.47214615e-01, -3.31956863e-01, -3.42967421e-01,\n",
              "         3.27845275e-01,  7.29148865e-01, -6.28078103e-01, -2.21779674e-01,\n",
              "         1.92606643e-01, -1.32702798e-01, -3.07997823e-01,  9.52632666e-01,\n",
              "        -2.79319048e-01,  4.77848262e-01,  5.54081321e-01, -4.89824086e-01,\n",
              "         4.31446820e-01, -2.89676487e-01,  3.70016158e-01,  7.41206050e-01,\n",
              "         1.64443076e+00, -2.46698201e-01, -2.76813619e-02,  2.96183020e-01,\n",
              "         3.90640318e-01,  1.48801565e-01,  5.83228990e-02, -4.22742039e-01,\n",
              "        -1.04098010e+00,  1.17128062e+00, -3.03287536e-01, -3.75103861e-01,\n",
              "         7.06537515e-02,  6.51394784e-01,  1.00935674e+00,  1.98295936e-01,\n",
              "        -1.34869218e+00, -6.05284810e-01,  2.53542274e-01,  5.53063273e-01,\n",
              "        -5.33057570e-01, -4.08649266e-01, -5.38198173e-01,  4.71130311e-01,\n",
              "         5.39834440e-01, -3.36140186e-01, -4.36470777e-01, -2.67266810e-01,\n",
              "         1.69187233e-01,  8.60671461e-01,  3.34872901e-01,  4.44209248e-01,\n",
              "        -2.59848595e-01,  8.31242919e-01, -3.74883711e-01,  8.75960350e-01,\n",
              "        -2.88422674e-01, -3.20384264e-01,  1.05555236e+00, -1.52034700e-01,\n",
              "         1.42167285e-01, -7.63401747e-01, -4.74391848e-01, -1.08150136e+00,\n",
              "         2.73100156e-02, -4.67119396e-01, -1.90643907e-01, -1.11098218e+00,\n",
              "         8.31155419e-01, -1.00643739e-01, -3.31394732e-01,  4.33780462e-01,\n",
              "         4.22964752e-01, -1.09144044e+00, -1.66204855e-01,  1.49957776e-01,\n",
              "        -7.73397923e-01,  7.75055647e-01,  3.89084458e-01, -2.85606444e-01,\n",
              "         8.83604825e-01,  2.15078399e-01, -4.31354553e-01, -1.94023587e-02,\n",
              "        -2.50908211e-02, -9.97427642e-01, -4.37845975e-01,  4.44427848e-01,\n",
              "         8.31143141e-01,  1.47684479e+00, -6.74332380e-01,  8.58500957e-01,\n",
              "         1.45127866e-02,  1.81045756e-01, -9.92039263e-01, -1.68150127e-01,\n",
              "         6.51002705e-01, -9.35824454e-01,  1.01025128e+00,  3.92203741e-02,\n",
              "         3.57191682e-01, -4.90727663e-01, -6.59700453e-01,  2.59405583e-01,\n",
              "        -1.11075628e+00,  2.35695988e-01, -2.71022886e-01,  3.34164083e-01,\n",
              "         9.09349859e-01,  1.65071577e-01,  1.17146039e+00, -4.00985003e-01,\n",
              "        -2.69223571e-01,  4.54131603e-01,  8.09777528e-02, -1.43079817e-01,\n",
              "        -5.06142378e-01, -1.20593703e+00, -7.23477185e-01,  2.92894453e-01,\n",
              "        -3.56238633e-01, -7.68872738e-01,  4.55794603e-01,  1.52064908e+00,\n",
              "        -5.97685814e-01,  3.69477838e-01,  1.18642712e+00,  5.08157313e-01,\n",
              "        -1.46637764e-03, -2.05658138e-01, -5.37829876e-01,  9.08850312e-01,\n",
              "        -5.79760611e-01,  3.00651312e-01, -4.04448748e-01,  7.03337252e-01,\n",
              "         1.01937270e+00,  1.07001078e+00, -1.18826509e+00, -9.75108027e-01,\n",
              "        -1.30569875e+00, -6.94921851e-01,  1.02202225e+00, -7.89907947e-02,\n",
              "        -1.90855110e+00, -1.38383269e-01, -6.91735148e-01, -9.89397764e-01,\n",
              "         4.31270860e-02, -2.95457244e-01, -6.24234676e-01, -2.81174064e-01,\n",
              "        -1.26394200e+00,  1.40979147e+00, -3.27516824e-01, -1.09666252e+00,\n",
              "        -1.29197168e+00, -4.10931587e-01,  1.74635291e-01, -5.18556774e-01,\n",
              "        -2.47350782e-01,  4.70866784e-02, -4.23873037e-01,  9.22579825e-01,\n",
              "         5.81514716e-01,  2.20006406e-01, -4.29672331e-01,  2.76595891e-01,\n",
              "         3.79133895e-02, -2.04356909e-01,  4.55681920e-01, -6.00035787e-01,\n",
              "         5.44672310e-01,  3.76913011e-01, -9.11329329e-01, -6.65914416e-01,\n",
              "         1.37013584e-01, -3.85922521e-01, -5.61984420e-01, -5.11238277e-01,\n",
              "         3.00435662e-01, -3.73141646e-01,  1.89819500e-01, -5.28885961e-01,\n",
              "         9.36110616e-01, -1.01810598e+00, -6.18855000e-01,  7.86228999e-02,\n",
              "         1.22955911e-01,  2.58849680e-01, -3.94870222e-01,  2.41137773e-01,\n",
              "        -3.56096029e-02,  1.43339634e+00,  7.41308033e-02, -2.94508398e-01,\n",
              "         1.49563909e-01,  8.69425058e-01,  8.72698605e-01,  7.43945718e-01,\n",
              "        -1.03626847e+00, -6.52838200e-02, -1.67631900e+00,  2.75351852e-01,\n",
              "        -7.51208007e-01,  1.40096873e-01, -1.94968998e-01, -5.38293242e-01,\n",
              "         3.16843837e-01, -1.07861292e+00, -6.24732792e-01, -7.97694363e-03,\n",
              "        -5.19672036e-01, -5.73024079e-02,  1.97469860e-01,  2.26013541e-01,\n",
              "         5.88306367e-01, -2.00210005e-01, -6.62425876e-01, -2.12306008e-02,\n",
              "        -1.05624461e+00, -5.43487132e-01, -2.78218836e-01, -7.75986090e-02,\n",
              "        -4.29181755e-01, -2.57166743e-01,  1.89376045e-02,  3.39896321e-01,\n",
              "        -1.09146118e-01,  1.31125808e-01,  6.89067692e-03,  3.46606642e-01,\n",
              "        -1.59908557e+00, -3.73102695e-01, -5.36052465e-01,  5.55362821e-01,\n",
              "        -6.68981910e-01, -6.01078212e-01, -3.14827234e-01, -1.31475717e-01,\n",
              "         7.27711082e-01, -4.73339446e-02,  2.23632157e-01,  2.25545973e-01,\n",
              "        -1.28771496e+00,  8.56059313e-01,  6.55856848e-01, -5.30177593e-01,\n",
              "         2.13892654e-01,  3.00836802e-01,  6.43549025e-01, -5.67952059e-02,\n",
              "        -6.18501186e-01, -8.94018531e-01,  8.81103337e-01,  7.67642260e-01,\n",
              "        -6.91479862e-01,  3.65105808e-01,  2.67600536e-01,  1.87008306e-02,\n",
              "         2.26884514e-01, -4.12000775e-01, -6.77577138e-01, -6.40879758e-03,\n",
              "        -2.02084020e-01,  4.85552073e-01, -5.28752089e-01,  2.96753824e-01,\n",
              "        -4.70797747e-01,  2.31040418e-01, -7.90956080e-01,  4.95338812e-02,\n",
              "         3.80933434e-01,  2.89543808e-01,  7.59268522e-01,  3.10735285e-01,\n",
              "         6.76486731e-01,  4.12432492e-01, -4.26761001e-01,  1.80887014e-01,\n",
              "        -1.40566528e-01, -3.73873591e-01, -6.87188506e-01,  1.13884759e+00,\n",
              "        -3.30580354e-01, -5.13362885e-01, -1.01160443e+00,  4.55882311e-01,\n",
              "        -1.36423028e+00,  1.77791074e-01,  6.96595669e-01,  5.44084311e-01,\n",
              "         2.12515235e-01, -3.97355646e-01,  4.19906467e-01,  7.33433545e-01,\n",
              "         4.78103340e-01, -5.45500457e-01,  1.52509257e-01, -1.77615896e-01,\n",
              "        -9.32146072e-01,  4.72254395e-01, -1.12650180e+00, -8.30115080e-01,\n",
              "        -1.48974836e-01, -2.87879646e-01,  2.61573493e-01, -9.24223602e-01,\n",
              "         9.65638459e-03,  8.42822790e-02,  3.08813322e-02, -8.27250481e-01,\n",
              "        -9.19773951e-02, -2.86703706e-01, -8.18914399e-02, -3.19047093e-01,\n",
              "        -6.60540700e-01, -3.06952745e-01,  1.68511599e-01,  4.13959682e-01,\n",
              "        -3.68230551e-01,  4.76791039e-02,  2.82941461e-01, -1.53495535e-01,\n",
              "        -7.99252272e-01, -3.91080916e-01, -4.30240422e-01,  3.85867953e-01,\n",
              "         3.22191596e-01,  1.89404294e-01, -4.56816494e-01, -5.91459095e-01,\n",
              "        -1.17203283e+00, -5.71433902e-02,  1.08337872e-01,  5.20554841e-01,\n",
              "         8.14293101e-02, -5.59003234e-01, -3.02931726e-01,  2.84159124e-01,\n",
              "        -7.43033767e-01, -2.69220501e-01, -3.84488434e-01,  8.36561382e-01,\n",
              "        -3.31863850e-01, -2.54711390e-01, -6.31444156e-01,  3.20811957e-01,\n",
              "        -2.72176594e-01, -6.34824872e-01,  5.26481152e-01,  2.14347899e-01],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-NN to calculate the Nearest neighbor of keywords\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "keyword_feaature_bert_embedding = keyword_feaature_bert_embedding[0:300]\n",
        "model = NearestNeighbors(n_neighbors=10,\n",
        "                         metric='cosine',\n",
        "                         algorithm='brute',\n",
        "                         n_jobs=-1)\n",
        "first_time_window_candidate_nn = model.fit(keyword_feaature_bert_embedding)\n",
        "\n",
        "# This gives a matrix of 300, 10 -> 300 keywords/arrays with each array containing 10 elements\n",
        "# representing its nearest neighbor\n",
        " \n",
        "distance, indeces = first_time_window_candidate_nn.kneighbors(keyword_feaature_bert_embedding)\n",
        "\n",
        "# Give the indeces of the neighbors that are nearest to the word under consideration\n",
        "## Here we have 300 indeces each with 10 sub array elements, representing it's 10 nearest keywords\n",
        "\n"
      ],
      "metadata": {
        "id": "ErZFPdrz-Qzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indeces[0]"
      ],
      "metadata": {
        "id": "Isapq28hr2l7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4e39cc-463a-4665-fc20-f0ec51bdcfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,  68, 192,   1,  44,  67, 131, 100, 132,  63])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords\n"
      ],
      "metadata": {
        "id": "c6KxF5phbaPQ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08de0bc4-7802-48a4-d85b-461d9c0fec63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['robot database',\n",
              " 'robot reset',\n",
              " 'physics designing',\n",
              " 'databases robot',\n",
              " 'speciiications 20dot',\n",
              " 'operator teaches',\n",
              " 'problem designing',\n",
              " 'robot roan',\n",
              " 'robot operator',\n",
              " 'networks structures',\n",
              " 'robot 30kg',\n",
              " 'osaka 560',\n",
              " 'operator brushes',\n",
              " 'robot arbitrarily',\n",
              " 'robot camera',\n",
              " 'camera 55deg',\n",
              " 'randomly algorithm',\n",
              " 'rewriting free',\n",
              " 'suzuki suguru',\n",
              " 'nodes 145',\n",
              " 'methodologies problem',\n",
              " 'robot identifies',\n",
              " 'rmbi robot',\n",
              " 'laboratories fig',\n",
              " '560 organizing',\n",
              " 'osaka toyonaka',\n",
              " 'neural networks',\n",
              " 'operator obstacles',\n",
              " 'roan uni',\n",
              " 'builds organizing',\n",
              " 'networks approximations',\n",
              " 'robot recalling',\n",
              " 'toyonaka osaka',\n",
              " 'associative databases',\n",
              " 'mobile robot',\n",
              " 'brushes obstacles',\n",
              " 'uni fig',\n",
              " 'associative database',\n",
              " 'terminal node',\n",
              " 'variables nodes',\n",
              " 'problem criterion',\n",
              " 'sampled randomly',\n",
              " 'stools handcarts',\n",
              " 'controls 3km',\n",
              " 'rewrites correlated',\n",
              " 'robot eyesight',\n",
              " 'problem compression',\n",
              " 'subject constructing',\n",
              " 'minimization problem',\n",
              " 'yin yin',\n",
              " 'camera loosened',\n",
              " 'obstacles operator',\n",
              " 'robot superimposes',\n",
              " 'problem minimization',\n",
              " 'subclass structures',\n",
              " 'passageways 18m',\n",
              " 'candidates mappings',\n",
              " 'organizing associative',\n",
              " 'letter fig',\n",
              " 'triangular technique',\n",
              " 'networks recursi',\n",
              " 'panorama 360deg',\n",
              " 'blem arises',\n",
              " 'white fig',\n",
              " 'camera preprocessing',\n",
              " 'roan fig',\n",
              " 'parameters samples',\n",
              " 'problem identifying',\n",
              " 'sampl fig',\n",
              " 'letter letter',\n",
              " 'notation candidates',\n",
              " 'oftree 522',\n",
              " 'operator subjectively',\n",
              " 'scanner fig',\n",
              " 'letters4 global',\n",
              " 'observes rewrites',\n",
              " 'computing coefficients',\n",
              " 'neural randomly',\n",
              " 'trajectory blem',\n",
              " 'computes parameters',\n",
              " 'tly slitting',\n",
              " 'learns landscapes',\n",
              " 'nand nodes',\n",
              " 'robot white',\n",
              " 'compress camera',\n",
              " 'synapses branching',\n",
              " 'notations nand',\n",
              " 'compute coefficients',\n",
              " 'constructing associative',\n",
              " 'reset pointer',\n",
              " 'camera inquires',\n",
              " 'recognizer parallelogram',\n",
              " 'relating parameters',\n",
              " 'scans slant',\n",
              " 'parameter recursive',\n",
              " 'candidates assumed',\n",
              " 'camera panorama',\n",
              " 'nodes memorize',\n",
              " 'loo fig',\n",
              " 'typographic english',\n",
              " 'reset periodically',\n",
              " 'periodically operator',\n",
              " 'preprocessing ion',\n",
              " 'problem wide',\n",
              " 'coefficients samples',\n",
              " 'structures learns',\n",
              " 'parameter assumes',\n",
              " 'letter recen',\n",
              " 'coefficients anza',\n",
              " 'networks eternally',\n",
              " 'nualber sampl',\n",
              " 'autonomous mobile',\n",
              " 'networks accelerate',\n",
              " 'levels 202',\n",
              " 'contrarily parameter',\n",
              " 'realizes handwritten',\n",
              " 'teaches recognizer',\n",
              " 'compu ters',\n",
              " 'camera face',\n",
              " 'reflects operator',\n",
              " 'candidates subset',\n",
              " 'recursive methodology',\n",
              " 'catches letter',\n",
              " 'passageway juts',\n",
              " 'constructions associative',\n",
              " 'nodes increases',\n",
              " 'parameter compactness',\n",
              " 'compression camera',\n",
              " 'slitting recognizing',\n",
              " 'letter recognizer',\n",
              " 'slant angular',\n",
              " 'recognizer scans',\n",
              " 'units computing',\n",
              " 'contrarily associative',\n",
              " 'robot flourishingly6',\n",
              " 'recalling camera',\n",
              " 'head letter',\n",
              " 'pointer arrives',\n",
              " 'camera preliminarily',\n",
              " 'recognizing typographic',\n",
              " 'simplest constructions',\n",
              " 'samples nualber',\n",
              " 'learns input',\n",
              " 'samples computes',\n",
              " 'suguru arimoto',\n",
              " 'nodes frequency',\n",
              " 'rewri ting',\n",
              " 'tion defines',\n",
              " 'camera autonomous',\n",
              " 'visual camera',\n",
              " 'recen tly',\n",
              " 'recognizer interaction',\n",
              " 'bstitu white',\n",
              " 'input output',\n",
              " 'subset mappings',\n",
              " 'configuration autonomous',\n",
              " 'approaches samples',\n",
              " 'procedures compute',\n",
              " 'vertically filtered',\n",
              " 'recursive observes',\n",
              " 'superimposes camera',\n",
              " 'contrarily neural',\n",
              " 'compression admittable',\n",
              " 'criterion operator',\n",
              " 'compactness assumed',\n",
              " 'recursi recursive',\n",
              " 'summation divided',\n",
              " 'letters3 matching',\n",
              " 'corresponds fig',\n",
              " 'mapping reals',\n",
              " 'observes sampled',\n",
              " 'computation proportionally',\n",
              " 'samples increases',\n",
              " 'truncating finitely',\n",
              " 'recognizing styles',\n",
              " 'parameters uniquely',\n",
              " 'samples likelihood',\n",
              " 'suffices camera',\n",
              " 'luminance white',\n",
              " 'compactness almighty',\n",
              " 'universality neural',\n",
              " 'reproduces criterion',\n",
              " 'recognizer accomplishes',\n",
              " 'letter autonomous',\n",
              " 'orthogonal dimension',\n",
              " 'levels converged',\n",
              " 'recognizing hand',\n",
              " 'accomplishes recursively',\n",
              " '800 nodes',\n",
              " 'converges samples',\n",
              " 'levels oftree',\n",
              " 'recursive guarantees',\n",
              " 'parallelogram maximal',\n",
              " 'recursive effectiveness',\n",
              " 'matching recognizing',\n",
              " 'converged 400',\n",
              " '400 levels',\n",
              " 'uniquely samples',\n",
              " 'maximal letter',\n",
              " '100 distinguishable']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_feaature_bert_embedding\n",
        "\n",
        "\n",
        "keywords_np = np.array(keywords)\n",
        "bigram_candidate_keywords_nparray = np.array(bigram_candidate_keywords) \n",
        "\n",
        "bigram_candidate_keywords_nparray\n",
        "\n",
        "keywords_np\n",
        "\n",
        "tup_nearest_neighbor = []\n",
        "for index, candidate_keyword in enumerate(keywords_np):\n",
        "    # Take the current index of the keyword and get the list of 10 nearest index from KNN algorithm\n",
        "    nearest_neighbors_indeces_of_current_keyword = indeces[index]\n",
        "\n",
        "    # Filter the keyword list using the list of indeces obtained in previous step\n",
        "    nearest_keywords = keywords_np[nearest_neighbors_indeces_of_current_keyword]\n",
        "\n",
        "    # Create tuple with first element as the keyword for current iteration and 2nd element as list of its nearest neighbors\n",
        "    tup_nearest_neighbor.append((candidate_keyword, nearest_keywords))\n",
        "\n",
        "\n",
        "tup_nearest_neighbor"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zTSKFRldqxax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37785cba-6643-40da-e8b7-f679506a1bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('robot database',\n",
              "  array(['robot database', 'sampl fig', 'parallelogram maximal',\n",
              "         'robot reset', 'rewrites correlated', 'problem identifying',\n",
              "         'recognizer scans', 'reset periodically', 'units computing',\n",
              "         'white fig'], dtype='<U26')),\n",
              " ('robot reset',\n",
              "  array(['robot reset', 'rewrites correlated', 'databases robot',\n",
              "         'robot database', 'learns landscapes', 'white fig',\n",
              "         'camera inquires', 'physics designing', 'typographic english',\n",
              "         'compression admittable'], dtype='<U26')),\n",
              " ('physics designing',\n",
              "  array(['physics designing', 'nodes memorize', 'reproduces criterion',\n",
              "         'tly slitting', 'robot arbitrarily', 'neural networks',\n",
              "         'mobile robot', 'osaka toyonaka', 'brushes obstacles',\n",
              "         'head letter'], dtype='<U26')),\n",
              " ('databases robot',\n",
              "  array(['databases robot', 'robot reset', 'rewrites correlated',\n",
              "         'nand nodes', 'robot database', 'compression admittable',\n",
              "         'learns landscapes', 'white fig', 'matching recognizing',\n",
              "         'blem arises'], dtype='<U26')),\n",
              " ('speciiications 20dot',\n",
              "  array(['speciiications 20dot', 'recognizer scans', 'robot flourishingly6',\n",
              "         'contrarily associative', 'slant angular', 'robot eyesight',\n",
              "         'blem arises', 'passageway juts', 'camera face', 'units computing'],\n",
              "        dtype='<U26')),\n",
              " ('operator teaches',\n",
              "  array(['operator teaches', 'contrarily neural', 'neural networks',\n",
              "         'coefficients samples', 'robot flourishingly6', 'robot eyesight',\n",
              "         'brushes obstacles', 'head letter', 'pointer arrives',\n",
              "         'constructing associative'], dtype='<U26')),\n",
              " ('problem designing',\n",
              "  array(['problem designing', 'robot roan', 'candidates assumed',\n",
              "         'variables nodes', 'terminal node', 'brushes obstacles',\n",
              "         'head letter', 'contrarily neural', 'coefficients samples',\n",
              "         'camera face'], dtype='<U26')),\n",
              " ('robot roan', array(['robot roan', 'problem designing', 'variables nodes',\n",
              "         'candidates assumed', 'contrarily neural', 'neural randomly',\n",
              "         'brushes obstacles', 'camera face', 'coefficients samples',\n",
              "         'recursive effectiveness'], dtype='<U26')),\n",
              " ('robot operator',\n",
              "  array(['robot operator', 'oftree 522', 'associative database',\n",
              "         'robot camera', 'subclass structures', 'samples computes',\n",
              "         'recen tly', 'reset pointer', 'recursive effectiveness',\n",
              "         'relating parameters'], dtype='<U26')),\n",
              " ('networks structures',\n",
              "  array(['networks structures', 'accomplishes recursively',\n",
              "         'parameter assumes', 'realizes handwritten',\n",
              "         'reproduces criterion', 'nualber sampl', 'stools handcarts',\n",
              "         'robot flourishingly6', 'camera preprocessing', 'levels converged'],\n",
              "        dtype='<U26')),\n",
              " ('robot 30kg',\n",
              "  array(['robot 30kg', 'recognizer parallelogram', 'reset pointer',\n",
              "         'relating parameters', 'realizes handwritten',\n",
              "         'speciiications 20dot', 'parameter assumes',\n",
              "         'contrarily parameter', 'samples increases', 'operator brushes'],\n",
              "        dtype='<U26')),\n",
              " ('osaka 560',\n",
              "  array(['osaka 560', 'roan fig', 'maximal letter', 'vertically filtered',\n",
              "         'problem compression', 'subclass structures', 'compression camera',\n",
              "         'sampl fig', '100 distinguishable', 'matching recognizing'],\n",
              "        dtype='<U26')),\n",
              " ('operator brushes',\n",
              "  array(['operator brushes', 'recognizing styles', 'terminal node',\n",
              "         'samples increases', 'candidates assumed', 'variables nodes',\n",
              "         'camera face', 'robot eyesight', 'nualber sampl',\n",
              "         'orthogonal dimension'], dtype='<U26')),\n",
              " ('robot arbitrarily',\n",
              "  array(['robot arbitrarily', 'physics designing', 'camera inquires',\n",
              "         'letter recen', 'nodes memorize', 'typographic english',\n",
              "         'recursive methodology', 'tly slitting', 'corresponds fig',\n",
              "         'orthogonal dimension'], dtype='<U26')),\n",
              " ('robot camera',\n",
              "  array(['robot camera', 'associative database', 'samples likelihood',\n",
              "         'uniquely samples', 'nodes 145', 'robot operator', 'suzuki suguru',\n",
              "         'camera 55deg', 'recursive methodology', 'passageway juts'],\n",
              "        dtype='<U26')),\n",
              " ('camera 55deg', array(['camera 55deg', 'suzuki suguru', 'uniquely samples',\n",
              "         'randomly algorithm', 'samples likelihood',\n",
              "         'recursive methodology', 'nodes 145', 'compactness almighty',\n",
              "         'methodologies problem', 'builds organizing'], dtype='<U26')),\n",
              " ('randomly algorithm',\n",
              "  array(['randomly algorithm', 'recursive methodology', 'camera 55deg',\n",
              "         'uniquely samples', 'suzuki suguru', 'methodologies problem',\n",
              "         'samples likelihood', 'robot flourishingly6', 'builds organizing',\n",
              "         'constructions associative'], dtype='<U26')),\n",
              " ('rewriting free',\n",
              "  array(['rewriting free', 'builds organizing', '400 levels', 'roan uni',\n",
              "         'organizing associative', 'teaches recognizer', 'catches letter',\n",
              "         'samples likelihood', 'summation divided', 'notation candidates'],\n",
              "        dtype='<U26')),\n",
              " ('suzuki suguru',\n",
              "  array(['suzuki suguru', 'camera 55deg', 'samples likelihood',\n",
              "         'uniquely samples', 'randomly algorithm', 'compactness almighty',\n",
              "         'nodes 145', 'recursive methodology', 'methodologies problem',\n",
              "         'builds organizing'], dtype='<U26')),\n",
              " ('nodes 145', array(['nodes 145', 'samples likelihood', 'uniquely samples',\n",
              "         'compactness almighty', 'methodologies problem', 'suzuki suguru',\n",
              "         'camera 55deg', 'recursive methodology', 'passageway juts',\n",
              "         'blem arises'], dtype='<U26')),\n",
              " ('methodologies problem',\n",
              "  array(['methodologies problem', 'randomly algorithm',\n",
              "         'recursive methodology', 'nodes 145', 'uniquely samples',\n",
              "         'camera 55deg', 'suzuki suguru', 'compactness almighty',\n",
              "         'parameter compactness', 'samples likelihood'], dtype='<U26')),\n",
              " ('robot identifies',\n",
              "  array(['robot identifies', 'laboratories fig', 'compress camera',\n",
              "         'camera face', 'robot flourishingly6', 'pointer arrives',\n",
              "         'head letter', 'operator teaches', 'periodically operator',\n",
              "         'passageway juts'], dtype='<U26')),\n",
              " ('rmbi robot', array(['rmbi robot', 'contrarily neural', 'compress camera',\n",
              "         'parameters uniquely', 'camera face', 'brushes obstacles',\n",
              "         'parameters samples', 'parameter compactness', 'operator teaches',\n",
              "         'neural networks'], dtype='<U26')),\n",
              " ('laboratories fig',\n",
              "  array(['laboratories fig', 'robot identifies', 'compress camera',\n",
              "         'pointer arrives', 'parameters uniquely', 'neural networks',\n",
              "         'head letter', 'operator teaches', 'robot flourishingly6',\n",
              "         'subject constructing'], dtype='<U26')),\n",
              " ('560 organizing',\n",
              "  array(['560 organizing', 'candidates mappings', 'passageways 18m',\n",
              "         'organizing associative', 'subclass structures', 'letter fig',\n",
              "         'candidates subset', 'mapping reals', 'summation divided',\n",
              "         'problem compression'], dtype='<U26')),\n",
              " ('osaka toyonaka',\n",
              "  array(['osaka toyonaka', 'orthogonal dimension', 'toyonaka osaka',\n",
              "         'problem compression', 'white fig', 'accomplishes recursively',\n",
              "         'operator subjectively', 'reproduces criterion',\n",
              "         'simplest constructions', 'luminance white'], dtype='<U26')),\n",
              " ('neural networks',\n",
              "  array(['neural networks', 'coefficients samples', 'operator teaches',\n",
              "         'contrarily neural', 'mobile robot', 'brushes obstacles',\n",
              "         'head letter', 'pointer arrives', 'periodically operator',\n",
              "         'constructing associative'], dtype='<U26')),\n",
              " ('operator obstacles',\n",
              "  array(['operator obstacles', 'roan uni', 'preprocessing ion',\n",
              "         'letters3 matching', 'associative database', 'camera panorama',\n",
              "         '400 levels', 'recognizer scans', 'compression camera', 'roan fig'],\n",
              "        dtype='<U26')),\n",
              " ('roan uni', array(['roan uni', 'preprocessing ion', 'operator obstacles',\n",
              "         'associative database', 'associative databases', '400 levels',\n",
              "         'camera panorama', 'recognizer scans', 'units computing',\n",
              "         'compression camera'], dtype='<U26')),\n",
              " ('builds organizing',\n",
              "  array(['builds organizing', 'camera 55deg', 'suzuki suguru',\n",
              "         'randomly algorithm', 'samples likelihood', 'uniquely samples',\n",
              "         'recursive methodology', 'compactness almighty',\n",
              "         'methodologies problem', 'nodes 145'], dtype='<U26')),\n",
              " ('networks approximations',\n",
              "  array(['networks approximations', 'robot recalling',\n",
              "         'networks accelerate', 'pointer arrives', 'robot eyesight',\n",
              "         'parameter assumes', 'problem wide', 'sampl fig',\n",
              "         'luminance white', 'reproduces criterion'], dtype='<U26')),\n",
              " ('robot recalling', array(['robot recalling', 'networks approximations',\n",
              "         'networks accelerate', 'builds organizing', 'compactness almighty',\n",
              "         'uniquely samples', 'suzuki suguru', 'recursive methodology',\n",
              "         'samples likelihood', 'methodologies problem'], dtype='<U26')),\n",
              " ('toyonaka osaka',\n",
              "  array(['toyonaka osaka', 'white fig', 'simplest constructions',\n",
              "         'periodically operator', 'osaka toyonaka', 'orthogonal dimension',\n",
              "         'operator subjectively', 'camera preprocessing', 'notations nand',\n",
              "         'suguru arimoto'], dtype='<U26')),\n",
              " ('associative databases',\n",
              "  array(['associative databases', 'recognizer scans', 'camera face',\n",
              "         'speciiications 20dot', 'robot eyesight', 'roan uni',\n",
              "         'luminance white', '400 levels', 'units computing',\n",
              "         'constructing associative'], dtype='<U26')),\n",
              " ('mobile robot',\n",
              "  array(['mobile robot', 'uni fig', 'neural networks', 'camera face',\n",
              "         'brushes obstacles', 'contrarily neural', 'luminance white',\n",
              "         'pointer arrives', 'head letter', 'camera loosened'], dtype='<U26')),\n",
              " ('brushes obstacles',\n",
              "  array(['brushes obstacles', 'head letter', 'camera face',\n",
              "         'contrarily neural', 'coefficients samples', 'mobile robot',\n",
              "         'periodically operator', 'pointer arrives', 'neural networks',\n",
              "         'operator teaches'], dtype='<U26')),\n",
              " ('uni fig', array(['uni fig', 'mobile robot', 'recursive effectiveness',\n",
              "         'camera face', 'brushes obstacles', 'contrarily neural',\n",
              "         'neural networks', 'camera loosened', 'robot white', 'head letter'],\n",
              "        dtype='<U26')),\n",
              " ('associative database',\n",
              "  array(['associative database', 'robot camera', 'subclass structures',\n",
              "         'robot operator', 'roan uni', 'recognizer scans',\n",
              "         'preprocessing ion', 'associative databases', 'operator obstacles',\n",
              "         'compression camera'], dtype='<U26')),\n",
              " ('terminal node',\n",
              "  array(['terminal node', 'variables nodes', 'candidates assumed',\n",
              "         'recognizing styles', 'operator brushes', 'brushes obstacles',\n",
              "         'head letter', 'camera face', 'orthogonal dimension',\n",
              "         'samples increases'], dtype='<U26')),\n",
              " ('variables nodes',\n",
              "  array(['variables nodes', 'terminal node', 'candidates assumed',\n",
              "         'head letter', 'problem wide', 'robot flourishingly6',\n",
              "         'periodically operator', 'problem designing',\n",
              "         'orthogonal dimension', 'brushes obstacles'], dtype='<U26')),\n",
              " ('problem criterion',\n",
              "  array(['problem criterion', 'stools handcarts', 'realizes handwritten',\n",
              "         'scans slant', 'luminance white', 'levels 202',\n",
              "         'notation candidates', 'sampled randomly', 'letter letter',\n",
              "         'levels converged'], dtype='<U26')),\n",
              " ('sampled randomly',\n",
              "  array(['sampled randomly', 'computes parameters', 'stools handcarts',\n",
              "         'problem criterion', 'white fig', 'operator subjectively',\n",
              "         'learns landscapes', 'realizes handwritten',\n",
              "         'reproduces criterion', 'structures learns'], dtype='<U26')),\n",
              " ('stools handcarts',\n",
              "  array(['stools handcarts', 'problem criterion', 'realizes handwritten',\n",
              "         'periodically operator', 'luminance white', 'sampled randomly',\n",
              "         'corresponds fig', 'scans slant', 'structures learns',\n",
              "         'levels 202'], dtype='<U26')),\n",
              " ('controls 3km',\n",
              "  array(['controls 3km', 'triangular technique', 'suguru arimoto',\n",
              "         'robot operator', 'reset pointer', 'rewrites correlated',\n",
              "         'camera inquires', 'simplest constructions', 'problem compression',\n",
              "         'samples likelihood'], dtype='<U26')),\n",
              " ('rewrites correlated',\n",
              "  array(['rewrites correlated', 'robot reset', 'databases robot',\n",
              "         'blem arises', 'physics designing', 'robot database',\n",
              "         'learns landscapes', 'robot eyesight', 'compression admittable',\n",
              "         'camera inquires'], dtype='<U26')),\n",
              " ('robot eyesight', array(['robot eyesight', 'blem arises', 'operator teaches',\n",
              "         'robot flourishingly6', 'speciiications 20dot', 'units computing',\n",
              "         'contrarily neural', 'pointer arrives', 'recognizer scans',\n",
              "         'slant angular'], dtype='<U26')),\n",
              " ('problem compression', array(['problem compression', 'camera preprocessing',\n",
              "         'recursive observes', 'passageways 18m', 'suguru arimoto',\n",
              "         'white fig', 'pointer arrives', 'periodically operator',\n",
              "         'osaka toyonaka', 'orthogonal dimension'], dtype='<U26')),\n",
              " ('subject constructing',\n",
              "  array(['subject constructing', 'pointer arrives', 'periodically operator',\n",
              "         'recalling camera', 'head letter', 'variables nodes',\n",
              "         'problem wide', 'brushes obstacles', 'camera face',\n",
              "         'neural networks'], dtype='<U26')),\n",
              " ('minimization problem',\n",
              "  array(['minimization problem', 'visual camera', 'camera autonomous',\n",
              "         'input output', 'subset mappings', 'approaches samples',\n",
              "         'bstitu white', 'samples computes', 'recen tly',\n",
              "         'procedures compute'], dtype='<U26')),\n",
              " ('yin yin', array(['yin yin', 'subclass structures', 'candidates mappings',\n",
              "         'white fig', 'camera loosened', 'passageways 18m', 'letter fig',\n",
              "         'orthogonal dimension', 'catches letter', 'synapses branching'],\n",
              "        dtype='<U26')),\n",
              " ('camera loosened',\n",
              "  array(['camera loosened', 'camera face', 'robot superimposes',\n",
              "         'brushes obstacles', 'periodically operator',\n",
              "         'recursive effectiveness', 'white fig', 'orthogonal dimension',\n",
              "         'pointer arrives', 'reproduces criterion'], dtype='<U26')),\n",
              " ('obstacles operator', array(['obstacles operator', 'recursive guarantees',\n",
              "         'camera preprocessing', 'problem compression', 'recursi recursive',\n",
              "         'recursive observes', 'suguru arimoto', 'osaka toyonaka',\n",
              "         'passageways 18m', 'recursive effectiveness'], dtype='<U26')),\n",
              " ('robot superimposes',\n",
              "  array(['robot superimposes', 'samples increases', 'camera loosened',\n",
              "         'pointer arrives', 'camera face', 'recognizer accomplishes',\n",
              "         'brushes obstacles', 'robot flourishingly6',\n",
              "         'constructions associative', 'passageway juts'], dtype='<U26')),\n",
              " ('problem minimization',\n",
              "  array(['problem minimization', 'samples increases', 'robot superimposes',\n",
              "         'operator teaches', 'samples likelihood', 'pointer arrives',\n",
              "         'robot flourishingly6', 'suzuki suguru', 'mapping reals',\n",
              "         'robot eyesight'], dtype='<U26')),\n",
              " ('subclass structures',\n",
              "  array(['subclass structures', 'associative database', 'yin yin',\n",
              "         'candidates mappings', 'sampl fig', 'passageways 18m',\n",
              "         'letter fig', 'compression camera', 'corresponds fig',\n",
              "         'organizing associative'], dtype='<U26')),\n",
              " ('passageways 18m',\n",
              "  array(['passageways 18m', 'candidates mappings', 'problem compression',\n",
              "         'camera preprocessing', 'recursive observes', 'suguru arimoto',\n",
              "         'subclass structures', 'organizing associative', 'letter fig',\n",
              "         'yin yin'], dtype='<U26')),\n",
              " ('candidates mappings',\n",
              "  array(['candidates mappings', 'passageways 18m', 'letter fig',\n",
              "         'organizing associative', 'subclass structures', 'yin yin',\n",
              "         '560 organizing', 'candidates subset', 'corresponds fig',\n",
              "         'parameter assumes'], dtype='<U26')),\n",
              " ('organizing associative',\n",
              "  array(['organizing associative', 'catches letter', 'compute coefficients',\n",
              "         'candidates mappings', 'samples nualber', 'luminance white',\n",
              "         'summation divided', 'passageways 18m', 'accomplishes recursively',\n",
              "         'pointer arrives'], dtype='<U26')),\n",
              " ('letter fig',\n",
              "  array(['letter fig', 'candidates mappings', 'letter recognizer',\n",
              "         'parameter compactness', 'constructions associative',\n",
              "         'passageway juts', 'candidates subset', 'compression camera',\n",
              "         'passageways 18m', 'subclass structures'], dtype='<U26')),\n",
              " ('triangular technique',\n",
              "  array(['triangular technique', 'problem compression', 'letter fig',\n",
              "         'problem identifying', 'passageways 18m', 'candidates mappings',\n",
              "         '560 organizing', 'sampl fig', 'compress camera',\n",
              "         'subclass structures'], dtype='<U26')),\n",
              " ('networks recursi',\n",
              "  array(['networks recursi', 'converged 400', 'rewrites correlated',\n",
              "         'computing coefficients', 'triangular technique', 'osaka toyonaka',\n",
              "         'learns landscapes', 'toyonaka osaka', 'camera preprocessing',\n",
              "         'robot reset'], dtype='<U26')),\n",
              " ('panorama 360deg',\n",
              "  array(['panorama 360deg', 'letter recen', 'camera inquires',\n",
              "         'catches letter', '400 levels', 'observes sampled',\n",
              "         'organizing associative', 'coefficients anza',\n",
              "         'accomplishes recursively', 'notations nand'], dtype='<U26')),\n",
              " ('blem arises',\n",
              "  array(['blem arises', 'robot eyesight', 'speciiications 20dot',\n",
              "         'recognizer scans', 'problem identifying', 'passageway juts',\n",
              "         'white fig', 'nodes 145', 'reproduces criterion',\n",
              "         'robot flourishingly6'], dtype='<U26')),\n",
              " ('white fig',\n",
              "  array(['white fig', 'robot flourishingly6', 'orthogonal dimension',\n",
              "         'learns landscapes', 'head letter', 'operator subjectively',\n",
              "         'toyonaka osaka', 'pointer arrives', 'problem compression',\n",
              "         'reproduces criterion'], dtype='<U26')),\n",
              " ('camera preprocessing',\n",
              "  array(['camera preprocessing', 'suguru arimoto', 'problem compression',\n",
              "         'recursive observes', 'passageways 18m', 'recursive guarantees',\n",
              "         'toyonaka osaka', 'notations nand', 'accomplishes recursively',\n",
              "         'obstacles operator'], dtype='<U26')),\n",
              " ('roan fig',\n",
              "  array(['roan fig', 'osaka 560', 'maximal letter', 'compression camera',\n",
              "         'operator obstacles', 'recognizer scans', 'roan uni',\n",
              "         'vertically filtered', 'blem arises', 'samples likelihood'],\n",
              "        dtype='<U26')),\n",
              " ('parameters samples',\n",
              "  array(['parameters samples', 'reset periodically', 'rmbi robot',\n",
              "         'samples increases', 'camera panorama', 'camera face',\n",
              "         'parameters uniquely', 'nodes increases', 'units computing',\n",
              "         'robot eyesight'], dtype='<U26')),\n",
              " ('problem identifying',\n",
              "  array(['problem identifying', 'passageway juts', 'slitting recognizing',\n",
              "         'compression camera', 'constructions associative', 'blem arises',\n",
              "         'robot eyesight', 'units computing', 'recognizer scans',\n",
              "         'parallelogram maximal'], dtype='<U26')),\n",
              " ('sampl fig', array(['sampl fig', 'reset periodically', 'subclass structures',\n",
              "         'camera panorama', 'problem compression', 'white fig',\n",
              "         'matching recognizing', 'networks approximations', 'yin yin',\n",
              "         'parallelogram maximal'], dtype='<U26')),\n",
              " ('letter letter',\n",
              "  array(['letter letter', 'levels 202', 'contrarily parameter',\n",
              "         'realizes handwritten', 'problem criterion', 'notation candidates',\n",
              "         'networks accelerate', 'scans slant', 'relating parameters',\n",
              "         'stools handcarts'], dtype='<U26')),\n",
              " ('notation candidates',\n",
              "  array(['notation candidates', 'teaches recognizer', 'scans slant',\n",
              "         'realizes handwritten', 'levels 202', 'problem criterion',\n",
              "         'letter letter', 'stools handcarts', 'contrarily parameter',\n",
              "         'luminance white'], dtype='<U26')),\n",
              " ('oftree 522', array(['oftree 522', 'robot operator', 'samples computes',\n",
              "         'camera autonomous', 'input output', 'subset mappings',\n",
              "         'approaches samples', 'bstitu white', 'recen tly',\n",
              "         'procedures compute'], dtype='<U26')),\n",
              " ('operator subjectively',\n",
              "  array(['operator subjectively', 'synapses branching', 'white fig',\n",
              "         'simplest constructions', 'converged 400', 'learns landscapes',\n",
              "         'orthogonal dimension', 'neural randomly', 'toyonaka osaka',\n",
              "         'head letter'], dtype='<U26')),\n",
              " ('scanner fig', array(['scanner fig', 'nand nodes', 'compression admittable',\n",
              "         'blem arises', 'speciiications 20dot', 'robot eyesight',\n",
              "         'reproduces criterion', 'recognizer scans',\n",
              "         'computing coefficients', 'compactness almighty'], dtype='<U26')),\n",
              " ('letters4 global',\n",
              "  array(['letters4 global', 'computing coefficients', 'neural randomly',\n",
              "         'head letter', 'brushes obstacles', 'parameters uniquely',\n",
              "         'operator subjectively', 'problem wide', 'coefficients samples',\n",
              "         'learns landscapes'], dtype='<U26')),\n",
              " ('observes rewrites',\n",
              "  array(['observes rewrites', 'matching recognizing', 'learns landscapes',\n",
              "         'computing coefficients', 'scanner fig', 'units computing',\n",
              "         'nand nodes', 'reset periodically', 'blem arises',\n",
              "         'neural randomly'], dtype='<U26')),\n",
              " ('computing coefficients',\n",
              "  array(['computing coefficients', 'slant angular', 'letters4 global',\n",
              "         'problem wide', 'neural randomly', 'contrarily associative',\n",
              "         'robot flourishingly6', 'converged 400', 'learns landscapes',\n",
              "         'toyonaka osaka'], dtype='<U26')),\n",
              " ('neural randomly',\n",
              "  array(['neural randomly', 'computing coefficients', 'letters4 global',\n",
              "         'converged 400', 'head letter', 'operator subjectively',\n",
              "         'learns landscapes', 'candidates assumed', 'variables nodes',\n",
              "         'reproduces criterion'], dtype='<U26')),\n",
              " ('trajectory blem',\n",
              "  array(['trajectory blem', 'neural randomly', 'matching recognizing',\n",
              "         'letters4 global', 'robot white', 'sampled randomly',\n",
              "         'recursive effectiveness', 'operator subjectively',\n",
              "         'reproduces criterion', 'learns landscapes'], dtype='<U26')),\n",
              " ('computes parameters',\n",
              "  array(['computes parameters', 'reflects operator', 'sampled randomly',\n",
              "         'superimposes camera', 'converges samples', 'stools handcarts',\n",
              "         'problem criterion', 'operator subjectively', 'trajectory blem',\n",
              "         'accomplishes recursively'], dtype='<U26')),\n",
              " ('tly slitting',\n",
              "  array(['tly slitting', '800 nodes', 'physics designing', 'osaka toyonaka',\n",
              "         'letter autonomous', 'learns landscapes', 'orthogonal dimension',\n",
              "         'periodically operator', 'mobile robot',\n",
              "         'accomplishes recursively'], dtype='<U26')),\n",
              " ('learns landscapes',\n",
              "  array(['learns landscapes', 'converged 400', 'white fig',\n",
              "         'operator subjectively', 'problem wide', 'head letter',\n",
              "         'computing coefficients', 'neural randomly',\n",
              "         'reproduces criterion', 'periodically operator'], dtype='<U26')),\n",
              " ('nand nodes', array(['nand nodes', 'compression admittable', 'scanner fig',\n",
              "         'learns landscapes', 'blem arises', 'reproduces criterion',\n",
              "         'compactness almighty', 'speciiications 20dot', 'white fig',\n",
              "         'robot eyesight'], dtype='<U26')),\n",
              " ('robot white', array(['robot white', 'camera face', 'contrarily neural',\n",
              "         'brushes obstacles', 'parameter compactness',\n",
              "         'robot flourishingly6', 'speciiications 20dot', 'mobile robot',\n",
              "         'constructions associative', 'compute coefficients'], dtype='<U26')),\n",
              " ('compress camera',\n",
              "  array(['compress camera', 'robot identifies', 'laboratories fig',\n",
              "         'brushes obstacles', 'camera face', 'neural networks',\n",
              "         'subject constructing', 'head letter', 'mobile robot',\n",
              "         'recalling camera'], dtype='<U26')),\n",
              " ('synapses branching', array(['synapses branching', 'operator subjectively',\n",
              "         'simplest constructions', 'orthogonal dimension', 'white fig',\n",
              "         'luminance white', 'toyonaka osaka', 'catches letter',\n",
              "         'head letter', 'camera loosened'], dtype='<U26')),\n",
              " ('notations nand',\n",
              "  array(['notations nand', 'compactness assumed', 'recursive observes',\n",
              "         'toyonaka osaka', 'camera preprocessing', 'problem compression',\n",
              "         'periodically operator', 'simplest constructions',\n",
              "         'recursive guarantees', 'suguru arimoto'], dtype='<U26')),\n",
              " ('compute coefficients',\n",
              "  array(['compute coefficients', 'samples nualber', 'robot flourishingly6',\n",
              "         'organizing associative', 'pointer arrives',\n",
              "         'constructing associative', 'contrarily neural',\n",
              "         'camera preliminarily', 'operator teaches', 'coefficients anza'],\n",
              "        dtype='<U26')),\n",
              " ('constructing associative',\n",
              "  array(['constructing associative', 'robot flourishingly6',\n",
              "         'pointer arrives', 'operator teaches', 'robot eyesight',\n",
              "         'contrarily neural', 'head letter', 'recalling camera',\n",
              "         'periodically operator', 'recursive methodology'], dtype='<U26')),\n",
              " ('reset pointer',\n",
              "  array(['reset pointer', 'relating parameters', 'realizes handwritten',\n",
              "         'passageway juts', 'levels 202', 'robot 30kg',\n",
              "         'contrarily parameter', 'recalling camera',\n",
              "         'constructions associative', 'camera face'], dtype='<U26')),\n",
              " ('camera inquires',\n",
              "  array(['camera inquires', 'nodes memorize', 'robot arbitrarily',\n",
              "         'physics designing', 'problem compression', 'camera preprocessing',\n",
              "         'panorama 360deg', 'suguru arimoto', 'recursive observes',\n",
              "         'rewrites correlated'], dtype='<U26')),\n",
              " ('recognizer parallelogram',\n",
              "  array(['recognizer parallelogram', 'robot 30kg', 'parameter recursive',\n",
              "         'rmbi robot', 'relating parameters', 'input output', '400 levels',\n",
              "         'reset pointer', 'compute coefficients', 'parameter compactness'],\n",
              "        dtype='<U26')),\n",
              " ('relating parameters',\n",
              "  array(['relating parameters', 'reset pointer', 'realizes handwritten',\n",
              "         'contrarily parameter', 'letter letter', 'passageway juts',\n",
              "         'levels 202', 'robot 30kg', 'recalling camera', 'scans slant'],\n",
              "        dtype='<U26')),\n",
              " ('scans slant',\n",
              "  array(['scans slant', 'realizes handwritten', 'problem criterion',\n",
              "         'notation candidates', 'stools handcarts', 'teaches recognizer',\n",
              "         'levels 202', 'letter letter', 'contrarily parameter',\n",
              "         'constructing associative'], dtype='<U26')),\n",
              " ('parameter recursive',\n",
              "  array(['parameter recursive', 'recognizer accomplishes',\n",
              "         'recalling camera', 'samples increases', 'robot superimposes',\n",
              "         'relating parameters', 'rmbi robot', 'camera face',\n",
              "         'recognizer parallelogram', 'reset pointer'], dtype='<U26')),\n",
              " ('candidates assumed',\n",
              "  array(['candidates assumed', 'terminal node', 'variables nodes',\n",
              "         'head letter', 'brushes obstacles', 'camera face',\n",
              "         'problem designing', 'orthogonal dimension',\n",
              "         'robot flourishingly6', 'contrarily neural'], dtype='<U26')),\n",
              " ('camera panorama',\n",
              "  array(['camera panorama', 'reset periodically', 'roan uni', '400 levels',\n",
              "         'corresponds fig', 'sampl fig', 'nodes increases',\n",
              "         'operator obstacles', 'associative databases', 'preprocessing ion'],\n",
              "        dtype='<U26')),\n",
              " ('nodes memorize',\n",
              "  array(['nodes memorize', 'physics designing', 'camera inquires',\n",
              "         'recognizing hand', 'loo fig', 'typographic english',\n",
              "         'robot arbitrarily', 'suguru arimoto', 'nodes frequency',\n",
              "         'recursive guarantees'], dtype='<U26')),\n",
              " ('loo fig', array(['loo fig', 'recognizing hand', 'suffices camera',\n",
              "         'universality neural', 'nodes memorize', 'obstacles operator',\n",
              "         'suguru arimoto', 'osaka toyonaka', 'compactness assumed',\n",
              "         'operator subjectively'], dtype='<U26')),\n",
              " ('typographic english',\n",
              "  array(['typographic english', 'nodes memorize', 'robot arbitrarily',\n",
              "         'rewrites correlated', 'robot reset', 'rewri ting',\n",
              "         'nodes frequency', 'physics designing', 'compactness almighty',\n",
              "         'suzuki suguru'], dtype='<U26')),\n",
              " ('reset periodically',\n",
              "  array(['reset periodically', 'sampl fig', 'camera panorama',\n",
              "         'nodes increases', 'matching recognizing', 'learns landscapes',\n",
              "         'robot eyesight', '400 levels', 'parameters samples',\n",
              "         'parallelogram maximal'], dtype='<U26')),\n",
              " ('periodically operator',\n",
              "  array(['periodically operator', 'head letter', 'problem wide',\n",
              "         'coefficients samples', 'brushes obstacles',\n",
              "         'orthogonal dimension', 'pointer arrives', 'subject constructing',\n",
              "         'neural networks', 'converged 400'], dtype='<U26')),\n",
              " ('preprocessing ion',\n",
              "  array(['preprocessing ion', 'roan uni', 'operator obstacles',\n",
              "         'associative database', 'recognizer scans',\n",
              "         'associative databases', 'camera panorama', 'letters3 matching',\n",
              "         '400 levels', 'teaches recognizer'], dtype='<U26')),\n",
              " ('problem wide',\n",
              "  array(['problem wide', 'slant angular', 'periodically operator',\n",
              "         'coefficients samples', 'head letter', 'contrarily associative',\n",
              "         'brushes obstacles', 'computing coefficients',\n",
              "         'robot flourishingly6', 'variables nodes'], dtype='<U26')),\n",
              " ('coefficients samples',\n",
              "  array(['coefficients samples', 'neural networks', 'head letter',\n",
              "         'brushes obstacles', 'periodically operator', 'operator teaches',\n",
              "         'problem wide', 'contrarily neural', 'converged 400',\n",
              "         'constructing associative'], dtype='<U26')),\n",
              " ('structures learns',\n",
              "  array(['structures learns', 'parallelogram maximal', 'head letter',\n",
              "         'periodically operator', 'subject constructing',\n",
              "         'stools handcarts', 'white fig', 'learns landscapes',\n",
              "         'pointer arrives', 'problem wide'], dtype='<U26')),\n",
              " ('parameter assumes',\n",
              "  array(['parameter assumes', 'reproduces criterion', 'corresponds fig',\n",
              "         'networks structures', 'accomplishes recursively',\n",
              "         'summation divided', 'orthogonal dimension', 'white fig',\n",
              "         '400 levels', 'luminance white'], dtype='<U26')),\n",
              " ('letter recen',\n",
              "  array(['letter recen', 'parameter assumes', 'panorama 360deg',\n",
              "         '400 levels', 'robot arbitrarily', 'orthogonal dimension',\n",
              "         'osaka toyonaka', 'corresponds fig', 'nodes frequency',\n",
              "         'catches letter'], dtype='<U26')),\n",
              " ('coefficients anza', array(['coefficients anza', 'camera preliminarily',\n",
              "         'compute coefficients', 'robot flourishingly6',\n",
              "         'constructing associative', 'speciiications 20dot',\n",
              "         'catches letter', 'recalling camera', 'pointer arrives',\n",
              "         'nualber sampl'], dtype='<U26')),\n",
              " ('networks eternally',\n",
              "  array(['networks eternally', 'obstacles operator', 'subset mappings',\n",
              "         'bstitu white', 'recognizing styles', 'samples computes',\n",
              "         'camera autonomous', 'uni fig', 'recursive effectiveness',\n",
              "         'samples increases'], dtype='<U26')),\n",
              " ('nualber sampl',\n",
              "  array(['nualber sampl', 'recognizing typographic', 'robot flourishingly6',\n",
              "         'speciiications 20dot', 'camera face', 'robot superimposes',\n",
              "         'samples increases', 'recalling camera', 'orthogonal dimension',\n",
              "         'passageway juts'], dtype='<U26')),\n",
              " ('autonomous mobile',\n",
              "  array(['autonomous mobile', 'brushes obstacles', 'robot eyesight',\n",
              "         'orthogonal dimension', 'reproduces criterion', 'slant angular',\n",
              "         'contrarily associative', 'robot flourishingly6', 'head letter',\n",
              "         'camera face'], dtype='<U26')),\n",
              " ('networks accelerate',\n",
              "  array(['networks accelerate', 'levels 202', 'realizes handwritten',\n",
              "         'networks approximations', 'contrarily parameter',\n",
              "         'luminance white', 'letter letter', 'robot recalling',\n",
              "         'stools handcarts', 'problem criterion'], dtype='<U26')),\n",
              " ('levels 202', array(['levels 202', 'realizes handwritten', 'letter letter',\n",
              "         'contrarily parameter', 'networks accelerate', 'head letter',\n",
              "         'notation candidates', 'problem criterion', 'subject constructing',\n",
              "         'brushes obstacles'], dtype='<U26')),\n",
              " ('contrarily parameter',\n",
              "  array(['contrarily parameter', 'realizes handwritten', 'letter letter',\n",
              "         'levels 202', 'recognizing styles', 'relating parameters',\n",
              "         'networks accelerate', 'terminal node', 'reset pointer',\n",
              "         'notation candidates'], dtype='<U26')),\n",
              " ('realizes handwritten',\n",
              "  array(['realizes handwritten', 'contrarily parameter', 'levels 202',\n",
              "         'letter letter', 'problem criterion', 'scans slant',\n",
              "         'stools handcarts', 'networks accelerate', 'relating parameters',\n",
              "         'notation candidates'], dtype='<U26')),\n",
              " ('teaches recognizer',\n",
              "  array(['teaches recognizer', 'notation candidates', 'levels 202',\n",
              "         'scans slant', 'realizes handwritten', 'problem criterion',\n",
              "         'roan uni', 'stools handcarts', 'periodically operator',\n",
              "         'luminance white'], dtype='<U26')),\n",
              " ('compu ters',\n",
              "  array(['compu ters', 'realizes handwritten', 'networks structures',\n",
              "         'stools handcarts', 'networks accelerate',\n",
              "         'compression admittable', 'structures learns', 'problem criterion',\n",
              "         'nand nodes', 'reproduces criterion'], dtype='<U26')),\n",
              " ('camera face',\n",
              "  array(['camera face', 'contrarily neural', 'brushes obstacles',\n",
              "         'mobile robot', 'pointer arrives', 'recursive effectiveness',\n",
              "         'speciiications 20dot', 'camera loosened', 'uni fig',\n",
              "         'head letter'], dtype='<U26')),\n",
              " ('reflects operator',\n",
              "  array(['reflects operator', 'superimposes camera', 'computes parameters',\n",
              "         'notation candidates', 'converges samples', 'stools handcarts',\n",
              "         'problem criterion', 'scans slant', 'luminance white',\n",
              "         'teaches recognizer'], dtype='<U26')),\n",
              " ('candidates subset',\n",
              "  array(['candidates subset', 'compression camera', 'letter fig',\n",
              "         'letter recognizer', 'compute coefficients', 'passageway juts',\n",
              "         '560 organizing', 'compress camera', 'computation proportionally',\n",
              "         'subclass structures'], dtype='<U26')),\n",
              " ('recursive methodology',\n",
              "  array(['recursive methodology', 'randomly algorithm', 'uniquely samples',\n",
              "         'samples likelihood', 'robot flourishingly6',\n",
              "         'methodologies problem', 'camera 55deg', 'nodes 145',\n",
              "         'suzuki suguru', 'constructing associative'], dtype='<U26')),\n",
              " ('catches letter',\n",
              "  array(['catches letter', 'organizing associative', 'luminance white',\n",
              "         'accomplishes recursively', 'orthogonal dimension',\n",
              "         'pointer arrives', 'simplest constructions',\n",
              "         'periodically operator', 'summation divided', 'corresponds fig'],\n",
              "        dtype='<U26')),\n",
              " ('passageway juts', array(['passageway juts', 'constructions associative',\n",
              "         'parameter compactness', 'compression camera',\n",
              "         'problem identifying', 'slitting recognizing',\n",
              "         'speciiications 20dot', 'recalling camera', 'robot flourishingly6',\n",
              "         'uniquely samples'], dtype='<U26')),\n",
              " ('constructions associative',\n",
              "  array(['constructions associative', 'passageway juts',\n",
              "         'parameter compactness', 'compression camera',\n",
              "         'problem identifying', 'uniquely samples', 'robot flourishingly6',\n",
              "         'slitting recognizing', 'recalling camera',\n",
              "         'recursive methodology'], dtype='<U26')),\n",
              " ('nodes increases',\n",
              "  array(['nodes increases', 'reset periodically', 'parameter compactness',\n",
              "         'camera panorama', 'summation divided', 'letter fig',\n",
              "         'robot eyesight', 'recursi recursive', 'robot white',\n",
              "         'letter recognizer'], dtype='<U26')),\n",
              " ('parameter compactness',\n",
              "  array(['parameter compactness', 'constructions associative',\n",
              "         'passageway juts', 'letter recognizer', 'compression camera',\n",
              "         'recognizer accomplishes', 'letter fig', 'robot flourishingly6',\n",
              "         'robot white', 'methodologies problem'], dtype='<U26')),\n",
              " ('compression camera',\n",
              "  array(['compression camera', 'passageway juts', 'slitting recognizing',\n",
              "         'problem identifying', 'constructions associative',\n",
              "         'parameter compactness', 'candidates subset', 'letter recognizer',\n",
              "         'recalling camera', 'letter fig'], dtype='<U26')),\n",
              " ('slitting recognizing', array(['slitting recognizing', 'compression camera',\n",
              "         'problem identifying', 'passageway juts',\n",
              "         'constructions associative', 'letter recognizer',\n",
              "         'parameter compactness', 'nodes 145', 'recalling camera',\n",
              "         'parallelogram maximal'], dtype='<U26')),\n",
              " ('letter recognizer',\n",
              "  array(['letter recognizer', 'letter fig', 'parameter compactness',\n",
              "         'passageway juts', 'compression camera', 'slitting recognizing',\n",
              "         'constructions associative', 'candidates subset',\n",
              "         'problem identifying', 'nodes 145'], dtype='<U26')),\n",
              " ('slant angular',\n",
              "  array(['slant angular', 'contrarily associative', 'problem wide',\n",
              "         'robot flourishingly6', 'computing coefficients',\n",
              "         'speciiications 20dot', 'pointer arrives', 'units computing',\n",
              "         'robot eyesight', 'recognizer scans'], dtype='<U26')),\n",
              " ('recognizer scans',\n",
              "  array(['recognizer scans', 'speciiications 20dot', 'units computing',\n",
              "         'blem arises', 'contrarily associative', 'robot flourishingly6',\n",
              "         'robot eyesight', 'associative databases', 'passageway juts',\n",
              "         'slant angular'], dtype='<U26')),\n",
              " ('units computing',\n",
              "  array(['units computing', 'recognizer scans', 'robot eyesight',\n",
              "         'slant angular', 'speciiications 20dot', 'robot flourishingly6',\n",
              "         'pointer arrives', 'problem identifying', 'contrarily associative',\n",
              "         'passageway juts'], dtype='<U26')),\n",
              " ('contrarily associative',\n",
              "  array(['contrarily associative', 'slant angular', 'robot flourishingly6',\n",
              "         'speciiications 20dot', 'pointer arrives', 'problem wide',\n",
              "         'operator teaches', 'recognizer scans', 'robot eyesight',\n",
              "         'contrarily neural'], dtype='<U26')),\n",
              " ('robot flourishingly6',\n",
              "  array(['robot flourishingly6', 'contrarily associative',\n",
              "         'speciiications 20dot', 'constructing associative',\n",
              "         'slant angular', 'pointer arrives', 'operator teaches',\n",
              "         'robot eyesight', 'recursive methodology',\n",
              "         'constructions associative'], dtype='<U26')),\n",
              " ('recalling camera',\n",
              "  array(['recalling camera', 'passageway juts', 'constructing associative',\n",
              "         'subject constructing', 'constructions associative',\n",
              "         'robot flourishingly6', 'head letter', 'pointer arrives',\n",
              "         'recursive methodology', 'compression camera'], dtype='<U26')),\n",
              " ('head letter',\n",
              "  array(['head letter', 'brushes obstacles', 'coefficients samples',\n",
              "         'periodically operator', 'orthogonal dimension',\n",
              "         'operator teaches', 'pointer arrives', 'problem wide',\n",
              "         'neural networks', 'robot flourishingly6'], dtype='<U26')),\n",
              " ('pointer arrives',\n",
              "  array(['pointer arrives', 'robot flourishingly6', 'brushes obstacles',\n",
              "         'slant angular', 'camera face', 'constructing associative',\n",
              "         'head letter', 'operator teaches', 'contrarily associative',\n",
              "         'robot eyesight'], dtype='<U26')),\n",
              " ('camera preliminarily', array(['camera preliminarily', 'coefficients anza',\n",
              "         'compute coefficients', 'speciiications 20dot',\n",
              "         'organizing associative', 'robot flourishingly6', 'catches letter',\n",
              "         'samples nualber', 'robot eyesight', 'slant angular'], dtype='<U26')),\n",
              " ('recognizing typographic',\n",
              "  array(['recognizing typographic', 'nualber sampl', 'luminance white',\n",
              "         'camera face', 'robot superimposes', 'slant angular',\n",
              "         'contrarily associative', 'pointer arrives', 'samples nualber',\n",
              "         'mobile robot'], dtype='<U26')),\n",
              " ('simplest constructions', array(['simplest constructions', 'toyonaka osaka',\n",
              "         'operator subjectively', 'catches letter', 'synapses branching',\n",
              "         'orthogonal dimension', 'pointer arrives', 'slant angular',\n",
              "         'white fig', 'luminance white'], dtype='<U26')),\n",
              " ('samples nualber', array(['samples nualber', 'compute coefficients',\n",
              "         'organizing associative', 'contrarily associative',\n",
              "         'pointer arrives', 'brushes obstacles', 'luminance white',\n",
              "         'contrarily neural', 'slant angular', 'head letter'], dtype='<U26')),\n",
              " ('learns input', array(['learns input', 'roan uni', 'associative database',\n",
              "         'teaches recognizer', 'preprocessing ion', 'candidates subset',\n",
              "         'operator obstacles', 'recognizer scans', 'units computing',\n",
              "         'subclass structures'], dtype='<U26')),\n",
              " ('samples computes',\n",
              "  array(['samples computes', 'approaches samples', 'camera autonomous',\n",
              "         'input output', 'subset mappings', 'bstitu white',\n",
              "         'procedures compute', 'minimization problem', 'visual camera',\n",
              "         'recen tly'], dtype='<U26')),\n",
              " ('suguru arimoto',\n",
              "  array(['suguru arimoto', 'camera preprocessing', 'recursive observes',\n",
              "         'problem compression', 'toyonaka osaka',\n",
              "         'accomplishes recursively', 'recursive guarantees',\n",
              "         'passageways 18m', 'osaka toyonaka', 'notations nand'],\n",
              "        dtype='<U26')),\n",
              " ('nodes frequency',\n",
              "  array(['nodes frequency', 'recursive guarantees', 'toyonaka osaka',\n",
              "         'osaka toyonaka', 'suguru arimoto', 'notations nand',\n",
              "         'simplest constructions', 'compactness assumed',\n",
              "         'camera preprocessing', 'operator subjectively'], dtype='<U26')),\n",
              " ('rewri ting', array(['rewri ting', 'approaches samples', 'camera autonomous',\n",
              "         'samples computes', 'typographic english', 'subset mappings',\n",
              "         'recen tly', 'procedures compute', 'recognizer interaction',\n",
              "         'configuration autonomous'], dtype='<U26')),\n",
              " ('tion defines',\n",
              "  array(['tion defines', 'approaches samples', 'subset mappings',\n",
              "         'recognizer interaction', 'configuration autonomous',\n",
              "         'input output', 'bstitu white', 'minimization problem',\n",
              "         'problem criterion', 'reflects operator'], dtype='<U26')),\n",
              " ('camera autonomous', array(['camera autonomous', 'input output', 'recen tly',\n",
              "         'subset mappings', 'bstitu white', 'approaches samples',\n",
              "         'samples computes', 'minimization problem', 'procedures compute',\n",
              "         'visual camera'], dtype='<U26')),\n",
              " ('visual camera',\n",
              "  array(['visual camera', 'minimization problem', 'camera autonomous',\n",
              "         'input output', 'approaches samples', 'subset mappings',\n",
              "         'bstitu white', 'samples computes', 'configuration autonomous',\n",
              "         'recen tly'], dtype='<U26')),\n",
              " ('recen tly', array(['recen tly', 'camera autonomous', 'subset mappings',\n",
              "         'bstitu white', 'input output', 'procedures compute',\n",
              "         'minimization problem', 'samples computes', 'approaches samples',\n",
              "         'visual camera'], dtype='<U26')),\n",
              " ('recognizer interaction',\n",
              "  array(['recognizer interaction', 'subset mappings', 'samples computes',\n",
              "         'bstitu white', 'procedures compute', 'approaches samples',\n",
              "         'camera autonomous', 'minimization problem', 'recen tly',\n",
              "         'input output'], dtype='<U26')),\n",
              " ('bstitu white', array(['bstitu white', 'subset mappings', 'input output',\n",
              "         'camera autonomous', 'procedures compute',\n",
              "         'configuration autonomous', 'recen tly', 'minimization problem',\n",
              "         'samples computes', 'approaches samples'], dtype='<U26')),\n",
              " ('input output', array(['input output', 'bstitu white', 'camera autonomous',\n",
              "         'subset mappings', 'procedures compute', 'approaches samples',\n",
              "         'samples computes', 'minimization problem', 'visual camera',\n",
              "         'recen tly'], dtype='<U26')),\n",
              " ('subset mappings', array(['subset mappings', 'bstitu white', 'input output',\n",
              "         'camera autonomous', 'approaches samples', 'samples computes',\n",
              "         'configuration autonomous', 'minimization problem', 'recen tly',\n",
              "         'procedures compute'], dtype='<U26')),\n",
              " ('configuration autonomous',\n",
              "  array(['configuration autonomous', 'subset mappings', 'bstitu white',\n",
              "         'input output', 'camera autonomous', 'approaches samples',\n",
              "         'minimization problem', 'visual camera', 'procedures compute',\n",
              "         'recen tly'], dtype='<U26')),\n",
              " ('approaches samples',\n",
              "  array(['approaches samples', 'samples computes', 'subset mappings',\n",
              "         'camera autonomous', 'input output', 'minimization problem',\n",
              "         'visual camera', 'bstitu white', 'configuration autonomous',\n",
              "         'procedures compute'], dtype='<U26')),\n",
              " ('procedures compute',\n",
              "  array(['procedures compute', 'input output', 'camera autonomous',\n",
              "         'bstitu white', 'subset mappings', 'recen tly', 'samples computes',\n",
              "         'approaches samples', 'minimization problem',\n",
              "         'recognizer interaction'], dtype='<U26')),\n",
              " ('vertically filtered',\n",
              "  array(['vertically filtered', 'camera autonomous', 'osaka 560',\n",
              "         'bstitu white', 'recen tly', 'roan fig', 'input output',\n",
              "         'procedures compute', 'subset mappings', 'samples computes'],\n",
              "        dtype='<U26')),\n",
              " ('recursive observes', array(['recursive observes', 'camera preprocessing',\n",
              "         'problem compression', 'notations nand', 'suguru arimoto',\n",
              "         'passageways 18m', 'recursive guarantees',\n",
              "         'accomplishes recursively', 'toyonaka osaka',\n",
              "         'periodically operator'], dtype='<U26')),\n",
              " ('superimposes camera',\n",
              "  array(['superimposes camera', 'reflects operator', 'computes parameters',\n",
              "         'problem criterion', 'scans slant', 'notation candidates',\n",
              "         'stools handcarts', 'luminance white', 'teaches recognizer',\n",
              "         'levels converged'], dtype='<U26')),\n",
              " ('contrarily neural',\n",
              "  array(['contrarily neural', 'camera face', 'brushes obstacles',\n",
              "         'operator teaches', 'neural networks', 'mobile robot',\n",
              "         'robot eyesight', 'coefficients samples', 'pointer arrives',\n",
              "         'robot flourishingly6'], dtype='<U26')),\n",
              " ('compression admittable',\n",
              "  array(['compression admittable', 'nand nodes', 'scanner fig',\n",
              "         'blem arises', 'robot eyesight', 'compactness almighty',\n",
              "         'speciiications 20dot', 'compu ters', 'white fig',\n",
              "         'recognizer scans'], dtype='<U26')),\n",
              " ('criterion operator',\n",
              "  array(['criterion operator', 'robot eyesight', 'neural networks',\n",
              "         'operator teaches', 'coefficients samples', 'contrarily neural',\n",
              "         'units computing', 'pointer arrives', 'parallelogram maximal',\n",
              "         'subject constructing'], dtype='<U26')),\n",
              " ('compactness assumed',\n",
              "  array(['compactness assumed', 'notations nand', 'toyonaka osaka',\n",
              "         'coefficients samples', 'recursive guarantees', 'osaka toyonaka',\n",
              "         'nodes frequency', 'computing coefficients',\n",
              "         'simplest constructions', 'problem compression'], dtype='<U26')),\n",
              " ('recursi recursive',\n",
              "  array(['recursi recursive', 'summation divided', 'builds organizing',\n",
              "         'parameter compactness', 'compute coefficients', 'passageways 18m',\n",
              "         'robot flourishingly6', 'nodes increases',\n",
              "         'constructing associative', 'problem compression'], dtype='<U26')),\n",
              " ('summation divided',\n",
              "  array(['summation divided', 'corresponds fig', 'levels converged',\n",
              "         'accomplishes recursively', 'luminance white', 'catches letter',\n",
              "         'mapping reals', 'organizing associative', 'parameter assumes',\n",
              "         'recursi recursive'], dtype='<U26')),\n",
              " ('letters3 matching',\n",
              "  array(['letters3 matching', 'operator obstacles', 'roan uni',\n",
              "         'preprocessing ion', 'converges samples', 'associative database',\n",
              "         'sampl fig', 'roan fig', 'recognizer scans',\n",
              "         'associative databases'], dtype='<U26')),\n",
              " ('corresponds fig',\n",
              "  array(['corresponds fig', 'summation divided', 'accomplishes recursively',\n",
              "         'catches letter', 'parameter assumes', 'stools handcarts',\n",
              "         'luminance white', '400 levels', 'levels converged',\n",
              "         'periodically operator'], dtype='<U26')),\n",
              " ('mapping reals',\n",
              "  array(['mapping reals', 'levels converged', 'robot flourishingly6',\n",
              "         'summation divided', 'speciiications 20dot',\n",
              "         'parameter compactness', 'constructions associative',\n",
              "         'robot eyesight', 'passageway juts', 'constructing associative'],\n",
              "        dtype='<U26')),\n",
              " ('observes sampled',\n",
              "  array(['observes sampled', 'panorama 360deg', 'compactness assumed',\n",
              "         'recursive guarantees', 'nodes frequency', 'notations nand',\n",
              "         'computing coefficients', 'nodes memorize', 'camera inquires',\n",
              "         'toyonaka osaka'], dtype='<U26')),\n",
              " ('computation proportionally',\n",
              "  array(['computation proportionally', 'candidates subset',\n",
              "         'recursi recursive', 'compression camera', 'builds organizing',\n",
              "         'letter fig', 'reproduces criterion', 'toyonaka osaka',\n",
              "         'robot white', 'compress camera'], dtype='<U26')),\n",
              " ('samples increases', array(['samples increases', 'robot superimposes',\n",
              "         'recognizer accomplishes', 'terminal node', 'pointer arrives',\n",
              "         'recognizing styles', 'camera face', 'robot eyesight',\n",
              "         'robot flourishingly6', 'operator brushes'], dtype='<U26')),\n",
              " ('truncating finitely',\n",
              "  array(['truncating finitely', 'white fig', 'variables nodes',\n",
              "         'head letter', 'parameters uniquely', 'operator subjectively',\n",
              "         'laboratories fig', 'candidates assumed', 'neural randomly',\n",
              "         'coefficients samples'], dtype='<U26')),\n",
              " ('recognizing styles',\n",
              "  array(['recognizing styles', 'terminal node', 'operator brushes',\n",
              "         'contrarily parameter', 'brushes obstacles', 'samples increases',\n",
              "         'variables nodes', 'camera face', 'head letter',\n",
              "         'orthogonal dimension'], dtype='<U26')),\n",
              " ('parameters uniquely',\n",
              "  array(['parameters uniquely', 'head letter', 'pointer arrives',\n",
              "         'periodically operator', 'coefficients samples', 'problem wide',\n",
              "         'neural networks', 'subject constructing', 'laboratories fig',\n",
              "         'letters4 global'], dtype='<U26')),\n",
              " ('samples likelihood',\n",
              "  array(['samples likelihood', 'uniquely samples', 'suzuki suguru',\n",
              "         'nodes 145', 'recursive methodology', 'camera 55deg',\n",
              "         'randomly algorithm', 'compactness almighty', 'robot camera',\n",
              "         'passageway juts'], dtype='<U26')),\n",
              " ('suffices camera',\n",
              "  array(['suffices camera', 'universality neural', 'loo fig',\n",
              "         'recognizing hand', 'simplest constructions', 'toyonaka osaka',\n",
              "         'osaka toyonaka', 'notations nand', 'nodes frequency',\n",
              "         'compactness assumed'], dtype='<U26')),\n",
              " ('luminance white', array(['luminance white', 'catches letter', 'camera face',\n",
              "         'pointer arrives', 'contrarily neural', 'neural networks',\n",
              "         'mobile robot', 'summation divided', 'periodically operator',\n",
              "         'slant angular'], dtype='<U26')),\n",
              " ('compactness almighty',\n",
              "  array(['compactness almighty', 'nodes 145', 'uniquely samples',\n",
              "         'suzuki suguru', 'samples likelihood', 'camera 55deg',\n",
              "         'methodologies problem', 'recursive methodology', 'blem arises',\n",
              "         'builds organizing'], dtype='<U26')),\n",
              " ('universality neural',\n",
              "  array(['universality neural', 'suffices camera', 'loo fig',\n",
              "         'recognizing hand', 'simplest constructions', 'notations nand',\n",
              "         'compactness assumed', 'suguru arimoto', 'networks approximations',\n",
              "         'recursive observes'], dtype='<U26')),\n",
              " ('reproduces criterion',\n",
              "  array(['reproduces criterion', 'parameter assumes', 'white fig',\n",
              "         'robot flourishingly6', 'head letter', 'speciiications 20dot',\n",
              "         'blem arises', 'robot eyesight', 'camera face', 'camera loosened'],\n",
              "        dtype='<U26')),\n",
              " ('recognizer accomplishes',\n",
              "  array(['recognizer accomplishes', 'parameter compactness',\n",
              "         'passageway juts', 'robot superimposes', 'samples increases',\n",
              "         'constructions associative', 'parameter recursive',\n",
              "         'recalling camera', 'slitting recognizing', 'compression camera'],\n",
              "        dtype='<U26')),\n",
              " ('letter autonomous',\n",
              "  array(['letter autonomous', 'learns landscapes', 'operator subjectively',\n",
              "         'converged 400', 'orthogonal dimension', 'camera loosened',\n",
              "         'reproduces criterion', 'osaka toyonaka', 'white fig',\n",
              "         'periodically operator'], dtype='<U26')),\n",
              " ('orthogonal dimension',\n",
              "  array(['orthogonal dimension', 'head letter', 'periodically operator',\n",
              "         'brushes obstacles', 'camera face', 'white fig', 'osaka toyonaka',\n",
              "         'variables nodes', 'robot flourishingly6', 'candidates assumed'],\n",
              "        dtype='<U26')),\n",
              " ('levels converged',\n",
              "  array(['levels converged', 'mapping reals', 'summation divided',\n",
              "         'catches letter', 'accomplishes recursively', 'luminance white',\n",
              "         'corresponds fig', 'stools handcarts', 'problem criterion',\n",
              "         'parameter assumes'], dtype='<U26')),\n",
              " ('recognizing hand', array(['recognizing hand', 'loo fig', 'suffices camera',\n",
              "         'universality neural', 'nodes memorize', 'obstacles operator',\n",
              "         'osaka toyonaka', 'suguru arimoto', 'operator subjectively',\n",
              "         'simplest constructions'], dtype='<U26')),\n",
              " ('accomplishes recursively',\n",
              "  array(['accomplishes recursively', 'networks structures',\n",
              "         'corresponds fig', 'catches letter', 'summation divided',\n",
              "         'periodically operator', 'levels converged', 'osaka toyonaka',\n",
              "         'parameter assumes', 'suguru arimoto'], dtype='<U26')),\n",
              " ('800 nodes',\n",
              "  array(['800 nodes', 'accomplishes recursively', 'corresponds fig',\n",
              "         'operator subjectively', 'luminance white', 'stools handcarts',\n",
              "         'networks structures', 'tly slitting', 'levels 202',\n",
              "         'problem criterion'], dtype='<U26')),\n",
              " ('converges samples',\n",
              "  array(['converges samples', 'roan uni', 'problem compression',\n",
              "         'problem criterion', 'stools handcarts', 'associative database',\n",
              "         'subclass structures', 'periodically operator', 'sampl fig',\n",
              "         'letters3 matching'], dtype='<U26')),\n",
              " ('levels oftree', array(['levels oftree', 'letter recognizer', 'letter fig',\n",
              "         'candidates subset', 'camera loosened', 'variables nodes',\n",
              "         'parameters samples', 'compress camera', 'white fig',\n",
              "         'subclass structures'], dtype='<U26')),\n",
              " ('recursive guarantees',\n",
              "  array(['recursive guarantees', 'nodes frequency', 'camera preprocessing',\n",
              "         'recursive observes', 'suguru arimoto', 'problem compression',\n",
              "         'obstacles operator', 'notations nand', 'toyonaka osaka',\n",
              "         'osaka toyonaka'], dtype='<U26')),\n",
              " ('parallelogram maximal',\n",
              "  array(['parallelogram maximal', 'structures learns', 'robot eyesight',\n",
              "         'problem identifying', 'operator teaches',\n",
              "         'constructing associative', 'pointer arrives', 'white fig',\n",
              "         'robot flourishingly6', 'neural networks'], dtype='<U26')),\n",
              " ('recursive effectiveness',\n",
              "  array(['recursive effectiveness', 'uni fig', 'camera face',\n",
              "         'brushes obstacles', 'camera loosened', 'contrarily neural',\n",
              "         'candidates assumed', 'mobile robot', 'terminal node',\n",
              "         'robot white'], dtype='<U26')),\n",
              " ('matching recognizing',\n",
              "  array(['matching recognizing', 'reset periodically', 'trajectory blem',\n",
              "         'sampl fig', 'observes rewrites', 'units computing',\n",
              "         'camera panorama', 'associative database', 'learns landscapes',\n",
              "         'sampled randomly'], dtype='<U26')),\n",
              " ('converged 400',\n",
              "  array(['converged 400', 'learns landscapes', 'periodically operator',\n",
              "         'head letter', 'coefficients samples', 'problem wide',\n",
              "         'operator subjectively', 'brushes obstacles', 'neural randomly',\n",
              "         'computing coefficients'], dtype='<U26')),\n",
              " ('400 levels', array(['400 levels', 'roan uni', 'associative databases',\n",
              "         'corresponds fig', 'camera panorama', 'blem arises',\n",
              "         'parameter assumes', 'reproduces criterion', 'white fig',\n",
              "         'summation divided'], dtype='<U26')),\n",
              " ('uniquely samples',\n",
              "  array(['uniquely samples', 'camera 55deg', 'samples likelihood',\n",
              "         'suzuki suguru', 'randomly algorithm', 'nodes 145',\n",
              "         'recursive methodology', 'constructions associative',\n",
              "         'compactness almighty', 'robot flourishingly6'], dtype='<U26')),\n",
              " ('maximal letter',\n",
              "  array(['maximal letter', 'osaka 560', 'roan fig', 'vertically filtered',\n",
              "         'operator obstacles', 'roan uni', 'problem compression',\n",
              "         'letters3 matching', 'camera preprocessing', 'subclass structures'],\n",
              "        dtype='<U26')),\n",
              " ('100 distinguishable',\n",
              "  array(['100 distinguishable', 'orthogonal dimension', '400 levels',\n",
              "         'synapses branching', 'simplest constructions', 'robot eyesight',\n",
              "         'osaka toyonaka', 'nodes increases', 'camera panorama',\n",
              "         'letter autonomous'], dtype='<U26'))]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST 2 - SENTENCE BERT : TEST/SAMPLE For 1 TIME WINDOW"
      ],
      "metadata": {
        "id": "9mImHMxgn63F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SBERT\n",
        "!pip install sentence_transformers "
      ],
      "metadata": {
        "id": "fw9T8bJ2owxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42262ee8-71dd-402c-cc27-75fbed07f131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.0+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.96)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.0+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.8.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Custom Network From Scratch\n",
        "\n",
        "# https://www.sbert.net/docs/training/overview.html\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "\n",
        "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "5008c41fab98446192efb54edeb90b5e",
            "08132ed3ec6540e7ad118fafbb834f3f",
            "a03e32d09384405ab7dd1b0c4b115ca2",
            "493a920f49ec4468a04b91e337b1e430",
            "df17bd23e4064464926c0a9aba307316",
            "0d58cc9afb684653bda3e3bf88a60d98",
            "b59a6ee8e5af4a20abd0e43afe5f1dfb",
            "78ac7abb7b554068a567ba8effe2aeec",
            "7d3581295fbf47aea94c4decac824446",
            "6971da95bf5347a79ff91d6babf6294e",
            "510c071f783241b7b8adb3d3e7ca5e03",
            "ba4f52e9467444cc80f4a1dcfe780d77",
            "2363d13ea96a406e8bd042d537bda599",
            "6305c5a1e8764696ace560778780bc9e",
            "23f4177328454c28ab97b6a139213d82",
            "49b53f150ab648348f88d05daa6eb65d",
            "58cb02b3b8174959912153dff4971fc8",
            "4349a979dc1a4542844befb7fe5aa20b",
            "80a3bfa1a68d4b999c77c9e444202bc8",
            "a08249b5f60e4d1c9b5b0213ea92812f",
            "bd77407844dc4f51a2497a105d424288",
            "3d3cdc3640a94926a4e7c618569bcadf"
          ]
        },
        "id": "zhXxh_HfpE5N",
        "outputId": "d5d8fa27-8c70-4572-baf6-0a39894a0dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5008c41fab98446192efb54edeb90b5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba4f52e9467444cc80f4a1dcfe780d77"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained Model // Comment this Cell to use the Custom Model in the above cell\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n"
      ],
      "metadata": {
        "id": "h4KI12CF1YmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Word Embeddings Using SBERT encode\n",
        "doc_embedding = model.encode(sent_tokens[:100])\n",
        "candidate_keyword_embedding = model.encode(candidate_keywords_bow)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WyXmtWiRq-pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(candidate_keyword_embedding[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sVI_S8vzpcF",
        "outputId": "1c8d4896-1ba0-4ed9-8c24-2c8509edb93b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Textual Similarity b/w corpus and candidate keywords\n",
        "from sentence_transformers import util\n",
        "\n",
        "cosine_scores = util.cos_sim(doc_embedding, candidate_keyword_embedding)\n",
        "\n",
        "print(len(candidate_keyword_embedding))\n",
        "\n",
        "top_n = 100\n",
        "keywords = [candidate_keywords_bow[index] for index in cosine_scores.argsort()[0][-top_n:]]\n",
        "keywords\n"
      ],
      "metadata": {
        "id": "_vypzg5cuEO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc3df47-f1b1-4410-c7f9-64d30b9d124a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['incorrect',\n",
              " 'negligible',\n",
              " 'free',\n",
              " 'contrarily',\n",
              " 'furthermore',\n",
              " 'problem',\n",
              " 'finite',\n",
              " 'implicitly',\n",
              " 'assumed',\n",
              " 'eternally',\n",
              " 'subject',\n",
              " 'frequency',\n",
              " 'attains',\n",
              " '400',\n",
              " 'observes',\n",
              " 'dissimilari',\n",
              " 'increases',\n",
              " 'denote',\n",
              " 'head',\n",
              " 'levels',\n",
              " 'letter',\n",
              " 'fig',\n",
              " 'recognizing',\n",
              " 'learns',\n",
              " '18m',\n",
              " 'defines',\n",
              " 'parameter',\n",
              " 'hand',\n",
              " 'camera',\n",
              " 'min',\n",
              " 'juts',\n",
              " 'approaches',\n",
              " 'preliminarily',\n",
              " 'compactness',\n",
              " 'samples increases',\n",
              " 'parameters',\n",
              " 'obstacles',\n",
              " 'criterion',\n",
              " 'output',\n",
              " 'luminance',\n",
              " 'white',\n",
              " 'reset',\n",
              " 'yin',\n",
              " '800',\n",
              " 'compression',\n",
              " 'converges',\n",
              " 'nand',\n",
              " 'converged',\n",
              " 'eyesight',\n",
              " 'structures',\n",
              " 'head letter',\n",
              " 'coefficients',\n",
              " 'handwritten',\n",
              " 'recognizer',\n",
              " 'passageway',\n",
              " 'samples',\n",
              " 'recursive',\n",
              " 'computes',\n",
              " 'white fig',\n",
              " 'nodes',\n",
              " 'dimension',\n",
              " 'operator',\n",
              " 'input',\n",
              " 'english',\n",
              " 'autonomous',\n",
              " 'associative',\n",
              " 'yin yin',\n",
              " 'global',\n",
              " 'input output',\n",
              " 'approaches samples',\n",
              " 'candidates',\n",
              " 'constructing',\n",
              " 'organizing',\n",
              " 'subject constructing',\n",
              " 'networks',\n",
              " 'teaches',\n",
              " 'designing',\n",
              " 'mobile',\n",
              " 'criterion operator',\n",
              " 'computation',\n",
              " 'economize',\n",
              " 'mapping',\n",
              " 'organizing associative',\n",
              " 'euclidean',\n",
              " 'neural',\n",
              " 'database',\n",
              " 'mappings',\n",
              " 'databases',\n",
              " 'constructing associative',\n",
              " 'algorithm',\n",
              " 'operator teaches',\n",
              " 'associative database',\n",
              " 'autonomous mobile',\n",
              " 'neural networks',\n",
              " 'associative databases',\n",
              " 'osaka',\n",
              " 'mobile robot',\n",
              " 'robot',\n",
              " 'robot eyesight',\n",
              " 'databases robot']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find Pairs with highest cosine similarity\n",
        "\n",
        "pairs = []\n",
        "for i in range(len(cosine_scores)-1):\n",
        "    for j in range(i+1, len(cosine_scores)):\n",
        "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
        "\n",
        "#Sort scores in decreasing order\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "for pair in pairs[0:10]:\n",
        "    i, j = pair['index']\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(len(doc_embedding[i]), len(candidate_keyword_embedding[j]), pair['score']))"
      ],
      "metadata": {
        "id": "77DA4Jdy0saE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6573816b-6dc2-438e-a0b6-f1e6c09738bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768 \t\t 768 \t\t Score: 0.9355\n",
            "768 \t\t 768 \t\t Score: 0.9143\n",
            "768 \t\t 768 \t\t Score: 0.9076\n",
            "768 \t\t 768 \t\t Score: 0.9036\n",
            "768 \t\t 768 \t\t Score: 0.8943\n",
            "768 \t\t 768 \t\t Score: 0.8874\n",
            "768 \t\t 768 \t\t Score: 0.8805\n",
            "768 \t\t 768 \t\t Score: 0.8757\n",
            "768 \t\t 768 \t\t Score: 0.8713\n",
            "768 \t\t 768 \t\t Score: 0.8704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-NN to calculate the Nearest neighbor of keywords\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "keyword_feaature_bert_embedding = candidate_keyword_embedding\n",
        "model = NearestNeighbors(n_neighbors=10,\n",
        "                         metric='cosine',\n",
        "                         algorithm='brute',\n",
        "                         n_jobs=-1)\n",
        "first_time_window_candidate_nn = model.fit(keyword_feaature_bert_embedding)\n",
        "\n",
        "# This gives a matrix of 300, 10 -> 300 keywords/arrays with each array containing 10 elements\n",
        "# representing its nearest neighbor\n",
        " \n",
        "distance, indeces = first_time_window_candidate_nn.kneighbors(keyword_feaature_bert_embedding)\n",
        "\n",
        "# Give the indeces of the neighbors that are nearest to the word under consideration\n",
        "## Here we have 300 indeces each with 10 sub array elements, representing it's 10 nearest keywords\n",
        "\n"
      ],
      "metadata": {
        "id": "aP5gV_Jo0Vds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_feaature_bert_embedding\n",
        "\n",
        "\n",
        "keywords_np = np.array(keywords)\n",
        "bigram_candidate_keywords_nparray = np.array(bigram_candidate_keywords) \n",
        "\n",
        "bigram_candidate_keywords_nparray\n",
        "\n",
        "keywords_np\n",
        "\n",
        "tup_nearest_neighbor = []\n",
        "for index, candidate_keyword in enumerate(keywords_np):\n",
        "    # Take the current index of the keyword and get the list of 10 nearest index from KNN algorithm\n",
        "    nearest_neighbors_indeces_of_current_keyword = indeces[index]\n",
        "\n",
        "    # Filter the keyword list using the list of indeces obtained in previous step\n",
        "    nearest_keywords = keywords_np[nearest_neighbors_indeces_of_current_keyword]\n",
        "\n",
        "    # Create tuple with first element as the keyword for current iteration and 2nd element as list of its nearest neighbors\n",
        "    tup_nearest_neighbor.append((candidate_keyword, nearest_keywords))\n",
        "\n",
        "\n",
        "tup_nearest_neighbor"
      ],
      "metadata": {
        "id": "W00nvgrU0bVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322cbe49-dc0a-4df0-c129-4f64e53a11c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('incorrect', array(['incorrect', 'approaches', 'mappings', 'database',\n",
              "         'associative databases', 'robot', 'economize', 'juts',\n",
              "         'organizing associative', 'observes'], dtype='<U24')),\n",
              " ('negligible',\n",
              "  array(['negligible', 'computes', 'handwritten', 'camera', 'parameter',\n",
              "         'global', 'designing', 'subject', 'assumed', '800'], dtype='<U24')),\n",
              " ('free', array(['free', 'recursive', 'hand', 'output', 'candidates',\n",
              "         'associative databases', 'converges', 'implicitly', 'letter',\n",
              "         'parameters'], dtype='<U24')),\n",
              " ('contrarily', array(['contrarily', 'head letter', 'associative databases',\n",
              "         'recognizing', 'candidates', 'converged', 'output', 'economize',\n",
              "         'constructing', 'parameters'], dtype='<U24')),\n",
              " ('furthermore',\n",
              "  array(['furthermore', 'frequency', 'implicitly', 'increases', 'observes',\n",
              "         'defines', 'global', 'parameters', 'recognizing', 'free'],\n",
              "        dtype='<U24')),\n",
              " ('problem', array(['problem', 'mobile', 'associative databases', 'converged',\n",
              "         'learns', 'defines', 'organizing associative', 'input output',\n",
              "         'converges', 'reset'], dtype='<U24')),\n",
              " ('finite', array(['finite', 'coefficients', 'parameters', 'free', 'learns',\n",
              "         'compression', 'constructing', 'recognizing', 'converged',\n",
              "         'output'], dtype='<U24')),\n",
              " ('implicitly',\n",
              "  array(['implicitly', 'increases', 'furthermore', 'free', 'frequency',\n",
              "         'recursive', 'hand', 'parameters', 'global', 'coefficients'],\n",
              "        dtype='<U24')),\n",
              " ('assumed',\n",
              "  array(['assumed', 'global', 'parameters', '18m', 'database', 'organizing',\n",
              "         'passageway', 'observes', 'operator teaches', 'robot'],\n",
              "        dtype='<U24')),\n",
              " ('eternally', array(['eternally', 'economize', 'recognizer', 'constructing',\n",
              "         'candidates', 'juts', 'database', 'mappings', 'euclidean',\n",
              "         'approaches'], dtype='<U24')),\n",
              " ('subject', array(['subject', 'attains', 'global', 'observes', 'luminance',\n",
              "         'database', 'assumed', 'passageway', 'letter',\n",
              "         'organizing associative'], dtype='<U24')),\n",
              " ('frequency',\n",
              "  array(['frequency', 'furthermore', 'increases', 'implicitly', 'observes',\n",
              "         'defines', 'parameters', '400', 'global', 'recognizing'],\n",
              "        dtype='<U24')),\n",
              " ('attains',\n",
              "  array(['attains', 'subject', '400', 'observes', 'global', 'assumed',\n",
              "         'parameter', 'passageway', 'database', 'defines'], dtype='<U24')),\n",
              " ('400', array(['400', 'observes', 'defines', 'recognizing', 'associative',\n",
              "         'input output', 'compression', 'candidates', 'constructing',\n",
              "         'reset'], dtype='<U24')),\n",
              " ('observes',\n",
              "  array(['observes', 'parameters', '400', 'recognizing', 'database',\n",
              "         'defines', 'letter', 'organizing associative', 'global',\n",
              "         'converged'], dtype='<U24')),\n",
              " ('dissimilari', array(['dissimilari', 'criterion', 'osaka', 'neural',\n",
              "         'organizing associative', 'yin', 'letter', 'approaches',\n",
              "         'converged', 'parameters'], dtype='<U24')),\n",
              " ('increases',\n",
              "  array(['increases', 'implicitly', 'frequency', 'furthermore', 'free',\n",
              "         'recursive', 'hand', 'coefficients', 'parameters', 'observes'],\n",
              "        dtype='<U24')),\n",
              " ('denote', array(['denote', 'min', 'approaches samples', 'input output',\n",
              "         'autonomous', 'juts', 'reset', 'yin', 'constructing',\n",
              "         'databases robot'], dtype='<U24')),\n",
              " ('head', array(['head', 'dimension', 'candidates', 'approaches samples',\n",
              "         'euclidean', 'approaches', 'converges', 'yin', 'constructing',\n",
              "         'mappings'], dtype='<U24')),\n",
              " ('levels',\n",
              "  array(['levels', 'neural networks', 'criterion operator', 'yin yin',\n",
              "         'preliminarily', 'neural', 'euclidean', 'organizing associative',\n",
              "         'criterion', 'letter'], dtype='<U24')),\n",
              " ('letter', array(['letter', 'yin', 'organizing associative', 'autonomous',\n",
              "         'candidates', 'associative databases', 'converges', 'databases',\n",
              "         'constructing', 'input output'], dtype='<U24')),\n",
              " ('fig', array(['fig', 'recognizing', 'output', 'constructing', 'associative',\n",
              "         'converged', 'approaches', 'candidates', 'associative databases',\n",
              "         'yin'], dtype='<U24')),\n",
              " ('recognizing',\n",
              "  array(['recognizing', 'fig', 'output', 'candidates', 'converged',\n",
              "         'approaches', 'constructing', 'associative',\n",
              "         'associative databases', 'juts'], dtype='<U24')),\n",
              " ('learns',\n",
              "  array(['learns', 'converges', 'associative databases', 'input output',\n",
              "         'autonomous', 'output', 'databases', 'candidates',\n",
              "         'approaches samples', 'free'], dtype='<U24')),\n",
              " ('18m',\n",
              "  array(['18m', 'parameters', 'approaches samples', 'juts', 'input output',\n",
              "         'approaches', 'autonomous', 'constructing', 'head letter',\n",
              "         'databases'], dtype='<U24')),\n",
              " ('defines',\n",
              "  array(['defines', 'mappings', 'organizing associative', 'recognizing',\n",
              "         'yin', 'candidates', 'osaka', 'letter', 'associative databases',\n",
              "         'observes'], dtype='<U24')),\n",
              " ('parameter',\n",
              "  array(['parameter', '800', 'preliminarily', 'camera', 'negligible',\n",
              "         'finite', 'defines', 'computes', 'subject', 'global'], dtype='<U24')),\n",
              " ('hand', array(['hand', 'recursive', 'white fig', 'obstacles', 'free',\n",
              "         'organizing', 'compression', 'global', 'candidates', 'parameters'],\n",
              "        dtype='<U24')),\n",
              " ('camera', array(['camera', 'negligible', '800', 'parameter', 'computes',\n",
              "         'handwritten', 'designing', 'defines', 'obstacles', 'global'],\n",
              "        dtype='<U24')),\n",
              " ('min', array(['min', 'denote', 'input output', 'approaches samples',\n",
              "         'constructing associative', 'autonomous', 'reset',\n",
              "         'databases robot', '18m', 'juts'], dtype='<U24')),\n",
              " ('juts',\n",
              "  array(['juts', 'candidates', 'constructing', 'approaches', 'autonomous',\n",
              "         'yin', 'approaches samples', 'database', 'associative databases',\n",
              "         'output'], dtype='<U24')),\n",
              " ('approaches',\n",
              "  array(['approaches', 'juts', 'yin', 'constructing', 'candidates',\n",
              "         'parameters', 'associative databases', 'output', 'euclidean',\n",
              "         'databases'], dtype='<U24')),\n",
              " ('preliminarily',\n",
              "  array(['preliminarily', 'parameter', 'criterion operator', 'euclidean',\n",
              "         'levels', 'yin yin', 'organizing associative', 'economize',\n",
              "         'finite', 'converges'], dtype='<U24')),\n",
              " ('compactness',\n",
              "  array(['compactness', 'juts', 'converged', 'compression', 'constructing',\n",
              "         'defines', 'head letter', 'output', 'white fig', 'finite'],\n",
              "        dtype='<U24')),\n",
              " ('samples increases',\n",
              "  array(['samples increases', 'nand', 'reset', 'approaches samples',\n",
              "         'robot eyesight', 'mappings', 'database', 'input output', 'robot',\n",
              "         'luminance'], dtype='<U24')),\n",
              " ('parameters',\n",
              "  array(['parameters', 'global', 'database', 'approaches', 'candidates',\n",
              "         'databases', 'robot', '18m', 'constructing', 'passageway'],\n",
              "        dtype='<U24')),\n",
              " ('obstacles', array(['obstacles', 'white fig', 'organizing', 'hand', 'global',\n",
              "         'compression', 'defines', 'juts', 'candidates', 'parameters'],\n",
              "        dtype='<U24')),\n",
              " ('criterion',\n",
              "  array(['criterion', 'neural', 'dissimilari', 'subject constructing',\n",
              "         'organizing associative', 'yin', 'approaches', 'osaka',\n",
              "         'constructing', 'mappings'], dtype='<U24')),\n",
              " ('output', array(['output', 'coefficients', 'constructing', 'recognizing',\n",
              "         'candidates', 'approaches', 'associative databases', 'free', 'yin',\n",
              "         'juts'], dtype='<U24')),\n",
              " ('luminance', array(['luminance', 'letter', 'defines', 'yin', 'parameters',\n",
              "         'organizing associative', 'observes', 'juts', 'autonomous',\n",
              "         'approaches samples'], dtype='<U24')),\n",
              " ('white',\n",
              "  array(['white', 'database', 'robot', 'passageway', 'letter', 'designing',\n",
              "         'candidates', 'converges', 'structures', 'databases'], dtype='<U24')),\n",
              " ('reset',\n",
              "  array(['reset', 'yin', 'associative databases', 'letter', 'constructing',\n",
              "         'fig', 'organizing associative', 'recognizing',\n",
              "         'constructing associative', 'input output'], dtype='<U24')),\n",
              " ('yin', array(['yin', 'constructing', 'approaches', 'candidates',\n",
              "         'organizing associative', 'juts', 'letter', 'converges',\n",
              "         'autonomous', 'associative databases'], dtype='<U24')),\n",
              " ('800',\n",
              "  array(['800', 'parameter', 'camera', 'defines', 'candidates', 'mappings',\n",
              "         'associative', 'constructing', 'economize', 'juts'], dtype='<U24')),\n",
              " ('compression',\n",
              "  array(['compression', 'recursive', 'candidates', 'hand', 'organizing',\n",
              "         'parameters', 'white fig', 'global', 'input output', 'juts'],\n",
              "        dtype='<U24')),\n",
              " ('converges', array(['converges', 'learns', 'autonomous', 'candidates', 'yin',\n",
              "         'associative databases', 'approaches samples', 'input output',\n",
              "         'databases robot', 'letter'], dtype='<U24')),\n",
              " ('nand', array(['nand', 'parameters', 'databases', 'approaches', 'database',\n",
              "         'juts', 'constructing', 'candidates', 'approaches samples', '18m'],\n",
              "        dtype='<U24')),\n",
              " ('converged', array(['converged', 'associative databases', 'recognizing',\n",
              "         'organizing associative', 'approaches', 'candidates', 'juts',\n",
              "         'output', 'yin', 'fig'], dtype='<U24')),\n",
              " ('eyesight',\n",
              "  array(['eyesight', 'databases', 'samples', 'parameters', 'converged',\n",
              "         'letter', 'candidates', 'output', 'white fig', 'juts'],\n",
              "        dtype='<U24')),\n",
              " ('structures', array(['structures', 'nodes', 'converges', 'yin yin', 'levels',\n",
              "         'preliminarily', 'constructing associative', 'letter', 'economize',\n",
              "         'criterion operator'], dtype='<U24')),\n",
              " ('head letter',\n",
              "  array(['head letter', 'contrarily', 'juts', '18m', 'parameters', 'output',\n",
              "         'associative databases', 'candidates', 'recognizing',\n",
              "         'constructing'], dtype='<U24')),\n",
              " ('coefficients',\n",
              "  array(['coefficients', 'output', 'finite', 'free', 'constructing',\n",
              "         'recognizing', 'parameters', 'fig', 'learns', 'approaches'],\n",
              "        dtype='<U24')),\n",
              " ('handwritten',\n",
              "  array(['handwritten', 'negligible', 'computes', 'camera', 'frequency',\n",
              "         'furthermore', 'increases', 'parameter', 'implicitly', 'attains'],\n",
              "        dtype='<U24')),\n",
              " ('recognizer', array(['recognizer', 'organizing associative', 'eternally',\n",
              "         'constructing', 'database', 'converged', 'mappings', 'economize',\n",
              "         'candidates', 'passageway'], dtype='<U24')),\n",
              " ('passageway',\n",
              "  array(['passageway', 'database', 'robot', 'parameters', 'global',\n",
              "         'candidates', 'approaches', 'constructing', 'observes', 'juts'],\n",
              "        dtype='<U24')),\n",
              " ('samples',\n",
              "  array(['samples', 'eyesight', 'finite', 'recursive', 'hand', 'assumed',\n",
              "         'databases', 'coefficients', 'white fig', 'compression'],\n",
              "        dtype='<U24')),\n",
              " ('recursive', array(['recursive', 'compression', 'hand', 'free', 'implicitly',\n",
              "         'candidates', 'parameters', 'white fig', 'organizing', 'global'],\n",
              "        dtype='<U24')),\n",
              " ('computes',\n",
              "  array(['computes', 'negligible', 'handwritten', 'camera', 'parameter',\n",
              "         'mobile', 'subject', 'attains', 'designing', '800'], dtype='<U24')),\n",
              " ('white fig',\n",
              "  array(['white fig', 'obstacles', 'organizing', 'hand', 'compression',\n",
              "         'global', 'juts', 'candidates', 'parameters', 'defines'],\n",
              "        dtype='<U24')),\n",
              " ('nodes',\n",
              "  array(['nodes', 'structures', 'yin yin', 'incorrect', 'levels', 'mapping',\n",
              "         'criterion operator', 'constructing associative', 'economize',\n",
              "         'preliminarily'], dtype='<U24')),\n",
              " ('dimension', array(['dimension', 'head', 'approaches samples', 'candidates',\n",
              "         'autonomous mobile', 'euclidean', 'operator', 'databases robot',\n",
              "         'associative database', 'input output'], dtype='<U24')),\n",
              " ('operator',\n",
              "  array(['operator', 'head', 'candidates', 'approaches', 'mappings', 'yin',\n",
              "         'recognizing', 'organizing associative', 'approaches samples',\n",
              "         'teaches'], dtype='<U24')),\n",
              " ('input',\n",
              "  array(['input', 'english', 'yin', 'associative', 'reset', 'parameters',\n",
              "         'luminance', 'criterion', 'approaches', 'samples increases'],\n",
              "        dtype='<U24')),\n",
              " ('english',\n",
              "  array(['english', 'input', 'reset', 'denote', 'approaches samples',\n",
              "         'letter', 'organizing associative', 'luminance', 'yin', 'subject'],\n",
              "        dtype='<U24')),\n",
              " ('autonomous', array(['autonomous', 'approaches samples', 'juts', 'converges',\n",
              "         'input output', 'yin', 'databases robot', 'letter', 'candidates',\n",
              "         'approaches'], dtype='<U24')),\n",
              " ('associative',\n",
              "  array(['associative', 'recognizing', 'constructing', 'fig', 'yin',\n",
              "         'output', 'juts', 'candidates', 'approaches', 'parameters'],\n",
              "        dtype='<U24')),\n",
              " ('yin yin', array(['yin yin', 'criterion operator', 'levels', 'preliminarily',\n",
              "         'constructing associative', 'converges', 'euclidean',\n",
              "         'neural networks', 'structures', 'economize'], dtype='<U24')),\n",
              " ('global', array(['global', 'parameters', 'assumed', 'database', 'organizing',\n",
              "         'passageway', 'compression', 'obstacles', 'robot', 'white fig'],\n",
              "        dtype='<U24')),\n",
              " ('input output',\n",
              "  array(['input output', 'approaches samples', 'autonomous', 'converges',\n",
              "         'databases robot', 'juts', '18m', 'candidates', 'letter',\n",
              "         'constructing associative'], dtype='<U24')),\n",
              " ('approaches samples',\n",
              "  array(['approaches samples', 'input output', 'autonomous', 'juts',\n",
              "         'converges', 'databases robot', 'candidates', '18m', 'approaches',\n",
              "         'yin'], dtype='<U24')),\n",
              " ('candidates',\n",
              "  array(['candidates', 'juts', 'constructing', 'approaches', 'yin',\n",
              "         'associative databases', 'converges', 'recognizing', 'output',\n",
              "         'parameters'], dtype='<U24')),\n",
              " ('constructing',\n",
              "  array(['constructing', 'candidates', 'juts', 'yin', 'approaches',\n",
              "         'output', 'euclidean', 'parameters', 'database', 'autonomous'],\n",
              "        dtype='<U24')),\n",
              " ('organizing',\n",
              "  array(['organizing', 'obstacles', 'white fig', 'hand', 'global',\n",
              "         'compression', 'candidates', 'parameters', 'defines', 'juts'],\n",
              "        dtype='<U24')),\n",
              " ('subject constructing',\n",
              "  array(['subject constructing', 'criterion', 'neural', 'dissimilari',\n",
              "         'organizing associative', 'neural networks', 'nand', 'approaches',\n",
              "         'converged', 'mappings'], dtype='<U24')),\n",
              " ('networks', array(['networks', 'recursive', 'free', 'compression',\n",
              "         'associative databases', 'output', 'constructing associative',\n",
              "         'recognizing', 'hand', 'letter'], dtype='<U24')),\n",
              " ('teaches',\n",
              "  array(['teaches', 'approaches samples', 'yin', 'euclidean', 'autonomous',\n",
              "         'converges', 'input output', 'associative databases', 'candidates',\n",
              "         'constructing associative'], dtype='<U24')),\n",
              " ('designing',\n",
              "  array(['designing', 'luminance', '800', 'candidates', 'mappings',\n",
              "         'robot eyesight', 'parameters', 'associative databases', 'head',\n",
              "         'white'], dtype='<U24')),\n",
              " ('mobile', array(['mobile', 'problem', 'learns', 'converges', 'input output',\n",
              "         'associative databases', 'approaches samples', 'autonomous',\n",
              "         'constructing associative', 'letter'], dtype='<U24')),\n",
              " ('criterion operator',\n",
              "  array(['criterion operator', 'levels', 'preliminarily', 'neural networks',\n",
              "         'yin yin', 'euclidean', 'autonomous', 'structures', 'converges',\n",
              "         'associative databases'], dtype='<U24')),\n",
              " ('computation',\n",
              "  array(['computation', 'luminance', '400', 'denote', 'input output',\n",
              "         'approaches samples', 'defines', 'reset', 'compression', 'robot'],\n",
              "        dtype='<U24')),\n",
              " ('economize',\n",
              "  array(['economize', 'eternally', 'converged', 'euclidean', 'constructing',\n",
              "         'mapping', 'organizing associative', 'approaches', 'recognizing',\n",
              "         'mappings'], dtype='<U24')),\n",
              " ('mapping', array(['mapping', 'economize', 'eternally', 'learns', 'euclidean',\n",
              "         'converges', 'yin yin', 'constructing associative', 'contrarily',\n",
              "         'candidates'], dtype='<U24')),\n",
              " ('organizing associative',\n",
              "  array(['organizing associative', 'yin', 'letter', 'converged',\n",
              "         'recognizer', 'mappings', 'approaches', 'constructing',\n",
              "         'candidates', 'defines'], dtype='<U24')),\n",
              " ('euclidean', array(['euclidean', 'candidates', 'approaches', 'constructing',\n",
              "         'associative databases', 'yin', 'output', 'converges',\n",
              "         'recognizing', 'juts'], dtype='<U24')),\n",
              " ('neural',\n",
              "  array(['neural', 'criterion', 'dissimilari', 'subject constructing',\n",
              "         'neural networks', 'organizing associative', 'levels', 'yin',\n",
              "         'approaches', 'osaka'], dtype='<U24')),\n",
              " ('database',\n",
              "  array(['database', 'passageway', 'robot', 'parameters', 'candidates',\n",
              "         'global', 'juts', 'approaches', 'constructing', 'output'],\n",
              "        dtype='<U24')),\n",
              " ('mappings', array(['mappings', 'defines', 'organizing associative', 'yin',\n",
              "         'associative databases', 'candidates', 'constructing', 'juts',\n",
              "         'economize', 'approaches'], dtype='<U24')),\n",
              " ('databases',\n",
              "  array(['databases', 'parameters', 'approaches', 'autonomous', 'juts',\n",
              "         'candidates', 'converges', 'eyesight', 'letter', 'constructing'],\n",
              "        dtype='<U24')),\n",
              " ('constructing associative',\n",
              "  array(['constructing associative', 'input output', 'autonomous',\n",
              "         'approaches samples', 'converges', 'databases robot', 'letter',\n",
              "         'learns', 'reset', 'economize'], dtype='<U24')),\n",
              " ('algorithm',\n",
              "  array(['algorithm', 'operator teaches', 'parameters', 'white fig',\n",
              "         'obstacles', 'nand', 'head letter', 'global', 'assumed',\n",
              "         'organizing'], dtype='<U24')),\n",
              " ('operator teaches',\n",
              "  array(['operator teaches', 'algorithm', 'parameters', 'head letter',\n",
              "         'assumed', 'global', 'nand', 'candidates', '18m', 'juts'],\n",
              "        dtype='<U24')),\n",
              " ('associative database',\n",
              "  array(['associative database', 'autonomous mobile', 'approaches',\n",
              "         'candidates', 'organizing associative', 'juts', 'database',\n",
              "         'constructing', 'incorrect', 'yin'], dtype='<U24')),\n",
              " ('autonomous mobile', array(['autonomous mobile', 'organizing associative',\n",
              "         'associative database', 'letter', 'constructing', 'yin',\n",
              "         'candidates', 'converged', 'economize', 'approaches'], dtype='<U24')),\n",
              " ('neural networks',\n",
              "  array(['neural networks', 'levels', 'neural', 'criterion operator',\n",
              "         'criterion', 'subject constructing', 'yin yin', 'preliminarily',\n",
              "         'dissimilari', 'euclidean'], dtype='<U24')),\n",
              " ('associative databases',\n",
              "  array(['associative databases', 'candidates', 'approaches', 'converges',\n",
              "         'yin', 'converged', 'output', 'juts', 'euclidean', 'constructing'],\n",
              "        dtype='<U24')),\n",
              " ('osaka', array(['osaka', 'dissimilari', 'defines', 'criterion',\n",
              "         'organizing associative', 'juts', 'input output', 'mappings',\n",
              "         'letter', 'autonomous'], dtype='<U24')),\n",
              " ('mobile robot',\n",
              "  array(['mobile robot', 'subject', 'autonomous mobile', 'robot',\n",
              "         'associative database', 'incorrect', 'passageway', 'economize',\n",
              "         'database', 'structures'], dtype='<U24')),\n",
              " ('robot',\n",
              "  array(['robot', 'passageway', 'database', 'parameters', 'approaches',\n",
              "         'constructing', 'yin', 'organizing associative', 'candidates',\n",
              "         'letter'], dtype='<U24')),\n",
              " ('robot eyesight',\n",
              "  array(['robot eyesight', 'letter', 'approaches', 'juts', '18m',\n",
              "         'approaches samples', 'osaka', 'input output', 'yin', 'defines'],\n",
              "        dtype='<U24')),\n",
              " ('databases robot',\n",
              "  array(['databases robot', 'autonomous', 'approaches samples',\n",
              "         'input output', 'candidates', 'converges', 'letter', 'juts',\n",
              "         'constructing associative', 'euclidean'], dtype='<U24'))]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALL TIME WINDOWS SCI-BERT"
      ],
      "metadata": {
        "id": "kLi7qvOGatyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert List of Time Slice DF paper_text content to lists\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Loop through every DF and convert paper_text to list and concatenate all the papers of one time slice \n",
        "## this will be a list like  [\"All paper content string of first slice\", \"all paper content string of 2nd slice\", ...] \n",
        "\n",
        "papers_contents_list = [\" \".join(time_slice_df[\"paper_text\"].tolist()) for time_slice_df in nips_papers_partitions.values()]\n",
        "\n",
        "#### MEASURE THE EXECUTION TIME FOR RUNNING THE CONCATENATION CODE\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "\n",
        "# papers_contents_list\n",
        "# len(papers_contents_list[0])"
      ],
      "metadata": {
        "id": "lrwZ1HOQa9XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee3f8a0-f7d1-465f-e5e7-14f78b974c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06901216506958008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Join Paper titles for bigram and unigram extraction\n",
        "\n",
        "\n",
        "papers_titles_list = [\" \".join(time_slice_df[\"title\"].tolist()) for time_slice_df in nips_papers_partitions.values()]\n",
        "\n"
      ],
      "metadata": {
        "id": "RcUQthFrPyZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 -  Pre Processing \n",
        "\n",
        "# Remove Stopwords "
      ],
      "metadata": {
        "id": "uGQDbS_N62fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# function to rmeove digits and numbers from papers \n",
        "def regex_remove_digits(papers_contents_list):      \n",
        "    # Remove any digits for the corpus\n",
        "    all_time_window_papers_content_list = [re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", time_slice_paper) \n",
        "                                                    for time_slice_paper in papers_contents_list] \n",
        "    # Remove words with length less than 3 \n",
        "    # https://stackoverflow.com/questions/24332025/remove-words-of-length-less-than-4-from-string\n",
        "    all_time_window_papers_content_list = [re.sub(r'\\b\\w{1,2}\\b', '', time_slice_paper) \n",
        "                                          for time_slice_paper in all_time_window_papers_content_list]\n",
        "\n",
        "    return all_time_window_papers_content_list\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4rWzW9Zl7FNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Custom Stopwords List for Scientific Literature \n",
        "\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "path_to_stop_words = '/gdrive/My Drive/Master_dataset/stopwords_10000_most_frequent_filtered.txt'\n",
        "\n",
        "with open(path_to_stop_words, \"r\") as file1:\n",
        "    FileasList = file1.readlines()\n",
        "\n",
        "\n",
        "stopwords = [s.strip('\\n') for s in FileasList]\n",
        "print(len(stopwords))\n",
        "\n",
        "\n",
        "scientific_literature_stopwords = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
        "\n",
        "len(scientific_literature_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYyOHKZ1QY0K",
        "outputId": "c1031101-dd9b-48c6-fb30-1de0f2f934e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9954\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9958"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all paper content and titles for bigram and unigram generation\n",
        "all_time_window_papers_content_list = regex_remove_digits(papers_contents_list)\n",
        "all_time_window_papers_title_list = regex_remove_digits(papers_titles_list)\n"
      ],
      "metadata": {
        "id": "EM2Ek6JKQVJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Bag Of Candidate Keywords For All Time Windows"
      ],
      "metadata": {
        "id": "uaAZrFV9eAu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Bag of Keywords BiGrams\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "\n",
        "\n",
        "def n_gram_generator(all_time_window_papers_content_list, n_gram=1, n_features=200, max_df=0.6, min_df=1):\n",
        "      \n",
        "    time_slices_sent_tokens_list_ = [nltk.sent_tokenize(time_window_papers[0:200000]) for time_window_papers in all_time_window_papers_content_list]\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    # time_slices_sent_tokens_list = time_slices_sent_tokens_list_\n",
        "    \n",
        "    # time_slice_vectorizer_list = []\n",
        "\n",
        "\n",
        "    vectorizer_birgram = CountVectorizer(stop_words= scientific_literature_stopwords , ngram_range=(n_gram,n_gram), max_features=n_features, min_df=min_df, max_df=max_df)\n",
        "    print(vectorizer_birgram)\n",
        "\n",
        "    x_train_time_slices_list = [vectorizer_birgram.fit_transform(time_slice_sent_tokens) for time_slice_sent_tokens in time_slices_sent_tokens_list_]\n",
        "\n",
        "\n",
        "    # x_train_time_slices_list = []\n",
        "    # for time_slice_sent_tokens in time_slices_sent_tokens_list_:\n",
        "    #         vectorizer_birgram = CountVectorizer(stop_words= scientific_literature_stopwords , ngram_range=(n_gram,n_gram), max_features=n_features) \n",
        "    #         x_train_time_slices_list.append(vectorizer_birgram.fit_transform(time_slice_sent_tokens))\n",
        "\n",
        "    list_of_bow_bi_grams_df_every_time_slice = [pd.DataFrame(x_train.toarray() ,columns=vectorizer_birgram.get_feature_names()) for x_train in x_train_time_slices_list]\n",
        "    #vect = Dictvectorizer_birgram()\n",
        "\n",
        "\n",
        "    # x_train = vectorizer_birgram.fit_transform(sent_tokens)\n",
        "\n",
        "    # df_bow_bi_grams = pd.DataFrame(x_train.toarray(),columns=vectorizer_birgram.get_feature_names())\n",
        "    # df_bow_bi_grams\n",
        "\n",
        "    ngram_candidate_keywords_time_slices = [bow_df.columns.values.tolist() for bow_df in list_of_bow_bi_grams_df_every_time_slice]\n",
        "    # bigram_candidate_keywords = vectorizer_birgram.get_feature_names()\n",
        "    # len(bigram_candidate_keywords_time_slices)\n",
        "\n",
        "    feature_frequencies_list_every_time_slice = [x_train.toarray().sum(axis=0) for x_train in x_train_time_slices_list]\n",
        "\n",
        "    \n",
        "    # print(feature_frequencies_list_every_time_slice)\n",
        "    \n",
        "    ngram_keyword_frequency_list_all_time = []\n",
        "    for index in range(len(ngram_candidate_keywords_time_slices)):\n",
        "      feature_frequency = {}\n",
        "      for j,keyword in enumerate(ngram_candidate_keywords_time_slices[index]):\n",
        "          feature_frequency[keyword] = feature_frequencies_list_every_time_slice[index][j]\n",
        "      \n",
        "      ngram_keyword_frequency_list_all_time.append(feature_frequency)\n",
        "      \n",
        "\n",
        "\n",
        "# feature_frequencies = x_train.toarray().sum(axis=0)\n",
        "\n",
        "\n",
        "# bi_gram_keyword_frequency = {}\n",
        "\n",
        "# for index, keyword in enumerate(vectorizer.get_feature_names()):\n",
        "#   bi_gram_keyword_frequency[keyword]=feature_frequencies[index]\n",
        "\n",
        "\n",
        "    return ngram_keyword_frequency_list_all_time, time_slices_sent_tokens_list_\n",
        "    # df_bow_bi_grams = pd.DataFrame(x_train.toarray(),columns=vect.get_feature_names())\n",
        "    # df_bow_bi_grams\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WMAHsELOeJYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_candidate_keywords_time_slices, time_slices_sent_tokens_list = n_gram_generator(all_time_window_papers_content_list, 2, 200)\n",
        "unigram_candidate_keywords_time_slices = n_gram_generator(all_time_window_papers_content_list, 1, 200)\n",
        "\n",
        "\n",
        "\n",
        "all_time_window_papers_titles = \" \".join(all_time_window_papers_title_list)\n",
        "\n",
        "unigram_candidate_keywords_time_slices_title = n_gram_generator([all_time_window_papers_titles], 1, 200,1.0, 0.00)\n",
        "bigram_candidate_keywords_time_slices_title = n_gram_generator([all_time_window_papers_titles], 2, 200, 0.6, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "9vVvUZgCRXXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85cfd209-ddeb-481e-f2c7-840b836e61a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(max_df=0.6, max_features=200, ngram_range=(2, 2),\n",
            "                stop_words=frozenset({'101', '10th', '11th', '121', '12th',\n",
            "                                      '143', '1b', '1st', '2', '2d', '2nd',\n",
            "                                      '3d', '3rd', '4', '4th', '5th', '64',\n",
            "                                      '6th', '7th', '86', '8th', '8vo', '9th',\n",
            "                                      'FALSE', 'George', 'I', \"I'd\", 'Michael',\n",
            "                                      'a', 'a.m.', ...}))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['adj', 'ain', 'anglo', 'aren', 'cf', 'ch', 'col', 'cong', 'couldn', 'didn', 'doesn', 'false', 'fashioned', 'father', 'ff', 'friend', 'ft', 'gen', 'george', 'girl', 'god', 'hadn', 'hasn', 'haven', 'icel', 'imp', 'isn', 'king', 'lady', 'law', 'majesty', 'michael', 'mother', 'mustn', 'ne', 'needn', 'oe', 'pp', 'prof', 'shan', 'shouldn', 'son', 'sq', 'th', 've', 'viz', 'wasn', 'weren', 'wife', 'woman', 'wouldn'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(max_df=0.6, max_features=200,\n",
            "                stop_words=frozenset({'101', '10th', '11th', '121', '12th',\n",
            "                                      '143', '1b', '1st', '2', '2d', '2nd',\n",
            "                                      '3d', '3rd', '4', '4th', '5th', '64',\n",
            "                                      '6th', '7th', '86', '8th', '8vo', '9th',\n",
            "                                      'FALSE', 'George', 'I', \"I'd\", 'Michael',\n",
            "                                      'a', 'a.m.', ...}))\n",
            "CountVectorizer(max_features=200, min_df=0.0,\n",
            "                stop_words=frozenset({'101', '10th', '11th', '121', '12th',\n",
            "                                      '143', '1b', '1st', '2', '2d', '2nd',\n",
            "                                      '3d', '3rd', '4', '4th', '5th', '64',\n",
            "                                      '6th', '7th', '86', '8th', '8vo', '9th',\n",
            "                                      'FALSE', 'George', 'I', \"I'd\", 'Michael',\n",
            "                                      'a', 'a.m.', ...}))\n",
            "CountVectorizer(max_df=0.6, max_features=200, ngram_range=(2, 2),\n",
            "                stop_words=frozenset({'101', '10th', '11th', '121', '12th',\n",
            "                                      '143', '1b', '1st', '2', '2d', '2nd',\n",
            "                                      '3d', '3rd', '4', '4th', '5th', '64',\n",
            "                                      '6th', '7th', '86', '8th', '8vo', '9th',\n",
            "                                      'FALSE', 'George', 'I', \"I'd\", 'Michael',\n",
            "                                      'a', 'a.m.', ...}))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_candidate_keywords_time_slices_title[0][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2FV1uUHlxiq",
        "outputId": "7d54b402-0963-477f-dbc8-bb67879acd70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adaptive dynamic': 3,\n",
              " 'adaptive neural': 4,\n",
              " 'algorithm neural': 5,\n",
              " 'algorithm observable': 3,\n",
              " 'algorithm reinforcement': 3,\n",
              " 'algorithm sequential': 3,\n",
              " 'algorithms neural': 4,\n",
              " 'analog neural': 10,\n",
              " 'analog vlsi': 31,\n",
              " 'analogue vlsi': 3,\n",
              " 'anomaly detection': 3,\n",
              " 'approximate iteration': 3,\n",
              " 'asymmetric hebbian': 3,\n",
              " 'asymptotic convergence': 3,\n",
              " 'attractor neural': 4,\n",
              " 'audio visual': 5,\n",
              " 'auditory cortex': 3,\n",
              " 'backpropagation networks': 3,\n",
              " 'bayesian averaging': 3,\n",
              " 'bayesian modeling': 3,\n",
              " 'bayesian models': 5,\n",
              " 'bayesian networks': 10,\n",
              " 'boosting algorithms': 3,\n",
              " 'cascade correlation': 3,\n",
              " 'characterization neural': 3,\n",
              " 'clustering algorithm': 3,\n",
              " 'clustering graph': 3,\n",
              " 'competitive neural': 5,\n",
              " 'computational power': 4,\n",
              " 'connectionist models': 6,\n",
              " 'connectionist networks': 3,\n",
              " 'context free': 3,\n",
              " 'decoding neuronal': 3,\n",
              " 'dirichlet allocation': 4,\n",
              " 'dynamic bayesian': 4,\n",
              " 'dynamic environments': 3,\n",
              " 'dynamic models': 4,\n",
              " 'dynamic programming': 14,\n",
              " 'dynamics bayesian': 4,\n",
              " 'dynamics neural': 6,\n",
              " 'entropy minimization': 3,\n",
              " 'face detection': 6,\n",
              " 'feedforward networks': 3,\n",
              " 'feedforward neural': 3,\n",
              " 'finite automata': 3,\n",
              " 'gaussian mixtures': 6,\n",
              " 'gaussian models': 11,\n",
              " 'gaussian regression': 10,\n",
              " 'generalization bounds': 4,\n",
              " 'generalization neural': 3,\n",
              " 'generative models': 4,\n",
              " 'genetic algorithms': 5,\n",
              " 'graph matching': 4,\n",
              " 'graphical models': 14,\n",
              " 'graphs cycles': 3,\n",
              " 'hierarchical mixtures': 4,\n",
              " 'hierarchical visual': 3,\n",
              " 'hippocampal neurons': 3,\n",
              " 'input output': 3,\n",
              " 'integrated segmentation': 3,\n",
              " 'invariant neural': 3,\n",
              " 'kernel classifiers': 3,\n",
              " 'kernel component': 3,\n",
              " 'kernel fisher': 3,\n",
              " 'kernel pca': 4,\n",
              " 'kernels semi': 3,\n",
              " 'laplacian eigenmaps': 3,\n",
              " 'layered networks': 4,\n",
              " 'linear dimension': 3,\n",
              " 'linear dimensionality': 3,\n",
              " 'linear dynamical': 3,\n",
              " 'linear models': 6,\n",
              " 'linear networks': 3,\n",
              " 'linear perceptrons': 3,\n",
              " 'linear programming': 7,\n",
              " 'linear regression': 4,\n",
              " 'logistic regression': 4,\n",
              " 'loopy propagation': 5,\n",
              " 'markov models': 20,\n",
              " 'markov monte': 3,\n",
              " 'markov networks': 4,\n",
              " 'matrix factorization': 7,\n",
              " 'matrix inversion': 3,\n",
              " 'mixtures experts': 10,\n",
              " 'modeling neural': 5,\n",
              " 'models auditory': 3,\n",
              " 'models modeling': 3,\n",
              " 'models neural': 5,\n",
              " 'models probabilistic': 4,\n",
              " 'models recognizing': 3,\n",
              " 'multi chip': 4,\n",
              " 'multi dimensional': 3,\n",
              " 'multi networks': 3,\n",
              " 'multi neural': 3,\n",
              " 'multi perceptron': 3,\n",
              " 'multidimensional scaling': 4,\n",
              " 'multilayer networks': 4,\n",
              " 'multilayer neural': 3,\n",
              " 'multilayer perceptrons': 5,\n",
              " 'networks adaptive': 4,\n",
              " 'networks analog': 6,\n",
              " 'networks bayesian': 3,\n",
              " 'networks coding': 4,\n",
              " 'networks complexity': 3,\n",
              " 'networks component': 3,\n",
              " 'networks computing': 3,\n",
              " 'networks connectionist': 5,\n",
              " 'networks generalized': 3,\n",
              " 'networks hierarchical': 5,\n",
              " 'networks implementation': 3,\n",
              " 'networks linear': 3,\n",
              " 'networks modeling': 4,\n",
              " 'networks neural': 18,\n",
              " 'networks nonlinear': 3,\n",
              " 'networks reinforcement': 3,\n",
              " 'networks spike': 3,\n",
              " 'networks spiking': 6,\n",
              " 'networks statistical': 4,\n",
              " 'networks unsupervised': 3,\n",
              " 'networks weights': 3,\n",
              " 'neural analog': 4,\n",
              " 'neural chip': 5,\n",
              " 'neural classifier': 5,\n",
              " 'neural classifiers': 6,\n",
              " 'neural computation': 4,\n",
              " 'neural implementation': 4,\n",
              " 'neural learns': 4,\n",
              " 'neural models': 6,\n",
              " 'neural neural': 8,\n",
              " 'neural optimal': 3,\n",
              " 'neural oscillator': 3,\n",
              " 'neural regression': 5,\n",
              " 'neural simulation': 3,\n",
              " 'neural spike': 3,\n",
              " 'neural visual': 3,\n",
              " 'neurons neural': 3,\n",
              " 'nonlinear models': 5,\n",
              " 'nonparametric bayesian': 4,\n",
              " 'observable markov': 4,\n",
              " 'ocular dominance': 9,\n",
              " 'ocular reflex': 5,\n",
              " 'optimal neural': 5,\n",
              " 'pac bayes': 6,\n",
              " 'pairwise clustering': 3,\n",
              " 'parti algorithm': 3,\n",
              " 'polynomial networks': 3,\n",
              " 'power law': 3,\n",
              " 'predictive models': 4,\n",
              " 'prefrontal cortex': 3,\n",
              " 'probabilistic clustering': 3,\n",
              " 'probabilistic models': 5,\n",
              " 'problem solving': 3,\n",
              " 'programmable analog': 3,\n",
              " 'propagation algorithm': 3,\n",
              " 'propagation connectionist': 3,\n",
              " 'propagation networks': 3,\n",
              " 'radial networks': 6,\n",
              " 'rbf networks': 4,\n",
              " 'recurrent networks': 15,\n",
              " 'recurrent neural': 21,\n",
              " 'regression neural': 3,\n",
              " 'regression vector': 3,\n",
              " 'reinforcement algorithm': 3,\n",
              " 'saccadic eye': 3,\n",
              " 'selective sampling': 3,\n",
              " 'semi supervised': 26,\n",
              " 'semidefinite programming': 3,\n",
              " 'sequential monte': 3,\n",
              " 'silicon retina': 5,\n",
              " 'sparse coding': 6,\n",
              " 'sparse gaussian': 5,\n",
              " 'sparse pca': 3,\n",
              " 'spectral clustering': 12,\n",
              " 'spike timing': 12,\n",
              " 'spiking neuron': 3,\n",
              " 'spiking neurons': 24,\n",
              " 'statistical mechanics': 6,\n",
              " 'statistical modeling': 3,\n",
              " 'statistical models': 4,\n",
              " 'stochastic dynamics': 3,\n",
              " 'stochastic neural': 3,\n",
              " 'structural minimization': 4,\n",
              " 'switching linear': 3,\n",
              " 'synaptic plasticity': 3,\n",
              " 'synchronization neural': 3,\n",
              " 'temporally asymmetric': 3,\n",
              " 'timing plasticity': 7,\n",
              " 'variational bayesian': 11,\n",
              " 'vector classifiers': 3,\n",
              " 'vector kernels': 3,\n",
              " 'vector quantization': 5,\n",
              " 'vector regression': 5,\n",
              " 'vestibulo ocular': 6,\n",
              " 'visual cortex': 17,\n",
              " 'visual tracking': 4,\n",
              " 'vlsi chip': 3,\n",
              " 'vlsi circuits': 3,\n",
              " 'vlsi implementation': 7,\n",
              " 'vlsi neural': 11,\n",
              " 'winner networks': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Paper Title Bigrams and Unigrams    \n",
        "bigram_candidate_keywords_time_slices_title\n",
        "unigram_candidate_keywords_time_slices_title\n",
        "\n",
        "\n",
        "# Paper Content Bigrams and Unigrams\n",
        "unigram_candidate_keywords_time_slices\n",
        "bigram_candidate_keywords_time_slices\n",
        "\n",
        "\n",
        "## Create ngrams that include both bigram and unigram for all time windows.\n",
        "paper_title_ngram_candidate_keywords_time_slice = []\n",
        "\n",
        "# Loop through the 11 time windows and for each time window create a dictionary of ngrams containing both bi and unigram\n",
        "paper_title_ngram_candidate_keywords_time_slice.append({**bigram_candidate_keywords_time_slices_title[0][0], **unigram_candidate_keywords_time_slices_title[0][0]})\n"
      ],
      "metadata": {
        "id": "w0o3QlKF2crH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort all the keywords for every time slice\n",
        "title_ngram_candidate_keywords_time_slices_sorted = [sorted(word_frequency.items(),\n",
        "        key=lambda item: item[1], reverse=True) for word_frequency in paper_title_ngram_candidate_keywords_time_slice]\n",
        "\n",
        "\n",
        "title_ngram_candidate_keywords_time_slices_sorted[:100]\n",
        "\n"
      ],
      "metadata": {
        "id": "EECfYbyvdYkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf2b006-d973-4797-e2fa-d7ad01e9e7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('neural', 425),\n",
              "  ('networks', 338),\n",
              "  ('models', 179),\n",
              "  ('bayesian', 123),\n",
              "  ('visual', 98),\n",
              "  ('algorithm', 91),\n",
              "  ('reinforcement', 86),\n",
              "  ('gaussian', 85),\n",
              "  ('clustering', 84),\n",
              "  ('linear', 84),\n",
              "  ('algorithms', 83),\n",
              "  ('kernel', 78),\n",
              "  ('adaptive', 77),\n",
              "  ('markov', 72),\n",
              "  ('neurons', 71),\n",
              "  ('multi', 69),\n",
              "  ('vector', 69),\n",
              "  ('analog', 66),\n",
              "  ('modeling', 66),\n",
              "  ('dynamic', 64),\n",
              "  ('vlsi', 58),\n",
              "  ('regression', 57),\n",
              "  ('recurrent', 56),\n",
              "  ('sparse', 56),\n",
              "  ('detection', 55),\n",
              "  ('dynamics', 55),\n",
              "  ('generalization', 54),\n",
              "  ('probabilistic', 54),\n",
              "  ('optimal', 53),\n",
              "  ('propagation', 51),\n",
              "  ('connectionist', 47),\n",
              "  ('hierarchical', 46),\n",
              "  ('stochastic', 43),\n",
              "  ('kernels', 42),\n",
              "  ('supervised', 42),\n",
              "  ('bounds', 41),\n",
              "  ('multiple', 41),\n",
              "  ('optimization', 41),\n",
              "  ('prediction', 41),\n",
              "  ('nonlinear', 39),\n",
              "  ('spike', 39),\n",
              "  ('statistical', 38),\n",
              "  ('convergence', 37),\n",
              "  ('cortex', 37),\n",
              "  ('spiking', 37),\n",
              "  ('unsupervised', 37),\n",
              "  ('gradient', 36),\n",
              "  ('approximate', 35),\n",
              "  ('classifiers', 35),\n",
              "  ('coding', 33),\n",
              "  ('mixtures', 33),\n",
              "  ('semi', 33),\n",
              "  ('programming', 32),\n",
              "  ('analog vlsi', 31),\n",
              "  ('approximation', 30),\n",
              "  ('boosting', 30),\n",
              "  ('computational', 30),\n",
              "  ('auditory', 29),\n",
              "  ('density', 29),\n",
              "  ('variational', 29),\n",
              "  ('associative', 28),\n",
              "  ('complexity', 28),\n",
              "  ('implementation', 28),\n",
              "  ('neuron', 28),\n",
              "  ('chip', 27),\n",
              "  ('semi supervised', 26),\n",
              "  ('component', 26),\n",
              "  ('dimensional', 26),\n",
              "  ('computation', 25),\n",
              "  ('cortical', 25),\n",
              "  ('graph', 25),\n",
              "  ('invariant', 25),\n",
              "  ('neuronal', 25),\n",
              "  ('regularization', 25),\n",
              "  ('spatial', 25),\n",
              "  ('spiking neurons', 24),\n",
              "  ('spectral', 24),\n",
              "  ('dynamical', 23),\n",
              "  ('extraction', 23),\n",
              "  ('minimization', 23),\n",
              "  ('silicon', 23),\n",
              "  ('adaptation', 22),\n",
              "  ('discriminative', 22),\n",
              "  ('distributions', 22),\n",
              "  ('organizing', 22),\n",
              "  ('sampling', 22),\n",
              "  ('recurrent neural', 21),\n",
              "  ('computing', 21),\n",
              "  ('estimating', 21),\n",
              "  ('graphical', 21),\n",
              "  ('segmentation', 21),\n",
              "  ('markov models', 20),\n",
              "  ('backpropagation', 20),\n",
              "  ('bayes', 20),\n",
              "  ('experts', 20),\n",
              "  ('matching', 20),\n",
              "  ('plasticity', 20),\n",
              "  ('sequential', 20),\n",
              "  ('entropy', 19),\n",
              "  ('pca', 19),\n",
              "  ('structured', 19),\n",
              "  ('networks neural', 18),\n",
              "  ('context', 18),\n",
              "  ('convex', 18),\n",
              "  ('finite', 18),\n",
              "  ('power', 18),\n",
              "  ('predictive', 18),\n",
              "  ('problem', 18),\n",
              "  ('sequences', 18),\n",
              "  ('tracking', 18),\n",
              "  ('visual cortex', 17),\n",
              "  ('decomposition', 17),\n",
              "  ('face', 17),\n",
              "  ('filtering', 17),\n",
              "  ('generalized', 17),\n",
              "  ('matrix', 17),\n",
              "  ('radial', 17),\n",
              "  ('selective', 17),\n",
              "  ('synaptic', 17),\n",
              "  ('theoretic', 17),\n",
              "  ('classifier', 16),\n",
              "  ('hand', 16),\n",
              "  ('hebbian', 16),\n",
              "  ('integration', 16),\n",
              "  ('likelihood', 16),\n",
              "  ('localization', 16),\n",
              "  ('monte', 16),\n",
              "  ('timing', 16),\n",
              "  ('recurrent networks', 15),\n",
              "  ('conditional', 15),\n",
              "  ('dimension', 15),\n",
              "  ('eye', 15),\n",
              "  ('graphs', 15),\n",
              "  ('hybrid', 15),\n",
              "  ('input', 15),\n",
              "  ('ocular', 15),\n",
              "  ('orientation', 15),\n",
              "  ('receptive', 15),\n",
              "  ('robot', 15),\n",
              "  ('scaling', 15),\n",
              "  ('simulation', 15),\n",
              "  ('techniques', 15),\n",
              "  ('dynamic programming', 14),\n",
              "  ('graphical models', 14),\n",
              "  ('boltzmann', 14),\n",
              "  ('correlation', 14),\n",
              "  ('dimensionality', 14),\n",
              "  ('incremental', 14),\n",
              "  ('maximization', 14),\n",
              "  ('multilayer', 14),\n",
              "  ('nonparametric', 14),\n",
              "  ('perceptron', 14),\n",
              "  ('applied', 13),\n",
              "  ('diffusion', 13),\n",
              "  ('discrete', 13),\n",
              "  ('embedding', 13),\n",
              "  ('global', 13),\n",
              "  ('induction', 13),\n",
              "  ('parameter', 13),\n",
              "  ('parametric', 13),\n",
              "  ('patterns', 13),\n",
              "  ('perceptrons', 13),\n",
              "  ('regularized', 13),\n",
              "  ('retrieval', 13),\n",
              "  ('world', 13),\n",
              "  ('spectral clustering', 12),\n",
              "  ('spike timing', 12),\n",
              "  ('approximations', 12),\n",
              "  ('bias', 12),\n",
              "  ('circuits', 12),\n",
              "  ('competitive', 12),\n",
              "  ('components', 12),\n",
              "  ('environments', 12),\n",
              "  ('estimates', 12),\n",
              "  ('feedback', 12),\n",
              "  ('filters', 12),\n",
              "  ('generative', 12),\n",
              "  ('ica', 12),\n",
              "  ('inferring', 12),\n",
              "  ('perceptual', 12),\n",
              "  ('relational', 12),\n",
              "  ('weights', 12),\n",
              "  ('gaussian models', 11),\n",
              "  ('variational bayesian', 11),\n",
              "  ('vlsi neural', 11),\n",
              "  ('alignment', 11),\n",
              "  ('dirichlet', 11),\n",
              "  ('discriminant', 11),\n",
              "  ('free', 11),\n",
              "  ('functional', 11),\n",
              "  ('hippocampal', 11),\n",
              "  ('iterative', 11),\n",
              "  ('metric', 11),\n",
              "  ('modelling', 11),\n",
              "  ('sensory', 11),\n",
              "  ('svm', 11),\n",
              "  ('synapses', 11),\n",
              "  ('variables', 11),\n",
              "  ('analog neural', 10),\n",
              "  ('bayesian networks', 10),\n",
              "  ('gaussian regression', 10),\n",
              "  ('mixtures experts', 10),\n",
              "  ('approaches', 10),\n",
              "  ('bottleneck', 10),\n",
              "  ('constraints', 10),\n",
              "  ('coupling', 10),\n",
              "  ('genetic', 10),\n",
              "  ('handwritten', 10),\n",
              "  ('hmm', 10),\n",
              "  ('iteration', 10),\n",
              "  ('lateral', 10),\n",
              "  ('modulation', 10),\n",
              "  ('neuromorphic', 10),\n",
              "  ('pomdps', 10),\n",
              "  ('protein', 10),\n",
              "  ('ranking', 10),\n",
              "  ('saliency', 10),\n",
              "  ('structures', 10),\n",
              "  ('tuning', 10),\n",
              "  ('ocular dominance', 9),\n",
              "  ('neural neural', 8),\n",
              "  ('linear programming', 7),\n",
              "  ('matrix factorization', 7),\n",
              "  ('timing plasticity', 7),\n",
              "  ('vlsi implementation', 7),\n",
              "  ('connectionist models', 6),\n",
              "  ('dynamics neural', 6),\n",
              "  ('face detection', 6),\n",
              "  ('gaussian mixtures', 6),\n",
              "  ('linear models', 6),\n",
              "  ('networks analog', 6),\n",
              "  ('networks spiking', 6),\n",
              "  ('neural classifiers', 6),\n",
              "  ('neural models', 6),\n",
              "  ('pac bayes', 6),\n",
              "  ('radial networks', 6),\n",
              "  ('sparse coding', 6),\n",
              "  ('statistical mechanics', 6),\n",
              "  ('vestibulo ocular', 6),\n",
              "  ('algorithm neural', 5),\n",
              "  ('audio visual', 5),\n",
              "  ('bayesian models', 5),\n",
              "  ('competitive neural', 5),\n",
              "  ('genetic algorithms', 5),\n",
              "  ('loopy propagation', 5),\n",
              "  ('modeling neural', 5),\n",
              "  ('models neural', 5),\n",
              "  ('multilayer perceptrons', 5),\n",
              "  ('networks connectionist', 5),\n",
              "  ('networks hierarchical', 5),\n",
              "  ('neural chip', 5),\n",
              "  ('neural classifier', 5),\n",
              "  ('neural regression', 5),\n",
              "  ('nonlinear models', 5),\n",
              "  ('ocular reflex', 5),\n",
              "  ('optimal neural', 5),\n",
              "  ('probabilistic models', 5),\n",
              "  ('silicon retina', 5),\n",
              "  ('sparse gaussian', 5),\n",
              "  ('vector quantization', 5),\n",
              "  ('vector regression', 5),\n",
              "  ('adaptive neural', 4),\n",
              "  ('algorithms neural', 4),\n",
              "  ('attractor neural', 4),\n",
              "  ('computational power', 4),\n",
              "  ('dirichlet allocation', 4),\n",
              "  ('dynamic bayesian', 4),\n",
              "  ('dynamic models', 4),\n",
              "  ('dynamics bayesian', 4),\n",
              "  ('generalization bounds', 4),\n",
              "  ('generative models', 4),\n",
              "  ('graph matching', 4),\n",
              "  ('hierarchical mixtures', 4),\n",
              "  ('kernel pca', 4),\n",
              "  ('layered networks', 4),\n",
              "  ('linear regression', 4),\n",
              "  ('logistic regression', 4),\n",
              "  ('markov networks', 4),\n",
              "  ('models probabilistic', 4),\n",
              "  ('multi chip', 4),\n",
              "  ('multidimensional scaling', 4),\n",
              "  ('multilayer networks', 4),\n",
              "  ('networks adaptive', 4),\n",
              "  ('networks coding', 4),\n",
              "  ('networks modeling', 4),\n",
              "  ('networks statistical', 4),\n",
              "  ('neural analog', 4),\n",
              "  ('neural computation', 4),\n",
              "  ('neural implementation', 4),\n",
              "  ('neural learns', 4),\n",
              "  ('nonparametric bayesian', 4),\n",
              "  ('observable markov', 4),\n",
              "  ('predictive models', 4),\n",
              "  ('rbf networks', 4),\n",
              "  ('statistical models', 4),\n",
              "  ('structural minimization', 4),\n",
              "  ('visual tracking', 4),\n",
              "  ('adaptive dynamic', 3),\n",
              "  ('algorithm observable', 3),\n",
              "  ('algorithm reinforcement', 3),\n",
              "  ('algorithm sequential', 3),\n",
              "  ('analogue vlsi', 3),\n",
              "  ('anomaly detection', 3),\n",
              "  ('approximate iteration', 3),\n",
              "  ('asymmetric hebbian', 3),\n",
              "  ('asymptotic convergence', 3),\n",
              "  ('auditory cortex', 3),\n",
              "  ('backpropagation networks', 3),\n",
              "  ('bayesian averaging', 3),\n",
              "  ('bayesian modeling', 3),\n",
              "  ('boosting algorithms', 3),\n",
              "  ('cascade correlation', 3),\n",
              "  ('characterization neural', 3),\n",
              "  ('clustering algorithm', 3),\n",
              "  ('clustering graph', 3),\n",
              "  ('connectionist networks', 3),\n",
              "  ('context free', 3),\n",
              "  ('decoding neuronal', 3),\n",
              "  ('dynamic environments', 3),\n",
              "  ('entropy minimization', 3),\n",
              "  ('feedforward networks', 3),\n",
              "  ('feedforward neural', 3),\n",
              "  ('finite automata', 3),\n",
              "  ('generalization neural', 3),\n",
              "  ('graphs cycles', 3),\n",
              "  ('hierarchical visual', 3),\n",
              "  ('hippocampal neurons', 3),\n",
              "  ('input output', 3),\n",
              "  ('integrated segmentation', 3),\n",
              "  ('invariant neural', 3),\n",
              "  ('kernel classifiers', 3),\n",
              "  ('kernel component', 3),\n",
              "  ('kernel fisher', 3),\n",
              "  ('kernels semi', 3),\n",
              "  ('laplacian eigenmaps', 3),\n",
              "  ('linear dimension', 3),\n",
              "  ('linear dimensionality', 3),\n",
              "  ('linear dynamical', 3),\n",
              "  ('linear networks', 3),\n",
              "  ('linear perceptrons', 3),\n",
              "  ('markov monte', 3),\n",
              "  ('matrix inversion', 3),\n",
              "  ('models auditory', 3),\n",
              "  ('models modeling', 3),\n",
              "  ('models recognizing', 3),\n",
              "  ('multi dimensional', 3),\n",
              "  ('multi networks', 3),\n",
              "  ('multi neural', 3),\n",
              "  ('multi perceptron', 3),\n",
              "  ('multilayer neural', 3),\n",
              "  ('networks bayesian', 3),\n",
              "  ('networks complexity', 3),\n",
              "  ('networks component', 3),\n",
              "  ('networks computing', 3),\n",
              "  ('networks generalized', 3),\n",
              "  ('networks implementation', 3),\n",
              "  ('networks linear', 3),\n",
              "  ('networks nonlinear', 3),\n",
              "  ('networks reinforcement', 3),\n",
              "  ('networks spike', 3),\n",
              "  ('networks unsupervised', 3),\n",
              "  ('networks weights', 3),\n",
              "  ('neural optimal', 3),\n",
              "  ('neural oscillator', 3),\n",
              "  ('neural simulation', 3),\n",
              "  ('neural spike', 3),\n",
              "  ('neural visual', 3),\n",
              "  ('neurons neural', 3),\n",
              "  ('pairwise clustering', 3),\n",
              "  ('parti algorithm', 3),\n",
              "  ('polynomial networks', 3),\n",
              "  ('power law', 3),\n",
              "  ('prefrontal cortex', 3),\n",
              "  ('probabilistic clustering', 3),\n",
              "  ('problem solving', 3),\n",
              "  ('programmable analog', 3),\n",
              "  ('propagation algorithm', 3),\n",
              "  ('propagation connectionist', 3),\n",
              "  ('propagation networks', 3),\n",
              "  ('regression neural', 3),\n",
              "  ('regression vector', 3),\n",
              "  ('reinforcement algorithm', 3),\n",
              "  ('saccadic eye', 3),\n",
              "  ('selective sampling', 3),\n",
              "  ('semidefinite programming', 3),\n",
              "  ('sequential monte', 3),\n",
              "  ('sparse pca', 3),\n",
              "  ('spiking neuron', 3),\n",
              "  ('statistical modeling', 3),\n",
              "  ('stochastic dynamics', 3),\n",
              "  ('stochastic neural', 3),\n",
              "  ('switching linear', 3),\n",
              "  ('synaptic plasticity', 3),\n",
              "  ('synchronization neural', 3),\n",
              "  ('temporally asymmetric', 3),\n",
              "  ('vector classifiers', 3),\n",
              "  ('vector kernels', 3),\n",
              "  ('vlsi chip', 3),\n",
              "  ('vlsi circuits', 3),\n",
              "  ('winner networks', 3)]]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate BERT Embeddings for all time windows\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "\n",
        "# List of Candidate Keywords [ [1st time slice candidate keywords], [2nd time slice candidate keywords], .....]\n",
        "content_time_slices_sent_tokens_list = time_slices_sent_tokens_list\n",
        "\n",
        "\n",
        "paper_content_bert_embeddings_all_time_slices_list = [ model.encode(time_slice_content[:100000]) for time_slice_content in content_time_slices_sent_tokens_list ]\n",
        "\n"
      ],
      "metadata": {
        "id": "SAKtIdbDzG7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(paper_content_bert_embeddings_all_time_slices_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL3CrYVUzB7L",
        "outputId": "8cbf4f15-bab4-4b90-8051-11998df6b3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get BERT EMbeddings for title ngrams\n",
        "title_ngrams_bert_embeddings_all_time_slices_list = [ model.encode([tup[0] for tup in time_slice_ngrams]) for time_slice_ngrams in title_ngram_candidate_keywords_time_slices_sorted ]\n",
        "\n",
        "\n",
        "title_ngrams_bert_embeddings_all_time_slices_list"
      ],
      "metadata": {
        "id": "pdmiU7aJG84r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_ngrams_bert_embeddings_all_time_slices_list = title_ngrams_bert_embeddings_all_time_slices_list[0]\n",
        "\n",
        "title_ngrams_bert_embeddings_all_time_slices_list"
      ],
      "metadata": {
        "id": "M-0BTDsLKQPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(title_ngrams_bert_embeddings_all_time_slices_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE-6P8GWQpZo",
        "outputId": "6d49f2c0-8b36-4be8-c3c2-1fc399d3c5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Trained Variables in Google Drive as pickle\n",
        "\n",
        "saved_map = {\n",
        "    \"paper_content_bert_embeddings_all_time_slices_list\" : paper_content_bert_embeddings_all_time_slices_list,\n",
        "    \"title_ngrams_bert_embeddings_all_time_slices_list\" : title_ngrams_bert_embeddings_all_time_slices_list\n",
        "}\n",
        "\n",
        "\n",
        "import pickle\n",
        "with open('/gdrive/My Drive/Master_dataset/variables.pickle', 'wb') as f:\n",
        "     pickle.dump(saved_map, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aOfPAvcsROEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the map from globals.\n",
        "del saved_map"
      ],
      "metadata": {
        "id": "8wHJ9V0lUqba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pickled variable saved in Drive.\n",
        "import pickle\n",
        "with open('/gdrive/My Drive/Master_dataset/variables.pickle', 'rb') as f:\n",
        "  saved_map = pickle.load(f)\n",
        "\n",
        "paper_content_bert_embeddings_all_time_slices_list_ = saved_map[\"paper_content_bert_embeddings_all_time_slices_list\"]"
      ],
      "metadata": {
        "id": "CqMQjkD6UvAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(paper_content_bert_embeddings_all_time_slices_list_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaT2BxFBU8fp",
        "outputId": "f264fcc4-a2a1-4590-85ce-14cad14f5422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2298"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 : Calculate Cosine Similarity of Candidate Keyowrds"
      ],
      "metadata": {
        "id": "GdSZMLzZb5v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Textual Similarity b/w corpus and candidate keywords\n",
        "from sentence_transformers import util\n",
        "\n",
        "cosine_scores = util.cos_sim(title_ngrams_bert_embeddings_all_time_slices_list, title_ngrams_bert_embeddings_all_time_slices_list)\n",
        "\n",
        "cosine_scores_list = paper_content_bert_embeddings_all_time_slices_list_\n",
        "\n",
        "top_n = 200\n",
        "# keywords = [candidate_keywords_bow[index] for index in cosine_scores.argsort()[0][-top_n:]]\n",
        "keywords\n",
        "\n",
        "\n",
        "# cosine_scores.argsort()[0][-top_n:]\n",
        "cosine_scores.argsort()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1cA73Q9cAgR",
        "outputId": "d24dad68-d3ba-4b16-a861-a73c987f00f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[343, 332, 344,  ..., 288, 220,   0],\n",
              "        [237, 257, 258,  ..., 355, 352,   1],\n",
              "        [243, 370, 257,  ...,  18, 343,   2],\n",
              "        ...,\n",
              "        [170, 344, 300,  ..., 224, 398, 397],\n",
              "        [243, 344, 188,  ..., 224, 397, 398],\n",
              "        [258, 386,  23,  ..., 215, 359, 399]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST 3 - BERT EMBEDDING GENERATE"
      ],
      "metadata": {
        "id": "kTigylDrZ2xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_ngram_candidate_keywords_time_slices_sorted = title_ngram_candidate_keywords_time_slices_sorted[0]"
      ],
      "metadata": {
        "id": "1kvz3ODWgWRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_ngram_candidate_keywords_time_slices_sorted"
      ],
      "metadata": {
        "id": "hSu_Hz4-ge76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "771b7174-007c-4188-d17e-fb04a4687f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('neural', 425),\n",
              " ('networks', 338),\n",
              " ('models', 179),\n",
              " ('bayesian', 123),\n",
              " ('visual', 98),\n",
              " ('algorithm', 91),\n",
              " ('reinforcement', 86),\n",
              " ('gaussian', 85),\n",
              " ('clustering', 84),\n",
              " ('linear', 84),\n",
              " ('algorithms', 83),\n",
              " ('kernel', 78),\n",
              " ('adaptive', 77),\n",
              " ('markov', 72),\n",
              " ('neurons', 71),\n",
              " ('multi', 69),\n",
              " ('vector', 69),\n",
              " ('analog', 66),\n",
              " ('modeling', 66),\n",
              " ('dynamic', 64),\n",
              " ('vlsi', 58),\n",
              " ('regression', 57),\n",
              " ('recurrent', 56),\n",
              " ('sparse', 56),\n",
              " ('detection', 55),\n",
              " ('dynamics', 55),\n",
              " ('generalization', 54),\n",
              " ('probabilistic', 54),\n",
              " ('optimal', 53),\n",
              " ('propagation', 51),\n",
              " ('connectionist', 47),\n",
              " ('hierarchical', 46),\n",
              " ('stochastic', 43),\n",
              " ('kernels', 42),\n",
              " ('supervised', 42),\n",
              " ('bounds', 41),\n",
              " ('multiple', 41),\n",
              " ('optimization', 41),\n",
              " ('prediction', 41),\n",
              " ('nonlinear', 39),\n",
              " ('spike', 39),\n",
              " ('statistical', 38),\n",
              " ('convergence', 37),\n",
              " ('cortex', 37),\n",
              " ('spiking', 37),\n",
              " ('unsupervised', 37),\n",
              " ('gradient', 36),\n",
              " ('approximate', 35),\n",
              " ('classifiers', 35),\n",
              " ('coding', 33),\n",
              " ('mixtures', 33),\n",
              " ('semi', 33),\n",
              " ('programming', 32),\n",
              " ('analog vlsi', 31),\n",
              " ('approximation', 30),\n",
              " ('boosting', 30),\n",
              " ('computational', 30),\n",
              " ('auditory', 29),\n",
              " ('density', 29),\n",
              " ('variational', 29),\n",
              " ('associative', 28),\n",
              " ('complexity', 28),\n",
              " ('implementation', 28),\n",
              " ('neuron', 28),\n",
              " ('chip', 27),\n",
              " ('semi supervised', 26),\n",
              " ('component', 26),\n",
              " ('dimensional', 26),\n",
              " ('computation', 25),\n",
              " ('cortical', 25),\n",
              " ('graph', 25),\n",
              " ('invariant', 25),\n",
              " ('neuronal', 25),\n",
              " ('regularization', 25),\n",
              " ('spatial', 25),\n",
              " ('spiking neurons', 24),\n",
              " ('spectral', 24),\n",
              " ('dynamical', 23),\n",
              " ('extraction', 23),\n",
              " ('minimization', 23),\n",
              " ('silicon', 23),\n",
              " ('adaptation', 22),\n",
              " ('discriminative', 22),\n",
              " ('distributions', 22),\n",
              " ('organizing', 22),\n",
              " ('sampling', 22),\n",
              " ('recurrent neural', 21),\n",
              " ('computing', 21),\n",
              " ('estimating', 21),\n",
              " ('graphical', 21),\n",
              " ('segmentation', 21),\n",
              " ('markov models', 20),\n",
              " ('backpropagation', 20),\n",
              " ('bayes', 20),\n",
              " ('experts', 20),\n",
              " ('matching', 20),\n",
              " ('plasticity', 20),\n",
              " ('sequential', 20),\n",
              " ('entropy', 19),\n",
              " ('pca', 19),\n",
              " ('structured', 19),\n",
              " ('networks neural', 18),\n",
              " ('context', 18),\n",
              " ('convex', 18),\n",
              " ('finite', 18),\n",
              " ('power', 18),\n",
              " ('predictive', 18),\n",
              " ('problem', 18),\n",
              " ('sequences', 18),\n",
              " ('tracking', 18),\n",
              " ('visual cortex', 17),\n",
              " ('decomposition', 17),\n",
              " ('face', 17),\n",
              " ('filtering', 17),\n",
              " ('generalized', 17),\n",
              " ('matrix', 17),\n",
              " ('radial', 17),\n",
              " ('selective', 17),\n",
              " ('synaptic', 17),\n",
              " ('theoretic', 17),\n",
              " ('classifier', 16),\n",
              " ('hand', 16),\n",
              " ('hebbian', 16),\n",
              " ('integration', 16),\n",
              " ('likelihood', 16),\n",
              " ('localization', 16),\n",
              " ('monte', 16),\n",
              " ('timing', 16),\n",
              " ('recurrent networks', 15),\n",
              " ('conditional', 15),\n",
              " ('dimension', 15),\n",
              " ('eye', 15),\n",
              " ('graphs', 15),\n",
              " ('hybrid', 15),\n",
              " ('input', 15),\n",
              " ('ocular', 15),\n",
              " ('orientation', 15),\n",
              " ('receptive', 15),\n",
              " ('robot', 15),\n",
              " ('scaling', 15),\n",
              " ('simulation', 15),\n",
              " ('techniques', 15),\n",
              " ('dynamic programming', 14),\n",
              " ('graphical models', 14),\n",
              " ('boltzmann', 14),\n",
              " ('correlation', 14),\n",
              " ('dimensionality', 14),\n",
              " ('incremental', 14),\n",
              " ('maximization', 14),\n",
              " ('multilayer', 14),\n",
              " ('nonparametric', 14),\n",
              " ('perceptron', 14),\n",
              " ('applied', 13),\n",
              " ('diffusion', 13),\n",
              " ('discrete', 13),\n",
              " ('embedding', 13),\n",
              " ('global', 13),\n",
              " ('induction', 13),\n",
              " ('parameter', 13),\n",
              " ('parametric', 13),\n",
              " ('patterns', 13),\n",
              " ('perceptrons', 13),\n",
              " ('regularized', 13),\n",
              " ('retrieval', 13),\n",
              " ('world', 13),\n",
              " ('spectral clustering', 12),\n",
              " ('spike timing', 12),\n",
              " ('approximations', 12),\n",
              " ('bias', 12),\n",
              " ('circuits', 12),\n",
              " ('competitive', 12),\n",
              " ('components', 12),\n",
              " ('environments', 12),\n",
              " ('estimates', 12),\n",
              " ('feedback', 12),\n",
              " ('filters', 12),\n",
              " ('generative', 12),\n",
              " ('ica', 12),\n",
              " ('inferring', 12),\n",
              " ('perceptual', 12),\n",
              " ('relational', 12),\n",
              " ('weights', 12),\n",
              " ('gaussian models', 11),\n",
              " ('variational bayesian', 11),\n",
              " ('vlsi neural', 11),\n",
              " ('alignment', 11),\n",
              " ('dirichlet', 11),\n",
              " ('discriminant', 11),\n",
              " ('free', 11),\n",
              " ('functional', 11),\n",
              " ('hippocampal', 11),\n",
              " ('iterative', 11),\n",
              " ('metric', 11),\n",
              " ('modelling', 11),\n",
              " ('sensory', 11),\n",
              " ('svm', 11),\n",
              " ('synapses', 11),\n",
              " ('variables', 11),\n",
              " ('analog neural', 10),\n",
              " ('bayesian networks', 10),\n",
              " ('gaussian regression', 10),\n",
              " ('mixtures experts', 10),\n",
              " ('approaches', 10),\n",
              " ('bottleneck', 10),\n",
              " ('constraints', 10),\n",
              " ('coupling', 10),\n",
              " ('genetic', 10),\n",
              " ('handwritten', 10),\n",
              " ('hmm', 10),\n",
              " ('iteration', 10),\n",
              " ('lateral', 10),\n",
              " ('modulation', 10),\n",
              " ('neuromorphic', 10),\n",
              " ('pomdps', 10),\n",
              " ('protein', 10),\n",
              " ('ranking', 10),\n",
              " ('saliency', 10),\n",
              " ('structures', 10),\n",
              " ('tuning', 10),\n",
              " ('ocular dominance', 9),\n",
              " ('neural neural', 8),\n",
              " ('linear programming', 7),\n",
              " ('matrix factorization', 7),\n",
              " ('timing plasticity', 7),\n",
              " ('vlsi implementation', 7),\n",
              " ('connectionist models', 6),\n",
              " ('dynamics neural', 6),\n",
              " ('face detection', 6),\n",
              " ('gaussian mixtures', 6),\n",
              " ('linear models', 6),\n",
              " ('networks analog', 6),\n",
              " ('networks spiking', 6),\n",
              " ('neural classifiers', 6),\n",
              " ('neural models', 6),\n",
              " ('pac bayes', 6),\n",
              " ('radial networks', 6),\n",
              " ('sparse coding', 6),\n",
              " ('statistical mechanics', 6),\n",
              " ('vestibulo ocular', 6),\n",
              " ('algorithm neural', 5),\n",
              " ('audio visual', 5),\n",
              " ('bayesian models', 5),\n",
              " ('competitive neural', 5),\n",
              " ('genetic algorithms', 5),\n",
              " ('loopy propagation', 5),\n",
              " ('modeling neural', 5),\n",
              " ('models neural', 5),\n",
              " ('multilayer perceptrons', 5),\n",
              " ('networks connectionist', 5),\n",
              " ('networks hierarchical', 5),\n",
              " ('neural chip', 5),\n",
              " ('neural classifier', 5),\n",
              " ('neural regression', 5),\n",
              " ('nonlinear models', 5),\n",
              " ('ocular reflex', 5),\n",
              " ('optimal neural', 5),\n",
              " ('probabilistic models', 5),\n",
              " ('silicon retina', 5),\n",
              " ('sparse gaussian', 5),\n",
              " ('vector quantization', 5),\n",
              " ('vector regression', 5),\n",
              " ('adaptive neural', 4),\n",
              " ('algorithms neural', 4),\n",
              " ('attractor neural', 4),\n",
              " ('computational power', 4),\n",
              " ('dirichlet allocation', 4),\n",
              " ('dynamic bayesian', 4),\n",
              " ('dynamic models', 4),\n",
              " ('dynamics bayesian', 4),\n",
              " ('generalization bounds', 4),\n",
              " ('generative models', 4),\n",
              " ('graph matching', 4),\n",
              " ('hierarchical mixtures', 4),\n",
              " ('kernel pca', 4),\n",
              " ('layered networks', 4),\n",
              " ('linear regression', 4),\n",
              " ('logistic regression', 4),\n",
              " ('markov networks', 4),\n",
              " ('models probabilistic', 4),\n",
              " ('multi chip', 4),\n",
              " ('multidimensional scaling', 4),\n",
              " ('multilayer networks', 4),\n",
              " ('networks adaptive', 4),\n",
              " ('networks coding', 4),\n",
              " ('networks modeling', 4),\n",
              " ('networks statistical', 4),\n",
              " ('neural analog', 4),\n",
              " ('neural computation', 4),\n",
              " ('neural implementation', 4),\n",
              " ('neural learns', 4),\n",
              " ('nonparametric bayesian', 4),\n",
              " ('observable markov', 4),\n",
              " ('predictive models', 4),\n",
              " ('rbf networks', 4),\n",
              " ('statistical models', 4),\n",
              " ('structural minimization', 4),\n",
              " ('visual tracking', 4),\n",
              " ('adaptive dynamic', 3),\n",
              " ('algorithm observable', 3),\n",
              " ('algorithm reinforcement', 3),\n",
              " ('algorithm sequential', 3),\n",
              " ('analogue vlsi', 3),\n",
              " ('anomaly detection', 3),\n",
              " ('approximate iteration', 3),\n",
              " ('asymmetric hebbian', 3),\n",
              " ('asymptotic convergence', 3),\n",
              " ('auditory cortex', 3),\n",
              " ('backpropagation networks', 3),\n",
              " ('bayesian averaging', 3),\n",
              " ('bayesian modeling', 3),\n",
              " ('boosting algorithms', 3),\n",
              " ('cascade correlation', 3),\n",
              " ('characterization neural', 3),\n",
              " ('clustering algorithm', 3),\n",
              " ('clustering graph', 3),\n",
              " ('connectionist networks', 3),\n",
              " ('context free', 3),\n",
              " ('decoding neuronal', 3),\n",
              " ('dynamic environments', 3),\n",
              " ('entropy minimization', 3),\n",
              " ('feedforward networks', 3),\n",
              " ('feedforward neural', 3),\n",
              " ('finite automata', 3),\n",
              " ('generalization neural', 3),\n",
              " ('graphs cycles', 3),\n",
              " ('hierarchical visual', 3),\n",
              " ('hippocampal neurons', 3),\n",
              " ('input output', 3),\n",
              " ('integrated segmentation', 3),\n",
              " ('invariant neural', 3),\n",
              " ('kernel classifiers', 3),\n",
              " ('kernel component', 3),\n",
              " ('kernel fisher', 3),\n",
              " ('kernels semi', 3),\n",
              " ('laplacian eigenmaps', 3),\n",
              " ('linear dimension', 3),\n",
              " ('linear dimensionality', 3),\n",
              " ('linear dynamical', 3),\n",
              " ('linear networks', 3),\n",
              " ('linear perceptrons', 3),\n",
              " ('markov monte', 3),\n",
              " ('matrix inversion', 3),\n",
              " ('models auditory', 3),\n",
              " ('models modeling', 3),\n",
              " ('models recognizing', 3),\n",
              " ('multi dimensional', 3),\n",
              " ('multi networks', 3),\n",
              " ('multi neural', 3),\n",
              " ('multi perceptron', 3),\n",
              " ('multilayer neural', 3),\n",
              " ('networks bayesian', 3),\n",
              " ('networks complexity', 3),\n",
              " ('networks component', 3),\n",
              " ('networks computing', 3),\n",
              " ('networks generalized', 3),\n",
              " ('networks implementation', 3),\n",
              " ('networks linear', 3),\n",
              " ('networks nonlinear', 3),\n",
              " ('networks reinforcement', 3),\n",
              " ('networks spike', 3),\n",
              " ('networks unsupervised', 3),\n",
              " ('networks weights', 3),\n",
              " ('neural optimal', 3),\n",
              " ('neural oscillator', 3),\n",
              " ('neural simulation', 3),\n",
              " ('neural spike', 3),\n",
              " ('neural visual', 3),\n",
              " ('neurons neural', 3),\n",
              " ('pairwise clustering', 3),\n",
              " ('parti algorithm', 3),\n",
              " ('polynomial networks', 3),\n",
              " ('power law', 3),\n",
              " ('prefrontal cortex', 3),\n",
              " ('probabilistic clustering', 3),\n",
              " ('problem solving', 3),\n",
              " ('programmable analog', 3),\n",
              " ('propagation algorithm', 3),\n",
              " ('propagation connectionist', 3),\n",
              " ('propagation networks', 3),\n",
              " ('regression neural', 3),\n",
              " ('regression vector', 3),\n",
              " ('reinforcement algorithm', 3),\n",
              " ('saccadic eye', 3),\n",
              " ('selective sampling', 3),\n",
              " ('semidefinite programming', 3),\n",
              " ('sequential monte', 3),\n",
              " ('sparse pca', 3),\n",
              " ('spiking neuron', 3),\n",
              " ('statistical modeling', 3),\n",
              " ('stochastic dynamics', 3),\n",
              " ('stochastic neural', 3),\n",
              " ('switching linear', 3),\n",
              " ('synaptic plasticity', 3),\n",
              " ('synchronization neural', 3),\n",
              " ('temporally asymmetric', 3),\n",
              " ('vector classifiers', 3),\n",
              " ('vector kernels', 3),\n",
              " ('vlsi chip', 3),\n",
              " ('vlsi circuits', 3),\n",
              " ('winner networks', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title_ngram_candidate_keywords_time_slices_sorted_ = [ngram[0] for ngram in title_ngram_candidate_keywords_time_slices_sorted[:100]]\n",
        "t = \"\\n\".join(title_ngram_candidate_keywords_time_slices_sorted_)\n",
        "t"
      ],
      "metadata": {
        "id": "4fcx51Krc-lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "1757d246-d98f-4f74-aab2-e80bd6d64a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neural\\nnetworks\\nmodels\\nbayesian\\nvisual\\nalgorithm\\nreinforcement\\ngaussian\\nclustering\\nlinear\\nalgorithms\\nkernel\\nadaptive\\nmarkov\\nneurons\\nmulti\\nvector\\nanalog\\nmodeling\\ndynamic\\nvlsi\\nregression\\nrecurrent\\nsparse\\ndetection\\ndynamics\\ngeneralization\\nprobabilistic\\noptimal\\npropagation\\nconnectionist\\nhierarchical\\nstochastic\\nkernels\\nsupervised\\nbounds\\nmultiple\\noptimization\\nprediction\\nnonlinear\\nspike\\nstatistical\\nconvergence\\ncortex\\nspiking\\nunsupervised\\ngradient\\napproximate\\nclassifiers\\ncoding\\nmixtures\\nsemi\\nprogramming\\nanalog vlsi\\napproximation\\nboosting\\ncomputational\\nauditory\\ndensity\\nvariational\\nassociative\\ncomplexity\\nimplementation\\nneuron\\nchip\\nsemi supervised\\ncomponent\\ndimensional\\ncomputation\\ncortical\\ngraph\\ninvariant\\nneuronal\\nregularization\\nspatial\\nspiking neurons\\nspectral\\ndynamical\\nextraction\\nminimization\\nsilicon\\nadaptation\\ndiscriminative\\ndistributions\\norganizing\\nsampling\\nrecurrent neural\\ncomputing\\nestimating\\ngraphical\\nsegmentation\\nmarkov models\\nbackpropagation\\nbayes\\nexperts\\nmatching\\nplasticity\\nsequential\\nentropy\\npca'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install Unidecode"
      ],
      "metadata": {
        "id": "j6BqKd5ZheKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0782e4e1-0610-44b2-aa2e-1404356c64ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.24.42)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.0+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.42 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.27.42)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.42->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.42->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.42->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 18.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from collections import OrderedDict\n",
        "import unidecode\n",
        "import numpy as np\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "# % matplotlib inline"
      ],
      "metadata": {
        "id": "x6P0dfVehoGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "mTl8ffpftcpl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c7453a1c-bce4-4a45-c2db-fb96f10f21a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Use the pre-trained Base BERT model \n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.cuda()\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "FKo54yLjzQut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s = \"This be a semi supervised string\"\n",
        "# if s.find(\"semi supervised\") != -1:\n",
        "#     print(\"Found\")\n",
        "# else:\n",
        "#     print(\"Not Found\")\n",
        "\n",
        "# targets = [\"pca\", \"semi supervised\"]\n",
        "# corpus = [\"This sentence is an example of pca\", \"pca is good for dimensionality reduction\", \"this is good example of semi supervised\"]\n",
        "\n",
        "\n",
        "# index, t_ind = datasets._preprocess(targets, corpus)\n",
        "# t_ind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8Zx2bfZuGl2",
        "outputId": "4bf61b0a-0e48-49a3-d251-6aab829317d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('pca', [0, 1]), ('semi supervised', [2])])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "class Data():\n",
        "\n",
        "    def __getitem__(self, content=None):\n",
        "         if content!=None:\n",
        "             self.doc = \"\".join(content)\n",
        "         return self.doc\n",
        "     \n",
        "    def _preprocess(self,targets,corpus):\n",
        "        self.index=[]\n",
        "        self.t_index=OrderedDict()\n",
        "        for target in targets:\n",
        "            \n",
        "            for _,item in enumerate(corpus):\n",
        "                # if target in item:\n",
        "                  if item.lower().find(target) != -1:\n",
        "                # if bool(re.search(target, item)):\n",
        "\n",
        "                      count_target=item.count(target)\n",
        "                  #   Avoiding the sentences with multiple occurrences of the target term for the time being###\n",
        "                      if count_target==1:\n",
        "                        if target not in self.t_index.keys():\n",
        "                            self.t_index[target]=[_]\n",
        "                        else:\n",
        "                            self.t_index[target].append(_)\n",
        "                        self.index.append(_)\n",
        "        return self.index,self.t_index"
      ],
      "metadata": {
        "id": "7FhI9iflZ-2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "LOAD & EXTRACT DATA\n",
        "'''\n",
        "import os\n",
        "\n",
        "\n",
        "# OUTPUT_DIR = root_dir+'Colab Notebooks/Challenge_Semeval/CLUSTERING/English_test/' # the path thatcontains the (Corpus1_text, Corpus2_text, Targets)\n",
        "# p1 = os.path.join(OUTPUT_DIR, 'ccoha1.txt')\n",
        "# p2 = os.path.join(OUTPUT_DIR, 'ccoha2.txt')\n",
        "# t = os.path.join(OUTPUT_DIR, 'targets.txt')\n",
        "\n",
        "# INPUT_DIR = '.\\\\evaluation\\\\semeval2020_ulscd_eng\\\\'\n",
        "# p1 = os.path.join(INPUT_DIR, 'corpus1\\\\ccoha1.txt')\n",
        "# p2 = os.path.join(INPUT_DIR, 'corpus2\\\\ccoha2.txt')\n",
        "# #TARGET_DIR = '.\\\\targets\\\\'\n",
        "# t = os.path.join(INPUT_DIR, 'targets.txt')\n",
        "# p1='ccoha1.txt'\n",
        "# p2='ccoha2.txt'\n",
        "# t='targets.txt'\n",
        "\n",
        "p1 = nips_papers_partitions[0][\"paper_text\"].tolist()\n",
        "p2 = nips_papers_partitions[1][\"paper_text\"].tolist()\n",
        "t = t\n",
        "datasets = Data() \n",
        "\n",
        "# doc1 =  [\"Sentence1\", \"Sentence2\".....]\n",
        "doc1=datasets.__getitem__(p1).split('\\n')   \n",
        "doc2=datasets.__getitem__(p2).split('\\n')\n",
        "t1=datasets.__getitem__(t).split('\\n')\n",
        "target_act=[x for x in t1 if len(x)>1]\n",
        "t1=[x.lower() for x in t1 if len(x)>1]\n",
        "index1=datasets._preprocess(t1,doc1)\n",
        "index2=datasets._preprocess(t1,doc2)         \n",
        "index_t1=index1[1]\n",
        "index_t2=index2[1]\n",
        "print('The target words are:',t1)\n",
        "target_words=t1\n",
        "\n",
        "print('The index_t1 are ', index_t1)\n",
        "print('The index_t2 are ', index_t2)\n",
        "\n",
        "\n",
        "#conversions\n",
        "target_uni=[unidecode.unidecode(m) for m in t1]\n",
        "target_toks=[]\n",
        "# print(target_uni)\n",
        "for k in t1:\n",
        "  target_toks.append(tokenizer.tokenize(k))\n",
        "print('converted target toks',target_toks)"
      ],
      "metadata": {
        "id": "WBbu-VZwaN3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(t1)\n",
        "print(len(index_t1))\n",
        "print(len(index_t2))\n",
        "target_toks"
      ],
      "metadata": {
        "id": "dkhf0E9Zoo1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _pre_bert(doc,index,t):\n",
        "  \n",
        "  # index =  index_t1 -> { target_w1: index, target_w2: index2, target_w1 : index5 } -  index = Sentence index in which target word appears\n",
        "  s=[\"Not Found\"]  \n",
        "  \n",
        "  if t in index.keys():\n",
        "      s=[doc[ind] for ind in index[t]]\n",
        "\n",
        "  print('len of sentences',len(s))\n",
        "  l=len(s)\n",
        "  marked_text = [\"[CLS] \" + text + \" [SEP]\" for text in s]\n",
        "  tokenized_text = [tokenizer.tokenize(m) for m in marked_text]\n",
        "  \n",
        "  tokenized_text=[x[:512] if len(x)>512 else x for x in tokenized_text]\n",
        "  indexed_tokens = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_text]\n",
        "  segments_ids = [[1] * len(x) for x in tokenized_text]\n",
        "  return s,marked_text,tokenized_text,indexed_tokens,segments_ids,l\n",
        "\n",
        "\n",
        "def _bert_features(tokens_tensor, segments_tensors,tokenized_text):\n",
        "  # print(len(tokens_tensor[0]))\n",
        "  with torch.no_grad():\n",
        "      encoded_layers, _ = model(tokens_tensor.to(device), segments_tensors.to(device))\n",
        "  # print (\"Number of layers:\", len(encoded_layers))\n",
        "  layer_i = 0\n",
        "\n",
        "  # # print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
        "  batch_i = 0\n",
        "\n",
        "  # print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
        "  token_i = 0\n",
        "\n",
        "  # print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))\n",
        "  # Convert the hidden state embeddings into single token vectors\n",
        "\n",
        "  # Holds the list of 12 layer embeddings for each token\n",
        "  # Will have the shape: [# tokens, # layers, # features]\n",
        "  token_embeddings = [] \n",
        "\n",
        "  # For each token in the sentence...\n",
        "  # tokenized_text=[x for x in tokenized_text if x not in ['_', 'n', '##n','v', '##b']]\n",
        "  for token_i in range(len(tokenized_text)):\n",
        "    \n",
        "    # Holds 12 layers of hidden states for each token \n",
        "    hidden_layers = [] \n",
        "    \n",
        "    # For each of the 12 layers...\n",
        "    for layer_i in range(len(encoded_layers)):\n",
        "      \n",
        "      # Lookup the vector for `token_i` in `layer_i`\n",
        "      vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "      \n",
        "      hidden_layers.append(vec)\n",
        "      \n",
        "    token_embeddings.append(hidden_layers)\n",
        "\n",
        "  # Sanity check the dimensions:\n",
        "  # print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
        "  # print (\"Number of layers per token:\", len(token_embeddings[0]))\n",
        "  return token_embeddings\n",
        "# s,marked_text,tokenized_text,indexed_tokens,segments_ids\n",
        "def _get_embeddings(pre,tg):\n",
        "  m_embed_full=[]\n",
        "  # print('len(pre[0])',len(pre[0]))\n",
        "  # print(tg)\n",
        "  for _,item in enumerate(pre[0]):\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    # print(item)\n",
        "    token_list=pre[2][_]\n",
        "    \n",
        "    tokens_tensor = torch.tensor([pre[3][_]])\n",
        "    segments_tensors = torch.tensor([pre[4][_]])\n",
        "    # Predict hidden states features for each layer\n",
        "    token_embeddings=_bert_features(tokens_tensor, segments_tensors,pre[2][_])\n",
        "    concatenated_last_4_layers = [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings] # [number_of_tokens, 3072]\n",
        "\n",
        "    summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]\n",
        "    \n",
        "    #consider the tokenized target\n",
        "  \n",
        "    indxs=[]\n",
        "    # print(token_list)\n",
        "    for tok in tg:\n",
        "      '''\n",
        "      remove -1,-2,-3\n",
        "      '''\n",
        "      if tok in token_list:\n",
        "        if tok not in ['_', 'n', '##n','v', '##b']:\n",
        "          indxs.append(token_list.index(tok))\n",
        "\n",
        "    # print('indxs',indxs)\n",
        "    if len(indxs)==1:\n",
        "      bert_embed=concatenated_last_4_layers [indxs[0]]\n",
        "      m_embed_full.append(bert_embed)\n",
        "    elif len(indxs)>1:\n",
        "      b_emb=[]\n",
        "      for ind in indxs:\n",
        "        b_emb.append(concatenated_last_4_layers[ind])\n",
        "      bert_embed= torch.sum(torch.stack(b_emb), 0)\n",
        "      m_embed_full.append(bert_embed)\n",
        "    # indx=token_list.index(tg.lower())\n",
        "    # indx = [i for (i, elem) in enumerate(pre[2][_]) if t in elem]\n",
        "    # print('indx',indx)\n",
        "    # print(pre[1][_],indx)\n",
        "\n",
        "    # if len(indx)>0:\n",
        "    # bert_embed=concatenated_last_4_layers[indx[0]]\n",
        "    \n",
        "    # cosine_similarity(summed_last_4_layers[10].reshape(1,-1), summed_last_4_layers[19].reshape(1,-1))[0][0]\n",
        "    \n",
        "    \n",
        "  return  m_embed_full\n",
        "# For a particular target word,do clustering and find if there is a sense change\n",
        "\n",
        "#[target_words[1]]\n",
        "def embeddings_extract(target_words,target_toks,doc1,index_t1,doc2,index_t2):\n",
        "  t=target_words\n",
        "  X=[]\n",
        "  X_C1=[]\n",
        "  X_C2=[]\n",
        "  sents_all=[]\n",
        "  lens1=[]\n",
        "  lens2=[]\n",
        "  for k,t in enumerate(target_words) :\n",
        "    berts=[]\n",
        "    sents=[]\n",
        "    print('The target word is',t)    \n",
        "    \n",
        "    #get the sentences from corpus c1 and c2 for the specific target word 't'\n",
        "    \n",
        "    pre1=_pre_bert(doc1,index_t1,t)\n",
        "\n",
        "    pre2=_pre_bert(doc2,index_t2,t)\n",
        "    # lens1.append(pre1[-1])\n",
        "    # lens2.append(pre2[-1])\n",
        "    # print(pre1)\n",
        "    \n",
        "    sents.extend(pre1[0])\n",
        "    sents.extend(pre2[0])\n",
        "    #aggregate all the embeddings\n",
        "    # s,marked_text,tokenized_text,indexed_tokens,segments_ids\n",
        "\n",
        "    '''\n",
        "    Get the embeddings of the targets from corpus 1 and 2\n",
        "    '''\n",
        "    b1=_get_embeddings(pre1,target_toks[k])\n",
        "    print('len of t1',len(b1))\n",
        "    b2=_get_embeddings(pre2,target_toks[k])\n",
        "    print('len of t2',len(b2))\n",
        "    '''\n",
        "    store the lenghts of no. of sentences extracted for each target word for each corpus\n",
        "    '''\n",
        "    lens1.append(len(b1))\n",
        "    lens2.append(len(b2))\n",
        "    \n",
        "    berts.extend(b1)\n",
        "    berts.extend(b2)\n",
        "    print('len of each target word extractions is',len(berts))\n",
        "    X.append(berts)\n",
        "    X_C1.append(b1)# the embeddings for C1\n",
        "    X_C2.append(b2)#embeddings for C2\n",
        "    sents_all.append(sents)\n",
        "  return X,X_C1,X_C2,lens1,lens2,sents_all"
      ],
      "metadata": {
        "id": "ec5y4w-gqamr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "embed_full,embed_C1,embed_C2,len_c1,len_c2,sents=embeddings_extract(target_words,target_toks,doc1,index_t1,doc2,index_t2)\n",
        "\n",
        "lens=[len_c1]\n",
        "lens.append(len_c2)\n",
        "print('saved')\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "LLgkz2ZztHOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embed_C1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWzlJE8F5UGV",
        "outputId": "d8ad5c90-9ab4-4dad-ec91-6e1cd75ed924"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # target_words.index(\"semi supervised\")\n",
        "# import re\n",
        "\n",
        "# print(len(index_t1.keys()))\n",
        "\n",
        "# print(list(set(t1) - set(index_t1.keys())))\n",
        "# # if bool(re.search(\"semi supervised\", \" \".join(title_ngrams_bert_embeddings_all_time_slices_list[0])) ):\n",
        "# #   print(\"as\")\n",
        "\n",
        "\n",
        "# if \"semi supervised\" in t1:\n",
        "#   print(\"found\")"
      ],
      "metadata": {
        "id": "AnbcGmS86_2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2ba352-0a0d-432f-bea0-8bf2ba9f3a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97\n",
            "['pca', 'semi supervised', 'markov models']\n",
            "found\n"
          ]
        }
      ]
    }
  ]
}